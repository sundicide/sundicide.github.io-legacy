{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"docker/","text":"docker ps format https://gist.github.com/wzulfikar/f6f7dc8b9d6aa5bc207eaa31913201d8 \uc601\uad6c \uc801\uc6a9 vi ~/.docker/config.json { \"psFormat\": \"ID\\t{{.ID}}\\nNAME\\t{{.Names}}\\nIMAGE\\t{{.Image}}\\nPORTS\\t{{.Ports}}\\nCOMMAND\\t{{.Command}}\\nCREATED\\t{{.CreatedAt}}\\nSTATUS\\t{{.Status}}\\n\" } \uc77c\uc2dc \uc801\uc6a9 export FORMAT=\"...\" docker ps --format=\"$FORMAT\"","title":"Docker"},{"location":"docker/#docker","text":"","title":"docker"},{"location":"docker/#ps-format","text":"https://gist.github.com/wzulfikar/f6f7dc8b9d6aa5bc207eaa31913201d8","title":"ps format"},{"location":"docker/#_1","text":"vi ~/.docker/config.json { \"psFormat\": \"ID\\t{{.ID}}\\nNAME\\t{{.Names}}\\nIMAGE\\t{{.Image}}\\nPORTS\\t{{.Ports}}\\nCOMMAND\\t{{.Command}}\\nCREATED\\t{{.CreatedAt}}\\nSTATUS\\t{{.Status}}\\n\" }","title":"\uc601\uad6c \uc801\uc6a9"},{"location":"docker/#_2","text":"export FORMAT=\"...\" docker ps --format=\"$FORMAT\"","title":"\uc77c\uc2dc \uc801\uc6a9"},{"location":"r/","text":"CSV Read CSV my_csv = read.csv('my.csv', row.names = 1, stringsAsFactors = FALSE) Select Some Columns # select all rows and select some columns # (col1, col2, col3) my_csv = my_csv[, c('col1', 'col2', 'col3')] DataFrame Create DataFrame df <- c(1,2,3,4) \ub7ad\ud06c <- c('D', 'E', 'A', 'B') data.frame(df, \ub7ad\ud06c) row \uc218\ub97c \uc54c\uc544\ub0b4\ub294 \ubc29\ubc95 nrow(df) bind row names to column values # my_csv\uc758 rownames\ub97c \uceec\ub7fc \uac12\uc73c\ub85c \ub123\ub294\ub2e4. rownames(df) = df[['\ub7ad\ud06c']] # age name \ub7ad\ud06c # D 25 Tom D # E 34 Harry E # A 28 Porter A # B 52 Harden B \uceec\ub7fc \uac12\uc744 \uc774\uc6a9\ud558\ub294 \ub300\uc2e0 pad\ub3c4 \uc774\uc6a9\ud55c\ub2e4. \uc608\ub97c \ub4e4\uc5b4 value2\uc548\uc5d0 code \uceec\ub7fc\uc758 \uac12\uc774 0001, 00002, 000003 \ub77c\uace0 \ud558\uc790. \uc774\ub7f4 \ub54c \uc544\ub798\uc640 \uac19\uc774 \ud574\uc8fc\uba74 rownames\uc5d0\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \ucc44\uc6cc\uc9c4\ub2e4. 000001 000002 000003 str_pad\ub97c \uc774\uc6a9\ud574 \uc790\ub9ac\uc218\uac00 \ubd80\uc871\ud55c \ub9cc\ud07c 0\uc73c\ub85c \ucc44\uc6cc\uc8fc\ub294 \uac83\uc774\ub2e4 rownames(value2) = str_pad(value2$'code', 6, side=c('left'), pad='0') transpose df dataframe\uc758 \ud589\uacfc \uc5f4\uc744 \ubc14\uafc0\ub54c \uc0ac\uc6a9\ud55c\ub2e4. df %>% t() get last rows of df df\ub0b4\uc5d0\uc11c \ub9c8\uc9c0\ub9c9 \uc904 \ub9cc \uc5bb\uace0 \uc2f6\uc744 \ub54c \uc0ac\uc6a9\ud55c\ub2e4. rowLength = nrow(df) df[rowLength, ] change column name \uceec\ub7fc \uba85\uc744 \uc6d0\ud558\ub294 \uc774\ub984\uc73c\ub85c \ubc14\uafc0 \uc218 \uc788\ub2e4. df = df[, c('\uceec\ub7fc1', '\uceec\ub7fc2')] \ub610\ub294 column\uc774 1\uac1c\uc77c \uacbd\uc6b0 \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218\ub3c4 \uc788\ub2e4. colnames(df) = '\uceec\ub7fc' dff\uc5d0\ub294 dataframe \uc911\ucca9\uc73c\ub85c \ub4e4\uc5b4\uac00 \uc788\ub2e4. df\ub0b4 \ub2e4\uc2dc df \uadf8\ub798\uc11c \uad6c\ud558\uace0\uc790 \ud558\ub294 \uac12\uc740 dff\uc758 \uceec\ub7fc\uba85\uc774 '\uceec\ub7fc1'\uc778 \uac12\ub4e4\uc778 '\uceec\ub7fc2'\uc778 \uac12\ub4e4\uc744 \uacf1\ud55c\ub4a4 2\ubc88 \uc9f8 \uceec\ub7fc\uc5d0 \ud574\ub2f9\ud558\ub294 \uac12\uc744 \ubf51\uace0 \uadf8 \uceec\ub7fc\uba85\uc744 'GG'\ub77c\uace0 \ud558\uace0 \uc2f6\uc744\ub54c \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218 \uc788\ub2e4. data_gpa = (dff$'\uceec\ub7fc1' * dff$'\uceec\ub7fc2')[2] %>% setNames('GG') get data df[[1]] # \uccab \ubc88\uc9f8 \ud589 \uac12\ub4e4\uc744 \uc5bb\ub294\ub2e4. df[1] # c\uccab \ubc88\uc9f8 \uc5f4 \uac12\ub4e4\uc744 \uc5bb\ub294\ub2e4. df$'\uceec\ub7fc' # \uceec\ub7fc\uba85\uc774 '\uceec\ub7fc'\uc778 \uc5f4 \uac12\ub4e4\uc744 \uc5bb\ub294\ub2e4. df['\ud5891', '\uceec\ub7fc1'] # \ud589\uba85\uc774 '\ud5891'\uc774\uace0 \uceec\ub7fc\uba85\uc774 '\uceec\ub7fc1'\uc778 \ub370\uc774\ud130\ub97c \uc5bb\ub294\ub2e4. set data myData[['id']] = 'myId' myData[['name']] = 'you' df[[i]] = myData \ud2b9\uc815 \uac12\uc5d0 \ud574\ub2f9\ud558\ub294 \uceec\ub7fc index \ucc3e\uae30 \uceec\ub7fc \uc774\ub984 \uc911 \uc791\ub144\uc5d0 \ud574\ub2f9\ud558\ub294 \uceec\ub7fc index \ucc3e\uae30 num_col = str_which(colnames(df), as.character(lubridate::year(Sys.Date()) - 1)) for\ubb38 for (i in 1 : nrow(df)) { curr = data[[i]] } bind_rows dplyr \ud328\ud0a4\uc9c0\ub0b4 bind_rows\ub97c \uc0ac\uc6a9\ud558\uba74 \ud589 \uae30\uc900\uc73c\ub85c \ud569\uce60 \uc218 \uc788\ub2e4. \uc5f4\uc774 \ube44\uc5b4\uc788\uc73c\uba74 \uc790\ub3d9\uc801\uc73c\ub85c NA\ub97c \uc0bd\uc785\ud55c\ub2e4. library(dplyr) # > making 2 data.frame examples # > df_1 <- data.frame(x = 1:3, y = 1:3) # > df_2 <- data.frame(x = 4:6, y = 4:6) df_1 <- data.frame(x = 1:3, y = 1:3) df_2 <- data.frame(x = 4:6, y = 4:6) bind_rows(df_1, df_2) # x y # 1 1 1 # 2 2 2 # 3 3 3 # 4 4 4 # 5 5 5 # 6 6 6 order by column data # \uceec\ub7fc1 \ub370\uc774\ud130\ub85c DESC \uc815\ub82c # ASC \uc815\ub82c\uc744 \ud560\ub54c\ub294 - \ub97c \uc81c\uac70\ud558\uba74 \ub41c\ub2e4. # \uceec\ub7fc1, \uceec\ub7fc2\ub85c \uc815\ub82c\ud558\uace0 \uc2f6\ub2e4\uba74 order\uc548\uc5d0 \ud30c\ub77c\ubbf8\ud130\ub85c \ucd94\uac00\ud558\uba74 \ub41c\ub2e4. orderedSector = df[order(-df$'\uceec\ub7fc1'),] Lubidrate library(lubridate) get now month lubridate::month(Sys.Date()) Strings substring raw = 'X123456' substr(raw, 2, 7) # \uc778\ub371\uc2a4\uac00 1\ubd80\ud130 \ud574\uc11c 7\ubc88\uc9f8\uc5d0\uc11c \uba48\ucd98\ub2e4. # 123456 substr(raw, 1, 1) # X substr(raw, 0, 1) # X substr(raw, 0, 2) # X1 substr(raw, 0, 10) # X123456 get length nchar(myStr)","title":"CSV"},{"location":"r/#csv","text":"","title":"CSV"},{"location":"r/#read-csv","text":"my_csv = read.csv('my.csv', row.names = 1, stringsAsFactors = FALSE)","title":"Read CSV"},{"location":"r/#select-some-columns","text":"# select all rows and select some columns # (col1, col2, col3) my_csv = my_csv[, c('col1', 'col2', 'col3')]","title":"Select Some Columns"},{"location":"r/#dataframe","text":"","title":"DataFrame"},{"location":"r/#create-dataframe","text":"df <- c(1,2,3,4) \ub7ad\ud06c <- c('D', 'E', 'A', 'B') data.frame(df, \ub7ad\ud06c)","title":"Create DataFrame"},{"location":"r/#row","text":"nrow(df)","title":"row \uc218\ub97c \uc54c\uc544\ub0b4\ub294 \ubc29\ubc95"},{"location":"r/#bind-row-names-to-column-values","text":"# my_csv\uc758 rownames\ub97c \uceec\ub7fc \uac12\uc73c\ub85c \ub123\ub294\ub2e4. rownames(df) = df[['\ub7ad\ud06c']] # age name \ub7ad\ud06c # D 25 Tom D # E 34 Harry E # A 28 Porter A # B 52 Harden B \uceec\ub7fc \uac12\uc744 \uc774\uc6a9\ud558\ub294 \ub300\uc2e0 pad\ub3c4 \uc774\uc6a9\ud55c\ub2e4. \uc608\ub97c \ub4e4\uc5b4 value2\uc548\uc5d0 code \uceec\ub7fc\uc758 \uac12\uc774 0001, 00002, 000003 \ub77c\uace0 \ud558\uc790. \uc774\ub7f4 \ub54c \uc544\ub798\uc640 \uac19\uc774 \ud574\uc8fc\uba74 rownames\uc5d0\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \ucc44\uc6cc\uc9c4\ub2e4. 000001 000002 000003 str_pad\ub97c \uc774\uc6a9\ud574 \uc790\ub9ac\uc218\uac00 \ubd80\uc871\ud55c \ub9cc\ud07c 0\uc73c\ub85c \ucc44\uc6cc\uc8fc\ub294 \uac83\uc774\ub2e4 rownames(value2) = str_pad(value2$'code', 6, side=c('left'), pad='0')","title":"bind row names to column values"},{"location":"r/#transpose-df","text":"dataframe\uc758 \ud589\uacfc \uc5f4\uc744 \ubc14\uafc0\ub54c \uc0ac\uc6a9\ud55c\ub2e4. df %>% t()","title":"transpose df"},{"location":"r/#get-last-rows-of-df","text":"df\ub0b4\uc5d0\uc11c \ub9c8\uc9c0\ub9c9 \uc904 \ub9cc \uc5bb\uace0 \uc2f6\uc744 \ub54c \uc0ac\uc6a9\ud55c\ub2e4. rowLength = nrow(df) df[rowLength, ]","title":"get last rows of df"},{"location":"r/#change-column-name","text":"\uceec\ub7fc \uba85\uc744 \uc6d0\ud558\ub294 \uc774\ub984\uc73c\ub85c \ubc14\uafc0 \uc218 \uc788\ub2e4. df = df[, c('\uceec\ub7fc1', '\uceec\ub7fc2')] \ub610\ub294 column\uc774 1\uac1c\uc77c \uacbd\uc6b0 \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218\ub3c4 \uc788\ub2e4. colnames(df) = '\uceec\ub7fc' dff\uc5d0\ub294 dataframe \uc911\ucca9\uc73c\ub85c \ub4e4\uc5b4\uac00 \uc788\ub2e4. df\ub0b4 \ub2e4\uc2dc df \uadf8\ub798\uc11c \uad6c\ud558\uace0\uc790 \ud558\ub294 \uac12\uc740 dff\uc758 \uceec\ub7fc\uba85\uc774 '\uceec\ub7fc1'\uc778 \uac12\ub4e4\uc778 '\uceec\ub7fc2'\uc778 \uac12\ub4e4\uc744 \uacf1\ud55c\ub4a4 2\ubc88 \uc9f8 \uceec\ub7fc\uc5d0 \ud574\ub2f9\ud558\ub294 \uac12\uc744 \ubf51\uace0 \uadf8 \uceec\ub7fc\uba85\uc744 'GG'\ub77c\uace0 \ud558\uace0 \uc2f6\uc744\ub54c \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218 \uc788\ub2e4. data_gpa = (dff$'\uceec\ub7fc1' * dff$'\uceec\ub7fc2')[2] %>% setNames('GG')","title":"change column name"},{"location":"r/#get-data","text":"df[[1]] # \uccab \ubc88\uc9f8 \ud589 \uac12\ub4e4\uc744 \uc5bb\ub294\ub2e4. df[1] # c\uccab \ubc88\uc9f8 \uc5f4 \uac12\ub4e4\uc744 \uc5bb\ub294\ub2e4. df$'\uceec\ub7fc' # \uceec\ub7fc\uba85\uc774 '\uceec\ub7fc'\uc778 \uc5f4 \uac12\ub4e4\uc744 \uc5bb\ub294\ub2e4. df['\ud5891', '\uceec\ub7fc1'] # \ud589\uba85\uc774 '\ud5891'\uc774\uace0 \uceec\ub7fc\uba85\uc774 '\uceec\ub7fc1'\uc778 \ub370\uc774\ud130\ub97c \uc5bb\ub294\ub2e4.","title":"get data"},{"location":"r/#set-data","text":"myData[['id']] = 'myId' myData[['name']] = 'you' df[[i]] = myData","title":"set data"},{"location":"r/#index","text":"\uceec\ub7fc \uc774\ub984 \uc911 \uc791\ub144\uc5d0 \ud574\ub2f9\ud558\ub294 \uceec\ub7fc index \ucc3e\uae30 num_col = str_which(colnames(df), as.character(lubridate::year(Sys.Date()) - 1))","title":"\ud2b9\uc815 \uac12\uc5d0 \ud574\ub2f9\ud558\ub294 \uceec\ub7fc index \ucc3e\uae30"},{"location":"r/#for","text":"for (i in 1 : nrow(df)) { curr = data[[i]] }","title":"for\ubb38"},{"location":"r/#bind_rows","text":"dplyr \ud328\ud0a4\uc9c0\ub0b4 bind_rows\ub97c \uc0ac\uc6a9\ud558\uba74 \ud589 \uae30\uc900\uc73c\ub85c \ud569\uce60 \uc218 \uc788\ub2e4. \uc5f4\uc774 \ube44\uc5b4\uc788\uc73c\uba74 \uc790\ub3d9\uc801\uc73c\ub85c NA\ub97c \uc0bd\uc785\ud55c\ub2e4. library(dplyr) # > making 2 data.frame examples # > df_1 <- data.frame(x = 1:3, y = 1:3) # > df_2 <- data.frame(x = 4:6, y = 4:6) df_1 <- data.frame(x = 1:3, y = 1:3) df_2 <- data.frame(x = 4:6, y = 4:6) bind_rows(df_1, df_2) # x y # 1 1 1 # 2 2 2 # 3 3 3 # 4 4 4 # 5 5 5 # 6 6 6","title":"bind_rows"},{"location":"r/#order-by-column-data","text":"# \uceec\ub7fc1 \ub370\uc774\ud130\ub85c DESC \uc815\ub82c # ASC \uc815\ub82c\uc744 \ud560\ub54c\ub294 - \ub97c \uc81c\uac70\ud558\uba74 \ub41c\ub2e4. # \uceec\ub7fc1, \uceec\ub7fc2\ub85c \uc815\ub82c\ud558\uace0 \uc2f6\ub2e4\uba74 order\uc548\uc5d0 \ud30c\ub77c\ubbf8\ud130\ub85c \ucd94\uac00\ud558\uba74 \ub41c\ub2e4. orderedSector = df[order(-df$'\uceec\ub7fc1'),]","title":"order by column data"},{"location":"r/#lubidrate","text":"library(lubridate)","title":"Lubidrate"},{"location":"r/#get-now-month","text":"lubridate::month(Sys.Date())","title":"get now month"},{"location":"r/#strings","text":"","title":"Strings"},{"location":"r/#substring","text":"raw = 'X123456' substr(raw, 2, 7) # \uc778\ub371\uc2a4\uac00 1\ubd80\ud130 \ud574\uc11c 7\ubc88\uc9f8\uc5d0\uc11c \uba48\ucd98\ub2e4. # 123456 substr(raw, 1, 1) # X substr(raw, 0, 1) # X substr(raw, 0, 2) # X1 substr(raw, 0, 10) # X123456","title":"substring"},{"location":"r/#get-length","text":"nchar(myStr)","title":"get length"},{"location":"rust/01-start-rust/","text":"01-start-rust \uac1c\uc694 https://docs.microsoft.com/ko-kr/learn/modules/rust-get-started/ \uc5d0\uc11c \ubc30\uc6b4 \ub0b4\uc6a9\uc744 \uc815\ub9ac\ud55c \ubb38\uc11c. \uc124\uce58 https://rustup.rs/ \uc5d0\uc11c \uc6b4\uc601\uccb4\uc81c\uc5d0 \ub9de\uac8c \uc124\uce58\ud55c\ub2e4. \uc774\uc804\uc5d0 rustup\ub97c \uc124\uce58\ud55c \uacbd\uc6b0\uc5d0\ub294 \uba85\ub839 rustup update\ub97c \uc2e4\ud589\ud558\uc5ec \ucd5c\uc2e0 \uc548\uc815\ud654 \ubc84\uc804\uc758 Rust\ub97c \uc5c5\ub370\uc774\ud2b8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Rust\uc5d0\ub294 Visual Studio 2013 \uc774\uc0c1\uc6a9 Microsoft C++ \ube4c\ub4dc \ub3c4\uad6c\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. Rust\ub97c \uc124\uce58\ud558\ub824\uba74 \uc774\ub7ec\ud55c \ube4c\ub4dc \ub3c4\uad6c\ub97c \uc124\uce58\ud574\uc57c \ud569\ub2c8\ub2e4. \uc124\uce58\ud55c \ube4c\ub4dc \ub3c4\uad6c\uac00 \uc5c6\uc73c\uba74 \ub2e4\uc74c \ub2e8\uacc4\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. Microsoft Visual Studio \ub2e4\uc6b4\ub85c\ub4dc \ud398\uc774\uc9c0\ub85c \uc774\ub3d9\ud569\ub2c8\ub2e4. \ube4c\ub4dc \ub3c4\uad6c \ub2e4\uc6b4\ub85c\ub4dc \ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4. \ub2e4\uc6b4\ub85c\ub4dc\uac00 \uc644\ub8cc\ub418\uba74 \uc124\uce58 \uad00\ub9ac\uc790 \ud30c\uc77c\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4. Visual Studio \uc124\uce58 \uad00\ub9ac\uc790 \ucc3d\uc774 \uc5f4\ub9bd\ub2c8\ub2e4. \ud31d\uc5c5 \ub300\ud654 \uc0c1\uc790\uc5d0\uc11c \uc608 \ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4. \ub2e4\uc74c \ud31d\uc5c5 \ub300\ud654 \uc0c1\uc790\uc5d0\uc11c \uacc4\uc18d \uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. \uc124\uce58 \uad00\ub9ac\uc790 \ucc3d\uc758 \ub370\uc2a4\ud06c\ud1b1 \ubc0f \ubaa8\ubc14\uc77c \uc5d0\uc11c \uc67c\ucabd\uc5d0 \uc788\ub294 C++ \ube4c\ub4dc \ub3c4\uad6c \uc635\uc158\uc758 \ud655\uc778\ub780\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd\uc758 \uc124\uce58 \uc138\ubd80 \uc815\ubcf4 \ucc3d\uc5d0\uc11c \ub2e4\uc74c \uc635\uc158\uc774 \uc120\ud0dd\ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd \uc544\ub798\uc5d0\uc11c \uc124\uce58\ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4. \uc124\uce58 \ud655\uc778 rustc --version cargo --version Hello World \uc5f0\uc2b5 mkdir ~/rust-learning-path cd ~/rust-learning-path mkdir hello-world cd hello-world touch main.rs main.rs \ud3b8\uc9d1 fn main() { println!(\"Hello, world!\"); } \ucef4\ud30c\uc77c \ubc0f \uc2e4\ud589 rustc main.rs ./main Cargo rustc \ucef4\ud30c\uc77c\ub7ec\ub294 \uac04\ub2e8\ud55c \ud504\ub85c\uadf8\ub7a8\uc5d0 \uc801\ud569\ud558\uc9c0\ub9cc \uac70\uc758 \ubaa8\ub4e0 \ud504\ub85c\uc81d\ud2b8\uc5d0\uc11c Rust \ucef4\ud30c\uc77c\ub7ec\ub97c \uc9c1\uc811 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub300\uc2e0 Rust\uc758 \ube4c\ub4dc \ub3c4\uad6c\uc640 \uc885\uc18d\uc131 \uad00\ub9ac\uc790 Cargo\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc88b\uc740 \uc810\uc740 rustup\ub97c \uc124\uce58\ud558\uba74 Cargo\uc758 \uc548\uc815\uc801\uc778 \ucd5c\uc2e0 \ubc84\uc804\ub3c4 \uc5bb\uc744 \uc218 \uc788\ub2e4\ub294 \uc810\uc785\ub2c8\ub2e4. Cargo\ub294 \ub2e4\uc74c\uc744 \ube44\ub86f\ud55c \ub9ce\uc740 \uc791\uc5c5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. cargo new \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0c8 \ud504\ub85c\uc81d\ud2b8 \ud15c\ud50c\ub9bf\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. cargo build \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8\ub97c \ube4c\ub4dc\ud569\ub2c8\ub2e4. cargo run \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8\ub97c \ube4c\ub4dc\ud558\uace0 \uc2e4\ud589\ud569\ub2c8\ub2e4. cargo test \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8\ub97c \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4. cargo check \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8 \uc720\ud615\uc744 \ud655\uc778\ud569\ub2c8\ub2e4. cargo doc \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8\uc5d0 \ub300\ud55c \uc124\uba85\uc11c\ub97c \uc791\uc131\ud569\ub2c8\ub2e4. cargo publish \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec crates.io\uc5d0 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uac8c\uc2dc\ud569\ub2c8\ub2e4. Hello World - Cargo cargo new hello-cargo cd hello-cargo cargo run","title":"01-start-rust"},{"location":"rust/01-start-rust/#01-start-rust","text":"","title":"01-start-rust"},{"location":"rust/01-start-rust/#_1","text":"https://docs.microsoft.com/ko-kr/learn/modules/rust-get-started/ \uc5d0\uc11c \ubc30\uc6b4 \ub0b4\uc6a9\uc744 \uc815\ub9ac\ud55c \ubb38\uc11c.","title":"\uac1c\uc694"},{"location":"rust/01-start-rust/#_2","text":"https://rustup.rs/ \uc5d0\uc11c \uc6b4\uc601\uccb4\uc81c\uc5d0 \ub9de\uac8c \uc124\uce58\ud55c\ub2e4. \uc774\uc804\uc5d0 rustup\ub97c \uc124\uce58\ud55c \uacbd\uc6b0\uc5d0\ub294 \uba85\ub839 rustup update\ub97c \uc2e4\ud589\ud558\uc5ec \ucd5c\uc2e0 \uc548\uc815\ud654 \ubc84\uc804\uc758 Rust\ub97c \uc5c5\ub370\uc774\ud2b8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Rust\uc5d0\ub294 Visual Studio 2013 \uc774\uc0c1\uc6a9 Microsoft C++ \ube4c\ub4dc \ub3c4\uad6c\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. Rust\ub97c \uc124\uce58\ud558\ub824\uba74 \uc774\ub7ec\ud55c \ube4c\ub4dc \ub3c4\uad6c\ub97c \uc124\uce58\ud574\uc57c \ud569\ub2c8\ub2e4. \uc124\uce58\ud55c \ube4c\ub4dc \ub3c4\uad6c\uac00 \uc5c6\uc73c\uba74 \ub2e4\uc74c \ub2e8\uacc4\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. Microsoft Visual Studio \ub2e4\uc6b4\ub85c\ub4dc \ud398\uc774\uc9c0\ub85c \uc774\ub3d9\ud569\ub2c8\ub2e4. \ube4c\ub4dc \ub3c4\uad6c \ub2e4\uc6b4\ub85c\ub4dc \ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4. \ub2e4\uc6b4\ub85c\ub4dc\uac00 \uc644\ub8cc\ub418\uba74 \uc124\uce58 \uad00\ub9ac\uc790 \ud30c\uc77c\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4. Visual Studio \uc124\uce58 \uad00\ub9ac\uc790 \ucc3d\uc774 \uc5f4\ub9bd\ub2c8\ub2e4. \ud31d\uc5c5 \ub300\ud654 \uc0c1\uc790\uc5d0\uc11c \uc608 \ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4. \ub2e4\uc74c \ud31d\uc5c5 \ub300\ud654 \uc0c1\uc790\uc5d0\uc11c \uacc4\uc18d \uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. \uc124\uce58 \uad00\ub9ac\uc790 \ucc3d\uc758 \ub370\uc2a4\ud06c\ud1b1 \ubc0f \ubaa8\ubc14\uc77c \uc5d0\uc11c \uc67c\ucabd\uc5d0 \uc788\ub294 C++ \ube4c\ub4dc \ub3c4\uad6c \uc635\uc158\uc758 \ud655\uc778\ub780\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd\uc758 \uc124\uce58 \uc138\ubd80 \uc815\ubcf4 \ucc3d\uc5d0\uc11c \ub2e4\uc74c \uc635\uc158\uc774 \uc120\ud0dd\ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd \uc544\ub798\uc5d0\uc11c \uc124\uce58\ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4.","title":"\uc124\uce58"},{"location":"rust/01-start-rust/#_3","text":"rustc --version cargo --version","title":"\uc124\uce58 \ud655\uc778"},{"location":"rust/01-start-rust/#hello-world","text":"mkdir ~/rust-learning-path cd ~/rust-learning-path mkdir hello-world cd hello-world touch main.rs","title":"Hello World \uc5f0\uc2b5"},{"location":"rust/01-start-rust/#mainrs","text":"fn main() { println!(\"Hello, world!\"); }","title":"main.rs \ud3b8\uc9d1"},{"location":"rust/01-start-rust/#_4","text":"rustc main.rs ./main","title":"\ucef4\ud30c\uc77c \ubc0f \uc2e4\ud589"},{"location":"rust/01-start-rust/#cargo","text":"rustc \ucef4\ud30c\uc77c\ub7ec\ub294 \uac04\ub2e8\ud55c \ud504\ub85c\uadf8\ub7a8\uc5d0 \uc801\ud569\ud558\uc9c0\ub9cc \uac70\uc758 \ubaa8\ub4e0 \ud504\ub85c\uc81d\ud2b8\uc5d0\uc11c Rust \ucef4\ud30c\uc77c\ub7ec\ub97c \uc9c1\uc811 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub300\uc2e0 Rust\uc758 \ube4c\ub4dc \ub3c4\uad6c\uc640 \uc885\uc18d\uc131 \uad00\ub9ac\uc790 Cargo\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc88b\uc740 \uc810\uc740 rustup\ub97c \uc124\uce58\ud558\uba74 Cargo\uc758 \uc548\uc815\uc801\uc778 \ucd5c\uc2e0 \ubc84\uc804\ub3c4 \uc5bb\uc744 \uc218 \uc788\ub2e4\ub294 \uc810\uc785\ub2c8\ub2e4. Cargo\ub294 \ub2e4\uc74c\uc744 \ube44\ub86f\ud55c \ub9ce\uc740 \uc791\uc5c5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. cargo new \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0c8 \ud504\ub85c\uc81d\ud2b8 \ud15c\ud50c\ub9bf\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. cargo build \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8\ub97c \ube4c\ub4dc\ud569\ub2c8\ub2e4. cargo run \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8\ub97c \ube4c\ub4dc\ud558\uace0 \uc2e4\ud589\ud569\ub2c8\ub2e4. cargo test \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8\ub97c \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4. cargo check \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8 \uc720\ud615\uc744 \ud655\uc778\ud569\ub2c8\ub2e4. cargo doc \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8\uc5d0 \ub300\ud55c \uc124\uba85\uc11c\ub97c \uc791\uc131\ud569\ub2c8\ub2e4. cargo publish \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec crates.io\uc5d0 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uac8c\uc2dc\ud569\ub2c8\ub2e4.","title":"Cargo"},{"location":"rust/01-start-rust/#hello-world-cargo","text":"cargo new hello-cargo cd hello-cargo cargo run","title":"Hello World - Cargo"},{"location":"scala/","text":"Welcome","title":"Welcome"},{"location":"scala/#welcome","text":"","title":"Welcome"},{"location":"scala/bigdata-analysis-with-scala-and-spark/","text":"bigdata Week1 Why Scala? Why spark? \uc2a4\uce7c\ub77c\ub85c \uad6c\ud604\ub41c Spark\ub294 leveraging \ud568\uc73c\ub85c\uc11c scale up \ud558\uae30 \uc27d\uac8c \ud574\uc900\ub2e4. Spark\ub294 large-scale data processing framework\uc774\ub2e4. Hadoop\ub3c4 \uc88b\uc740 \uc120\ud0dd\uc774\ub2e4. Spark\ub294 more expressive \ud558\ub2e4. flatMap filter\ub4f1 \ub2e4\uc591\ud55c collection \ud568\uc218\ub97c \uc4f8 \uc218 \uc788\ub2e4. Performant \ud558\ub2e4. Good for data science. iteration\uc744 \uac00\ub2a5\ud558\uac8c \ud574\uc900\ub2e4. Hadoop\uc740 \uc774\ub97c \uad6c\ud604\ud558\uae30 \ud798\ub4e4\ub2e4. Distributed data parallelism\uc740 Shared Memory data parallelism\uacfc \ub2e4\ub974\ub2e4. Shared Memory Split the data Workers/threads independently operate on the data shards in parallel. Combine when done (if necessary) Distributed Split the data over several nodes. Nodes independently operate on the data shards in parallel. Combine when done (if necessary) \ud558\uc9c0\ub9cc network latency between workers\ub97c \uac71\uc815\ud574\uc57c \ud55c\ub2e4. Distributed data parallelism\uc744 \uc0ac\uc6a9\ud568\uc73c\ub85c\uc11c collections abstraction\uc744 \uc720\uc9c0\ud560 \uc218 \uc788\ub2e4. Apache Spark\ub294 distributed data-parallel programming\uc744 \uc704\ud55c framework\uc774\ub2e4. Spark\ub294 Resilient Distributed Datasets(RDDs)\ub77c\uace0 \ubd88\ub9ac\ub294 distributed data parallel model\uc744 implelments\ud55c\ub2e4. \uc774\uc804\uc5d0 \ubc30\uc6e0\ub358 parallel\ub4e4\uc740 1 machine \ub0b4\uc5d0\uc11c single multicore / multi-processor\ub97c \uc774\uc6a9\ud55c Data parallelism\uc774\uc5c8\ub2e4. \uc774\uc81c\ub294 \uc774\ub97c Multiple machine\uc73c\ub85c \ud655\uc7a5\ud55c\ub2e4. \uc774\ub85c \uc778\ud574 \uace0\ub824\ud574\uc57c \ud560 \uc0ac\ud56d\ub4e4\uc774 \uc788\ub2e4. Partial Failure Latency Spark\ub294 \uc774 2\uac00\uc9c0 \uc774\uc288\ub97c \uc798 \ud574\uacb0\ud574\uc900\ub2e4. Spark\ub294 fault-tolerance\uc640 handling latency\ub97c functional programming \ubc29\uc2dd\uc73c\ub85c \ud574\uacb0\ud55c\ub2e4. Idea : Keep all data immutable and in-memory. All operations on data are just functional transformations, like regular Scala collections. Fault tolerance is achieved by replaying functional transformations over original dataset. Result : Spark has been shown to be l00x more performant than Hadoop, while adding even more expressive APls. RDD\ub294 immutable sequential \ud639\uc740 parallel Scala collections\uc640 \uc720\uc0ac\ud558\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc740 Combinators\ub97c \uc81c\uacf5\ud55c\ub2e4. map flatMap filter reduce fold aggregate RDD \ub97c \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc740 \ub450 \uac00\uc9c0\uac00 \uc788\ub2e4. Transforming an existing RDD From a SparkContext (or SparkSession) object. Scala\uc758 collections\uc5d0\ub294 Transformer\uc640 Accessors\uac00 \uc788\ub2e4. Transformer : Return new collections as results e.g. map, filter, flatMap, groupBy map(f: A => B): Traversable[B] Accessor : Return single values as results e.g. reduce, fold, aggregate. reduce(op: (A, A) => A): A \uc774\uc640 \uc720\uc0ac\ud558\uac8c Spark\uc5d0\uc11c\ub294 RDD\uc5d0 Transformation\uacfc Actions\ub97c \uc815\uc758\ud55c\ub2e4. Transformations\ub294 lazy\ud558\uace0 actions\ub294 eager\ud558\ub2e4\ub294 \uac83\uc744 \ubc18\ub4dc\uc2dc \uae30\uc5b5\ud574\uc57c \ud55c\ub2e4. Transformations: Return new RDDs as results. They are lazy, their result RDD is not immediately computed. Actions: Compute a result based on an RDD, and either returned or saved to an external storage system (e.g. HDFS). They are eager, their result is immediately computed. \uc774\uac8c \uc911\uc694\ud55c \uc774\uc720\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. Laziness/eagerness is how we can limit network communication using the programming model. \uc544\ub798\uc758 \uc608\ub97c \uc0b4\ud3b4\ubcf4\uc790 \uc5ec\uae30\uc5d0\uc11c sc\ub294 SparkContext\ub97c \uc758\ubbf8\ud55c\ub2e4. val largelist: List[String] = ... val wordsRdd = sc.parallelize(largelist) val lengthsRdd = wordsRdd.map(_.length) val totalChars = lengthsRdd.reduce(_ + _) 1,2,3\ubc88 \uc9f8 \uc904\uc758 \ucf54\ub4dc\uae4c\uc9c0 \uc2e4\ud589\ud588\uc744 \ub54c cluster\uc5d0\ub294 \uc5b4\ub5a4 \ubcc0\ud654\uac00 \uc77c\uc5b4\ub0a0\uae4c? \ub2f5\uc740 \uc544\ubb34\uc77c\ub3c4 \uc77c\uc5b4\ub098\uc9c0 \uc54a\ub294\ub2e4 \uc774\ub2e4. \uc65c\ub0d0\ud558\uba74 \uc774 \uac83\ub4e4\uc740 \ubaa8\ub450 transfomrations\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. 4\ubc88\uc9f8 \uc904\uc758 \ucf54\ub4dc\uc5d0 \uac00\uc11c\uc57c actions\uac00 \uc788\uae30\uc5d0 \ud3c9\uac00\ub41c\ub2e4. Common Transformations in the Wild \uc544\ub798\uc5d0 \uc788\ub294 \ud568\uc218\ub4e4\uc740 \ubaa8\ub450 RDD\ub97c \ub9ac\ud134\ud558\ubbc0\ub85c transformation\uc774\ub2e4. = Lazy\ud558\ub2e4!! map: map[B](f: A => B): RDD[B] flatMap: flatMap[B](f: A => TraversableOnce[B]): RDD[B] filter: filter(pred: A => Boolean): RDD[A] distinct: distinct(): RDD[B] , \uc911\ubcf5\uc774 \uc81c\uac70\ub41c RDD\ub97c \ub9ac\ud134\ud55c\ub2e4. Common Actions in the Wild \ub9ac\ud134 \ud0c0\uc785\uc774 RDD \ud639\uc740 action\uc774\ub2e4. = Eager\ud558\ub2e4!! collect: collect(): Array[T] , RDD\uc758 \ubaa8\ub4e0 elements\ub97c \ub9ac\ud134\ud55c\ub2e4. count: count(): Long , RDD\uc758 elements \uac1c\uc218\ub97c \ub9ac\ud134\ud55c\ub2e4. take: take(num: Int): Array[T] , RDD \uc911 \uccab \ubc88\uc9f8\ubd80\ud130 num \uae4c\uc9c0\uc758 elements\ub97c \ub9ac\ud134\ud55c\ub2e4. reduce: reduce(op: (A, A) => A): A , RDD\uc758 elements\ub4e4\uc744 op \ud568\uc218\ub85c combine\ud558\uace0 \uadf8 \uacb0\uacfc\ub97c \ub9ac\ud134\ud55c\ub2e4. foreach: foreach(f: T => Unit): Unit , RDD\uc758 \uac01 element\uc5d0 function\uc744 \uc801\uc6a9\ud55c\ub2e4. RDD[String]\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4 gigabytes\uc758 log\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud558\ub2e4. \uadf8\ub807\ub2f4 2016\ub144 12\uc6d4\uc5d0 \ubc1c\uc0dd\ud55c ERROR \ub85c\uadf8\uc758 \uac1c\uc218\ub294 \uc5b4\ub5bb\uac8c \uce74\uc6b4\ud2b8 \ud560\uae4c? val lastYearslogs: RDD[String] = ... val numDecErrorlogs = lastYearslogs.filter( lg=> lg.contains(\"2016-12\") && lg.contains(\"error\") ) .count() \uc774\ub7f0 Laziness\ub85c \uc778\ud574 \uc5bb\uc744 \uc218 \uc788\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4. val lastYearslogs: RDD[String] = ... val firstlogsWithErrors = lastYearslogs.filter(_.contains(\"ERROR\")) .take(10) \uc704\uc640 \uac19\uc774 10\uac1c\ub9cc \ucde8\ud558\ub824 \ud560\ub54c \ubaa8\ub4e0 \ub370\uc774\ud130\ub97c \uc21c\ud68c\ud560 \ud544\uc694 \uc5c6\uc774 10\uac1c\ub97c \uc5bb\ub294 \uc21c\uac04 \uc885\ub8cc\ub97c \ud560 \uc218 \uc788\ub2e4. Transformations on Two RDDs RDDs also support set-like operations, like union and intersection. Two-RDD transformations combine two RDDs are combined into one. \ub9ac\ud134\ud0c0\uc785\uc774 \ud56d\uc0c1 RDD \uc774\ubbc0\ub85c Lazy\ud558\ub2e4! union: union(other: RDD[T]): RDD[T] intersection: intersection(other: RDD[T]): RDD[T] subtract: subtract(other: RDD[T]): RDD[T] cartesian: cartesian[U](other: RDD[U]): RDD[(T, U)] / \ub2e4\ub978 RDD\uc640\uc758 Cartesian product \ub97c \ub9ac\ud134\ud55c\ub2e4 Other Useful ROD Actions Scala collections\uc5d0\ub294 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294 Spark\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \ubd84\uc0b0 \ucc98\ub9ac\uc5d0 \uc7a5\uc810\uc774 \uc788\ub294 actions\ub3c4 \uc788\ub2e4 \uc544\ub798\uc758 \ud568\uc218\ub4e4\uc740 \ubc18\ud658\ud0c0\uc785\uc774 RDD\uac00 \uc544\ub2c8\uae30\ub54c\ubb38\uc5d0 actions\uc774\ub2e4. = Eager\ud558\ub2e4. takeSample: takeSample(withRepl: Boolean, num: Int): Array[T] / Return an array with a random sample of num elements of the dataset, with or without replacement. takeOrdered: takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] / Return the first n elements of the ROD using either their natural order or a custom comparator. saveAsTextFile: saveAsTextFile(path: String): Unit / Write the elements of the dataset as a text file in the local filesystem or HDFS. saveAsSequenceFile: saveAsSequenceFile(path: String): Unit / Write the elements of the dataset as a Hadoop Se\u00adquenceFile in the local filesystem or HDFS. Why is Spark Good for Data Science? \ub300\ubd80\ubd84\uc758 data science probles\ub294 iteration\uc744 \ud3ec\ud568\ud55c\ub2e4. \uc704 \uadf8\ub9bc\uc5d0\uc11c \uc704\ucabd\uc740 Hadoop, \uc544\ub798\ucabd\uc740 Spark\uc774\ub2e4. \ubcf4\uba74 \uc54c \uc218 \uc788\ub4ef\uc774 Iteration\uc774 \uc77c\uc5b4\ub0a0\ub54c\ub9c8\ub2e4 Hadoop\uc740 IO \ud0c0\uc784\uc774 \uc99d\uac00\ud558\ub294 \ubc18\uba74 Spark\uc5d0\uc11c\ub294 memory computation\uc73c\ub85c\uc368 IO time\uc744 \uc904\uc600\ub2e4. Iteration, Example: Logistic Regression Logistic regression\uc740 classification\uc744 \uc704\ud55c iterative algorithm\uc774\ub2e4. \ub2e4\ub978 classification algorithms\uc640 \uac19\uc774 classifier's weights\ub294 training dataset\uc5d0 \uae30\ubc18\ud558\uc5ec \ubc18\ubcf5\uc801\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8 \ub41c\ub2e4. val points = sc.textFile(...).map(parsePoint) var w = Vector.zeros(d) for (i <- 1 to numIterations) { val gradients = points.map { p => (1 / (1 + exp(-p.y * w.dot(p.x))) - 1) * p.y * p.y }.reduce(_ + _) w = -= alpha * gradient } RDD\ub294 \uae30\ubcf8\uc801\uc73c\ub85c action\uc744 run\ud560 \ub54c\ub9c8\ub2e4 recomputed \ub41c\ub2e4. \uc774\ub294 data\uac00 \ub9ce\uc544\uc9c8\uc218\ub85d expensive \ud574\uc9c4\ub2e4. Spark\ub294 memory\uc5d0 \uce90\uc2dc\ub97c \ucee8\ud2b8\ub864 \ud560 \uc218 \uc788\ub3c4\ub85d \uc9c0\uc6d0\ud574\uc900\ub2e4. RDD\ub97c \uba54\ubaa8\ub9ac\uc5d0 \uce90\uc2dc\ud558\uae30 \uc704\ud574\uc120 persist() \ud639\uc740 cache() \ub97c \uc0ac\uc6a9\ud558\uba74 \ub41c\ub2e4. val lastYearsLogs: RDD[String] = ... val logsWithErrors = lastYearsLogs.filter(_.contains(\"ERROR\")) .persist() val firstLogsWithErrors = logsWithErrors.take(10) val numErrors = logsWithErrors.count() \ub9cc\uc57d logsWithErrors\uc5d0 persist\ub97c \ucd94\uac00\ud558\uc9c0 \uc54a\uc558\ub354\ub77c\uba74(\uce90\uc2f1\ud558\uc9c0 \uc54a\uc558\ub354\ub77c\uba74) firstLogsWithErrors\ub97c \uacc4\uc0b0\ud560 \ub54c\uc640 numErrors\ub97c \uacc4\uc0b0\ud560 \ub54c \ucd1d 2\ubc88 \uacc4\uc0b0 \ud588\uc5b4\uc57c \ud560 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uce90\uc2f1\uc744 \ud588\uae30 \ub54c\ubb38\uc5d0 \ud55c \ubc88\uc758 logsWithErrors \uacc4\uc0b0\uc774 \uc774\ub8e8\uc5b4\uc9c4\ub2e4. \uc774\ub97c \uc704\uc5d0 Logsitic Regression\ucabd\uc5d0 \uc801\uc6a9\ud558\ub824\uba74 points \uc120\uc5b8 \ub9c8\uc9c0\ub9c9\uc5d0 .persist()\ub97c \ucd94\uac00\ud574\uc8fc\uba74 \ub41c\ub2e4. \ub370\uc774\ud130\ub97c persist\ud558\ub294 \ubc29\ubc95\uc740 \uc5ec\ub7ec \uac00\uc9c0\uac00 \uc788\ub2e4. in memory as regular Java objects on disk as regular Java objects in memory as serialized Java objects (more compact) on disk as serialized Java objects (more compact) both in memory and on disk (spill over to disk to avoid re-computation) cache() Shorthand for using the default storage level, which is in memory only as regular Java objects. persist() Persistence can be customized with this method. Pass the storage level you\u2019d like as a parameter to persist. Key takeway: Despite similar-looking API to Scala Collections, the deferred semantics of Spark's RDDs are very unlike Scala Collections. One of the most common performance bottlenecks of newcomers to Spark arises from unknowingly reevaluating several transformations when caching could be used. Spark\ub294 Lazy\ub97c \ubc1c\uacac\ud558\uba74 \uc5b4\ub5bb\uac8c \ud558\uba74 optimization\ud560 \uc218 \uc788\uc744\uc9c0 analyze\ud55c\ub2e4. Excerciese object WikipediaRanking extends WikipediaRankingInterface { val langs = List( \"JavaScript\", \"Java\", \"PHP\", \"Python\", \"C#\", \"C++\", \"Ruby\", \"CSS\", \"Objective-C\", \"Perl\", \"Scala\", \"Haskell\", \"MATLAB\", \"Clojure\", \"Groovy\") // true\ub85c \uac12\uc744 \uc124\uc815\ud55c\ub2e4\ub294 \uac83\uc740 local\ub85c \ub9cc\ub4e0\ub2e4\ub294 \uac83\uc774\ub2e4. // appName\uc744 \uc124\uc815\ud55c \uac83\uc740 SparkContext\ub97c \ub9cc\ub4e4\uae30 \uc704\ud568\uc774\ub2e4. val conf: SparkConf = new SparkConf(true).setAppName(\"myApp\") // local[2]\ub85c \ub9cc\ub4e0 \uac83\uc740 \ucc98\uc74c\uc5d0 sc\ub97c \ub9cc\ub4dc\ub294 \uacfc\uc815\uc5d0\uc11c \uc624\ub958\uac00 \ubc1c\uc0dd\ud588\uace0 \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 stackoverflow\uc5d0\uc11c \ucc3e\uc740 \ubc29\ubc95\uc774\ub2e4. // new SparkContext(conf)\ub85c \ud588\ub354\ub2c8 \uc624\ub958\uac00 \ubc1c\uc0dd\ud588\ub2e4. val sc: SparkContext = new SparkContext(\"local[2]\", \"myApp\", conf) // Hint: use a combination of `sc.parallelize`, `WikipediaData.lines` and `WikipediaData.parse` val wikiRdd: RDD[WikipediaArticle] = sc.parallelize(WikipediaData.lines.map(line => WikipediaData.parse(line))) /** Returns the number of articles on which the language `lang` occurs. * Hint1: consider using method `aggregate` on RDD[T]. * Hint2: consider using method `mentionsLanguage` on `WikipediaArticle` */ def occurrencesOfLang(lang: String, rdd: RDD[WikipediaArticle]): Int = rdd .aggregate(0)((b: Int,a: WikipediaArticle) => if (WikipediaArticle(a.title, a.text).mentionsLanguage(lang)) b + 1 else b , (b, a) => b + a) /* (1) Use `occurrencesOfLang` to compute the ranking of the languages * (`val langs`) by determining the number of Wikipedia articles that * mention each language at least once. Don't forget to sort the * languages by their occurrence, in decreasing order! * * Note: this operation is long-running. It can potentially run for * several seconds. */ def rankLangs(langs: List[String], rdd: RDD[WikipediaArticle]): List[(String, Int)] = langs.map(lang => (lang, occurrencesOfLang(lang, rdd))).sortWith(_._2 >_._2) /* Compute an inverted index of the set of articles, mapping each language * to the Wikipedia pages in which it occurs. */ def makeIndex(langs: List[String], rdd: RDD[WikipediaArticle]): RDD[(String, Iterable[WikipediaArticle])] = rdd.flatMap(rd => langs.filter(lang => rd.mentionsLanguage(lang)).map(lang => (lang, rd))).groupByKey() /* (2) Compute the language ranking again, but now using the inverted index. Can you notice * a performance improvement? * * Note: this operation is long-running. It can potentially run for * several seconds. */ def rankLangsUsingIndex(index: RDD[(String, Iterable[WikipediaArticle])]): List[(String, Int)] = { index.mapValues(d => d.size) .collect() .sortWith((before, after) => before._2 > after._2) .toList } /* (3) Use `reduceByKey` so that the computation of the index and the ranking are combined. * Can you notice an improvement in performance compared to measuring *both* the computation of the index * and the computation of the ranking? If so, can you think of a reason? * * Note: this operation is long-running. It can potentially run for * several seconds. */ def rankLangsReduceByKey(langs: List[String], rdd: RDD[WikipediaArticle]): List[(String, Int)] = rdd.flatMap(rd => for { lang <- langs if rd.mentionsLanguage(lang) } yield (lang, 1)) .reduceByKey((x, y) => x + y) .collect() .sortWith((before, after) => before._2 > after._2) .toList } Week2 distributed Spark\uc5d0\uc11c reduce \uac19\uc774 Reduction Operation\uc774\ub780 \ubb34\uc5c7\uc778\uac00? - foldLeft, reduceRight \ub4f1\uacfc \uac19\uc774 - collection\uc744 \uc21c\ud658\ud558\uba74\uc11c - neighboring element\ub97c combine \ud558\uba74\uc11c - collection together\ub97c \ud574\uc11c - single element result\ub97c \ub9cc\ub4e4\uc5b4 \ub0b4\ub294 \uac83\uc774\ub2e4. Spark\uc5d0 RDD\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\ub294 Reduction Operations\ub294 \uc544\ub798\uc640 \uac19\ub2e4. 1. fold 2. reduce 3. aggregate \ub9cc\uc57d reduction\uc744 \ud558\uba74\uc11c \ub9ac\ud134 \ud0c0\uc785\uc744 \ubc14\uafb8\uace0 \uc2f6\ub2e4\uba74 aggregate\ub97c \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4. foldLeft\uc640 foldRight\uc740 \ubd88\uac00\ub2a5\ud558\ub2e4. \ub9cc\uc57d \uc21c\ucc28\uc801\uc73c\ub85c \ucc98\ub9ac\ud574\uc11c \uc774\ub97c \uad6c\ud604\ud560 \uc218 \uc788\uc9c0 \uc54a\uc744\uae4c? \ub77c\ub294 \uc0dd\uac01\uc774 \ub4e4\uc218\ub3c4 \uc788\uc9c0\ub9cc distributed system\uc5d0\uc11c \uc5b4\ub5a4 \uc77c\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \ucc98\ub9ac \ud55c\ub2e4\ub294 \uac83\uc740 \ud798\ub4e4\ub2e4. aggregate\ub294 reduction operator\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ub41c\ub2e4. large-scale data\ub97c \ub2e4\ub8f0 \ub54c data type\uc744 \uc904\uc774\ub294 \uac83\uc740 \uc911\uc694\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. big data processing\uc5d0 \uc788\uc5b4 most common operation on data\ub294 key-value pairs \uc774\ub2e4. \uad6c\uae00\uc5d0\uc11c\ub294 key-value pairs\ub97c manipulating\ud558\uae30 \uc704\ud574 MapReduce \ub97c \ub514\uc790\uc778 \ud588\ub2e4. Spark\uc5d0\uc11c\ub294 distributed key-value pairs\ub97c Pair RDDs \ub77c\uace0 \ubd80\ub978\ub2e4. element type\uc774 pair\uc778 RDD\uac00 \ub9cc\ub4e4\uc5b4\uc9c0\uba74 Spark\ub294 \uc790\ub3d9\uc801\uc73c\ub85c \uc720\uc6a9\ud55c \uba87 \uac00\uc9c0 \uba54\uc18c\ub4dc\ub4e4\uc744 \ucd94\uac00\ud574\uc900\ub2e4. def groupByKey(): RDD[(K, Iterable[V])] def reduceByKey(func: (V, V) => V): RDD[(K, V)] def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] Pair RDD\ub97c \ub9cc\ub4dc\ub294 \uac00\uc7a5 \uc790\uc8fc\uc4f0\uc774\ub294 \ubc29\ubc95\uc740 \uc774\ubbf8 \uc874\uc7ac\ud558\ub294 non-pair RDD\ub85c \ubd80\ud130 \ub9cc\ub4dc\ub294 \uac83\uc774\ub2e4. RDD\uc5d0 map operation\uc744 \uc801\uc6a9\ud558\ub294 \uac83\uc774 \uc608\uac00 \ub420 \uc218 \uc788\ub2e4. val rdd: RDD[WikipediaPage] = // Has type: org.apache.spark.rdd.RDDE(String, String)] val pairRdd = rdd.map(page => (page.title, page.text)) \uc704\uc640 \uac19\uc774 pairRdd\ub97c \ub9cc\ub4e4\uba74 groupByKey, reduceByKey, join\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. Some interesting Pair RDDs operations Transformations(=Lazy) - groupByKey - reduceByKey - mapValues - keys - join - leftOuterJoin / rightOuterJoin Action(=Eager) - countByKey val ages = List(2, 52, 44, 23, 17, 14, 12, 82, 51 , 64) val grouped = ages.groupBy { age => if (age >= 18 && age < 65) \"adult\" else if (age < 18) \"child\" else \"senior\" } // grouped: scala.collection.immutable.MapEString,List[Int]] = // Map(senior -> List(82), adult -> List(52, 44, 23, 51, 64), // child -> List(2, 17, 14, 12)) gropuByKey \uc608\uc2dc) case class Event(organizer: String, name: String, budget: Int) val eventsRdd = sc.parallelize(...) .map(event => (event.organizer, event.budget)) val groupedRdd = eventsRdd.groupByKey() // TRICK QUESTION! As-is, it \"does\" nothing. It returns an unevaluated RDD (groupByKey\ub294 lazy\ud558\uae30 \ub54c\ubb38\uc774\ub2e4.) groupedRdd.collect().foreach(println) // (Prime Sound,CompactBuffer(42000)) reduceByKey \uc608\uc2dc) - reduceByKey\ub294 groupByKey\ub97c \ud558\uace0 reduce\ub97c \ud558\ub294 \uac83\ubcf4\ub2e4 more efficient\ud558\ub2e4. - key\uc5d0\ub294 \uad00\uc2ec\uc774 \uc5c6\uace0 value\uc5d0\ub9cc operate\ud55c\ub2e4. case class Event(organizer: String, name: String, budget: Int) val eventsRdd = sc.parallelize(...) .map(event => (event.organizer, event.budget)) val budgetsRdd = eventsRdd.reduceByKey(_+_) reducedRdd.collect().foreach(println) // (Prime Sound,42000) // (Sportorg,36400) // (Innotech,320000) // (Association Balelec,50000) mapValues\uc608\uc2dc) def mapValues[U](f: V => U): RDD[(K, U)] // \uc704\uc758 \uc815\uc758\ub294 \uc544\ub798\uc640 \uac19\uc774 \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4. rdd.map { case (x, y): (x, func(y)) } countByKey (def countByKey(): Map[K, Long]) simply counts the number of elements per key in a Pair RDD, returning a normal Scala Map (remember, it\u2019s an action!) mapping from keys to counts. val intermediate = eventsRdd.mapValues(b => (b, 1)) .reduceByKey((v1, v2) => (v1._1 + v2._1, v1._2 + v2._2)) // intermediate: RDD[(String, (Int, Int))] val avgBudgets = intermediate.mapValues { case (budget, numberOfEvents) => budget / numberOfEvents } avgBudgets.collect().foreach(println) // (Prime Sound,42000) // (Sportorg,12133) // (Innotech,106666) // (Association Balelec,50000) keys \uc608\uc2dc) - keys (def keys: RDD[K]) Return an RDD with the keys of each tuple. - Transformation\uc774\ub2e4. = RDD\ub97c \ub9ac\ud134\ud55c\ub2e4. case class Visitor(ip: String, timestamp: String, duration: String) val visits: RDD[Visitor] = sc.textfile(...) // keys\uc640 distinct\ub294 transformation\uc774\uae30 \ub54c\ubb38\uc5d0 count\ub97c \uc774\uc6a9\ud574 // \uacc4\uc0b0\uc744 \ud55c\ub2e4. val numllniqueVisits = visits.keys.distinct().count() // numllniqueVisits: Long = 3391 join\ub3c4 pari RDD\uc5d0\ub9cc \uc801\uc6a9\uac00\ub2a5\ud558\uae30 \ub54c\ubb38\uc5d0 normal RDD\uc5d0\ub294 \uc801\uc6a9 \ubd88\uac00\ub2a5\ud558\ub2e4. join\uc740 \ud55c \ub9c8\ub514\ub85c 2\uac1c\uc758 pair RDD\ub97c 1\uac1c\uc758 pair RDD\ub85c \ud569\uce58\ub294 \uac83\uc774\ub2e4. val as = List( (101, (\"Ruetli\", AG)), (102, (\"Brelaz\", DemiTarif)), (103, (\"Gress\", DemiTarifVisa)), (104, (\"Schatten\", DemiTarif))) val abos = sc.parallelize(as) val ls = List( (101, \"Bern\"), (101, \"Thun\"), (102, \"Lausanne\"), (102, \"Geneve\"), (102, \"Nyon\"), (103, \"Zurich\"), (103, \"St-Gallen\"), (103, \"Chur\") ) val locations = sc.parallelize(ls) inner join Inner joins return a new RDD containing combined pairs whose keys are present in both input RDDs. val trackedCustomers = abos.join(locations) // trackedCustomers: RDD[(Int, ((String, Abonnement), String))] trackedCustomers.collect().foreach(println) // (101,((Ruetli,AG),Bern)) // (101,((Ruetli,AG),Thun)) // (102,((Brelaz,DemiTarif),Nyon)) // (102,((Brelaz,DemiTarif),Lausanne)) // (102,((Brelaz,DemiTarif),Geneve)) // (103,((Gress,DemiTarifVisa),St-Gallen)) // (103,((Gress,DemiTarifVisa),Chur)) // (103,((Gress,DemiTarifVisa),Zurich)) \uc704\uc758 \uc608\ub97c \ubcf4\uba74 as\uc5d0\uc11c 104\uac00 \uc0ac\ub77c\uc84c\ub2e4. location\uc5d0\ub294 key 104\uac00 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4. Outer join Outer joins return a new RDD containing combined pairs whose keys don\u2019t have to be present in both input RDDs. def leftOuterJoinEW](other: RDD[(K, W)]): RDD[(K, (V, OptionEW]))] def rightOuterJoinEW](other: RDD[(K, W)]): RDD[(K, (Option[V], W))] val abosWithOptionalLocations = abos.leftOuterJoin(locations) abosWithOptionalLocations.collect().foreach(println) // (101,((Ruetli,AG),Some(Thun))) // (101,((Ruetli,AG),Some(Bern))) // (102,((Brelaz,DemiTarif),Some(Geneve))) // (102,((Brelaz,DemiTarif),Some(Nyon))) // (102,((Brelaz,DemiTarif),Some(Lausanne))) // (103,((Gress,DemiTarifVisa),Some(Zurich))) // (103,((Gress,DemiTarifVisa),Some(St-Gallen))) // (103,((Gress,DemiTarifVisa),Some(Chur))) // (104,((Schatten,DemiTarif),None)) val customersWithLocationDataAndOptionalAbos = abos.rightOuterJoin(locations) // RDD[(Int, (Option[(String, Abonnement)], String))] customersWithLocationDataAndOptionalAbos.collect().foreach(println) // (101,(Some((Ruetli,AG)),Bern)) // (101,(Some((Ruetli,AG)),Thun)) // (102,(Some((Brelaz,DemiTarif)),Lausanne)) // (102,(Some((Brelaz,DemiTarif)),Geneve)) // (102,(Some((Brelaz,DemiTarif)),Nyon)) // (103,(Some((Gress,DemiTarifVisa)),Zurich)) // (103,(Some((Gress,DemiTarifVisa)),St-Gallen)) // (103,(Some((Gress,DemiTarifVisa))\uff0cChur)) Week3 Remember our data is distributed! val pairs = sc.parallelize( List( (1, \"one\"), (2, \"two\"), (3, \"three\") ) ) pairs.groupByKey() // res2: org.apache.spark.rdd.RDD[(Int, Iterable[String])] // = ShuffledRDD[16] at groupByKey at <console>:37 We typically have to move data from one node to another to be \"grouped with\" its key. Doing this is called \"shuffling\". Shuffles can be an enormous hit to because it means that Spark must send data from one node to another. Why? Latency! Goal: calculate how many trips, and how much money was spent by each individual customer over the course of the month. val purchasesRdd: RDD[CFFPurchaseJ = sc.textFile(...) // Returns: Array[(Int, (Int, Double))] val purchasesPerMonth = purchasesRdd .map(p => (p.customerld, p.price)) // Pair RDD .groupByKey() // groupByKey returns RDD[(K, Iterable[VJ)J .map(p => (p._1, (p._2.size, p._2.sum))) .collect() \uc704\uc758 \uadf8\ub9bc\uc5d0\uc11c \ub178\ub780\uc0c9 \uc601\uc5ed\uc774 shuffle\uc774 \uc77c\uc5b4\ub098\ub294 \uacf3\uc774\ub2e4. \uc544\ub798\ucc98\ub7fc reduceByKey\ub85c optimize\ud560 \uc218 \uc788\ub2e4. val purchasesPerMonth = purchasesRdd .map(p => (p.customerld, (1, p.price))) // Pair ROD .reduceByKey((v1, v2) => (v1._1 + v2._1, v1._2 + v2._2)) .collect() \uc704 \uadf8\ub9bc\ucc98\ub7fc network\ub97c \ud1b5\ud558\uae30 \uc804\uc5d0 \uba3c\uc800 reduce\ub97c \ud574\uc11c \ub354 \ud6a8\uc728\uc801\uc774\ub2e4.(\ub354 \uc801\uac8c \uc8fc\uace0\ubc1b\uae30 \ub54c\ubb38) \uadf8\ub807\ub2e4\uba74 Spark\ub294 \uc5b4\ub5bb\uac8c data\ub97c partitioning(\ud544\uc694\ud55c machine\uc5d0 \ub370\uc774\ud130\ub97c \ubcf4\ub0b4\ub294 \uac83) \ud560\uae4c? \ub2e4\uc74c \ubd80\ud130 \uc54c\uc544\ubcf4\uc790 Partitioning\uc740 key base\uc774\uae30 \ub54c\ubb38\uc5d0 pair RDD \uc5d0\uc11c\ub9cc \ub3d9\uc791\ud55c\ub2e4. number of partitions\ub294 configurable\ud558\uba70 \uae30\ubcf8\uc740 total number of cores on all executor nodes\uc774\ub2e4. Spark\uc5d0\uc11c \uac00\ub2a5\ud55c partitioning\uc740 2\uac00\uc9c0\uac00 \uc788\ub2e4 1. Hash partitioning 2. Range partitioning Hash partitioning val purchasesPerCust = purchasesRdd .map(p => (p.customerld, p.price)) // Pair RDD .groupByKey() groupByKey first computes per tuple (k, v) its partition p: p = k.hashCode() % numPartitions Then, all tuples in the same partition p are sent to the machine hosting p. hash partitioning attempts to spread data evenly across partitions based on the key. Range partitioning Pair RDDs may contain keys that have an ordering defined. For such RDDs, range partitioning may be more efficient. Using a range partitioner, keys are partitioned according to: an ordering for keys a set of sorted ranges of keys Property: tuples with keys in the same range appear on the same machine. Consider a Pair RDD, with keys [8, 96, 240, 400, 401, 800], and a desired number of partitions of 4. Furthermore, suppose that hashCode() is the identity (n.hashCode() == n). p = n.hashCode() % numPartitions => p = n % numPartitions \uc640 \uac19\uc740 \uacb0\ub860\uc774 \ub41c\ub2e4. \uadf8\ub807\uae30\uc5d0 \uc774\ub97c \uc801\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ubd84\uc0b0\ub41c\ub2e4. partition 0: [8, 96, 240, 400, 800] partition 1: [401] partition 2: [] partition 3: [] The result is a very unbalanced distribution which hurts performance. Using range partitioning the distribution can be improved significantly: - Assumptions: (a) keys non-negative, (b) 800 is biggest key in the RDD. - Set of ranges: [1 , 200], [201 , 400], [401 , 600], [601 , 800] In this case, range partitioning distributes the keys as follows among the partitions: - partition 0: [8, 96] - partition 1: [240, 400] - partition 2: [401J - partition 3: [800] The resulting partitioning is much more balanced How do we set a partitioning for our data? There are two ways to create RDDs with specific partitionings: Call partitionBy on an RDD, providing an explicit Partitioner. Using transformations that return RDDs with specific partitioners. val pairs = purchasesRdd.map(p => (p.customerld, p.price)) val tunedPartitioner = new RangePartitioner(8, pairs) val partitioned = pairs.partitionBy(tunedPartitioner).persist() Important: the result of partitionBy should be persisted. Otherwise, the partitioning is repeatedly applied (involving shuffling!) each time the partitioned RDD is used. Partitioner from parent RDD: Pair RDDs that are the result of a transformation on a partitioned Pair RDD typically is configured to use the hash partitioner that was used to construct it. Automatically-set partitioners: Some operations on RDDs automatically result in an RDD with a known partitioner - for when it makes sense. For example, by default, when using sortByKey, a RangePartitioner is used. Further, the default partitioner when using groupByKey, is a HashPartitioner, as we saw earlier. Operations on Pair RDDs that hold to (and propagate) a partitioner: - cogroup - groupWith - join - leftOuterJoin - rightOuterJoin - groupByKey - reduceByKey - foldByKey - combineByKey - partitionBy - sort - mapValues (if parent has a partitioner) - flatMapValues (if parent has a partitioner) - filter (if parent has a partitioner) All other operations will produce a result without a partitioner. map\uc740 \uc65c lose the partitioner\ud560\uae4c? \uc65c\ub0d0\ud558\uba74 \ud0a4\ub97c \ubc14\uafc0\uc218\ub3c4 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. rdd.map((k: String, v: Int)=> (\"doh!\" v)) \ud558\uc9c0\ub9cc mapValues\ub294 \ud0a4\ub97c \ubc14\uafb8\uc9c0 \uc54a\ub294\ub2e4. = partitioner\uac00 \uc720\uc9c0\ub41c\ub2e4. => Pair RDD\ub97c \ub2e4\ub8f0 \ub550 mapValues\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \ucd5c\uc6b0\uc120 \uace0\ub824\ud574\ub77c. userData\uac00 \ud070 \ub370\uc774\ud130\uc774\uace0 events\uac00 \uc791\uc740 \ub370\uc774\ud130 \uc77c \ub54c \uc544\ub798 \ucf54\ub4dc\ub97c \uc0b4\ud3b4\ubcf4\uc790. val sc = new SparkContext(...) val userData = sc.sequenceFile[UserID, Userlnfo](\"hdfs://...\").persist() def processNewlogs(logFileName: String) { val events = sc.sequenceFile[UserID, Linklnfo](logFileName) val joined = userData.join(events) //ROD of (UserID, (Userlnfo, Linklnfo)) val offTopicVisits = joined.filter { case (userld, (userlnfo, linklnfo)) => //Expand the tuple !userlnfo.topics.contains(linklnfo.topic) }.count() println(''Number of visits to non-subscribed topics: '' + offTopicVisits) } \uc704\uc640 \uac19\uc740 \ucf54\ub4dc\uac00 \uc788\uc744\ub54c processNewlogs\ub97c \uc2e4\ud589\ud560 \ub54c\ub9c8\ub2e4 join\uc774 \ubd88\ub9ac\uae30 \ub54c\ubb38\uc5d0 keys\uac00 dataset\uc5d0 \uc5b4\ub5bb\uac8c partitioned \ub418\uc5b4\uc788\ub294\uc9c0 \uc54c \uc218 \uc5c6\ub2e4. => \ube44\ud6a8\uc728\uc801\uc774\ub2e4. By default, this operation will hash all the keys of both datasets, sending elements with the same key hash across the network to the same machine, and then join together the elements with the same key on that machine. Even though userData doesn't change! \uadf8\ub798\uc11c \uc544\ub798\uc640 \uac19\uc774 \uc218\uc815\ud55c\ub2e4. val userData = sc.sequenceFile[UserID, Userlnfo](\"hdfs://...\") .partitionBy(new HashPartitioner(100)) // Create 100 partitions .persist() Since we called partitionBy when building userData, Spark will now know that it is hash-partitioned, and calls to join on it will take advantage of this information. In particular, when we call userData.join(events), Spark will shuffle only the events ROD, sending events with each particular UserID to the machine that contains the corresponding hash partition of userData You can also figure out whether a shuffle has been planned/executed via: The return type of certain transformations, e.g., org.apache.spark.rdd.RDD[(String, Int)]= ShuffledRDD[366J Using function toDebugString to see its execution plan: partitioned .reduceByKey((v1, v2) => (v1._1 + v2._1, v1._2 + v2._2)) .toDebugString // res9: String= (8) MapPartitionsRDD[l6221J at reduceByKey at <console>:49 [] shuffle\uc744 \uc720\ubc1c\ud558\ub294 operations - cogroup - groupWith - join - leftOuterJoin - rightOuterJoin - groupByKey - reduceByKey - combineByKey - d istinct - intersection - repartition - coalesce There are a few ways to use operations that might cause a shuffle and to still avoid much or all network shuffling. reduceByKey running on a pre-partitioned ROD will cause the values to be computed locally, requiring only the final reduced value has to be sent from the worker to the driver. join called on two RDDs that are pre-partitioned with the same partitioner and cached on the same machine will cause the join to be computed locally, with no shuffling across the network. How your data is organized on the cluster, and what operations you're doing with it matters! Rule of thumb: a shuffle can occur when the resulting RDD depends on other elements from the same RDD or another RDD. In fact, RDD dependencies encode when data must move across the network. Narrow Dependencies Each partition of the parent RDD is used by at most one partition of the child RDD. Fast! No shuffle necessary. Optimizations like pipelining possible. Wide Dependencies Each partition of the parent RDD may be depended on by multiple child partitions. Slow! Requires all or some data to be shuffled over the network. \uc5b4\ub5bb\uac8c dependencies\ub97c \ubcfc \uc218 \uc788\uc744\uae4c? dependencies method on RDDs. dependencies returns a sequence of Dependency objects, which are actually the dependencies used by Spark\u2019s sched\ub098ler to know how this RDD depends on other RDDs. val wordsRdd = sc.parallelize(largeList) val pairs = wordsRdd.map(c => (c, 1)) .groupByKey() .dependencies // pairs: SeqEorg.apache.spark.Dependency // List(org.apache.spark.ShuffleDependency@4294a23d) Narrow dependency objects: \u25ba OneToOneDependency \u25ba PruneDependency \u25ba RangeDependency Wide dependency objects: \u25ba ShuffleDependency 2\ubc88\uc9f8 \ubc29\ubc95 toDebugString method on RDDs. toDebugString prints out a visualization of the RDD\u2019s lineage, and other information pertinent to scheduling. For example, indentations in the output separate groups of narrow transformations that may be pipelined together with wide transformations that require shuffles. These groupings are called stages. val wordsRdd = sc.parallelize(largeList) val pairs = wordsRdd.map(c => (c, 1)) .groupByKey() .toDebugString //pairs: String = //(8) ShuffledRDD[219] at groupByKey at <console>:38 [] // +-(8) MapPartitionsRDD[218] at map at <console>:37 [] // | ParallelCollectionRDD[217] at parallelize at <console>:36 [] Recomputing missing partitions fast for narrow dependencies. But slow for wide dependencies! Weel4 \ub611\uac19\uc740 \uacb0\uacfc\uac00 \ub098\uc624\ub294 \ucf54\ub4dc\uc774\ub354\ub77c\ub3c4 \uc5b4\ub5bb\uac8c \uc218\ud589\ud558\ub290\ub0d0\uc5d0 \ub530\ub77c runtime\uc774 \ud655\uc5f0\ud558\uac8c \ucc28\uc774\ub09c\ub2e4. Case1: innerjoin first demographics.join(finances).filter { p => p._2._1.country == \"Switzerland\" && p._2._2.hasFinancialDependents && p._2._2.hasDebt }.count Case2: filter first val filtered = finances.filter(p => p._2.hasFinancialDependents && p._2.hasDebt) demographics.filter(p => p._2.country == \"Switzerland\") .join(filtered) .count Case3: Cartesian Product val cartesian = demographics.cartesian(finances) cartesian.filter { case (pl, p2) => p1._1 == p2._l } .filter { case (pl, p2) => (pl._2.country == \"Switzerland\") && (p2._2.hasFinancialDependents) && (p2._2.hasDebt) }.count \uc2e4\ud589 \uc2dc\uac04\uc740 \uc5b4\ub5bb\uac8c \ub420\uae4c? 150,000 people\uc5d0 \ub300\ud574\uc11c \uacc4\uc0b0\uc744 \ud588\uc744 \ub54c 1\ubc88\uc740 4.97\ucd08 2\ubc88\uc740 1.35\ucd08\uac00 \uac78\ub838\ub2e4. \ubc18\uba74 3\ubc88\uc740 \uc57d 4\ubd84\uc774 \uac78\ub838\ub2e4. \uadf8\ub9cc\ud07c Cartesian Product\ub294 \uc131\ub2a5\uc774 \uc88b\uc9c0 \uc54a\ub2e4. \ub530\ub77c\uc11c \uac00\ub2a5\ud558\uba74 Cartesian Product\ub294 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294 \uac83\uc774 \uc88b\ub2e4. \uadf8\ub807\ub2e4\uba74 \uac1c\ubc1c\uc790\ub294 \ub9e4\ubc88 \uc774\ub7f0 \uc131\ub2a5\uc744 \uc608\uc0c1\ud558\uba74\uc11c \ucf54\ub529\uc744 \ud574\uc57c \ud560\uae4c? => DataType\uc5d0 \ub530\ub77c \ub2e4\ub974\ub2e4. \uc6b0\ub9ac\uac00 \uadf8\uac04 \ub2e4\ub904\uc654\ub358 \ub370\uc774\ud130 \ud0c0\uc785\ub4e4\uc740 unstructured data\uc600\ub2e4. text or json, xml ... \uc774 \uacbd\uc6b0\uc5d4 Spark\uac00 \ub0b4\ubd80\ub97c \ub4e4\uc5ec\ub2e4 \ubcf4\uae30 \ud798\ub4e4\uae30\uc5d0 optimization\uc774 \uc548\ub41c\ub2e4. \ud558\uc9c0\ub9cc DB\uc640 \uac19\uc774 structured data\uc758 \uacbd\uc6b0\uc5d0\ub294 Spark\uac00 \ub370\uc774\ud130 \uad6c\uc870\ub97c \uc54c \uc218 \uc788\uace0 \uc608\uce21\ud560 \uc218 \uc788\uae30\uc5d0 aggressive optimization\uc774 \uac00\ub2a5\ud558\ub2e4. => Spark SQL \uc774 \uc774\ub97c \uac00\ub2a5\ud558\uac8c \ud574\uc900\ub2e4.! Spark SQL Spark SQL\uc758 goal\uc740 1. Spark programs(on RDD)\uc640 external data source with a friendly API \uac04\uc758 relational processing\uc744 \uc9c0\uc6d0\ud558\ub294 \uac83\uc774\ub2e4. 2. database\ub97c research\ud558\ub294\ub370 high performance\ub97c \ubc1c\ud718\ud55c\ub2e4. 3. semi-structured data\uc640 external databases\ub97c easily support \ud55c\ub2e4. Spark SQL\uc740 Spark\uc758 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc911 \ud558\ub098\ub85c\uc368 3\uac1c\uc758 main API\uac00 \uc788\ub2e4. - SQL literal syntax - DataFrames - Datasets \ub610\ud55c 2\uac1c\uc758 \ud2b9\ubcc4\ud55c backend component\uac00 \uc788\ub2e4. - Catalyst, query optimizer - Tungsten, off-heap serializer Relational Queries(SQL) Everything about SQL is structured. In fact, SQL stands for structural query language. There are a set of fixed data types. Int, Long, String, etc. There are fixed set of operations. SELECT, WHERE, GROUP BY, etc. Research and industry surrounding relational databases has focused on exploiting this rigidness to get all kinds of performance speedups. DataFrame is Spark SQL\u2019s core abstraction. (Conceptually equivalent to a table in a relational database.) DataFrames are, conceptually, RDDs full of records with a known schema . Unlike RDDs though, DataFrames require some kind of schema info! DataFrames are untyped! That is, the Scala compiler doesn\u2019t check the types in its schema! ( RDD[T] \ub85c \uc4f0\ub294 \ubc18\uba74 DataFrame \uc740 \ub4a4\uc5d0 \ud0c0\uc785\uc774 \uc548\ubd99\ub294\ub2e4.) DataFrames contain Rows which can contain any schema. Transformations on DataFrames are also known as untyped transformations SparkSession To get started using Spark SQL, everything starts with the SparkSession import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(\"My App\") //.config(\"spark.some.config.option\", \"some-value\") .getOrCreate() Creating DataFrames DataFrames can be created in two ways: From an existing RDD. Either with schema inference, or with an explicit schema. Reading in a specific data source from file. Common structured or semi-structured formats such as JSON. (1a) Create DataFrame from RDD, schema reflectively inferred val tupleRDD = ...// Assume RDD[(Int, String String, String)] val tupleDF = tupleRDD.toDF(\"id\", \"name\", \"city\", \"country\") // column names Note: ifyou use toDF without arguments, Spark will assign numbers as attributes (column names) to your DataFrame. If you already have an RDD containing some kind of case class instance, then Spark can infer the attributes from the case class's fields. case class Person(id: Int, name: String, city: String) val peopleRDD =...// Assume RDD[Person] val peopleDF = peopleRDD.toDF (1b) Create DataFrame from existing RDD, schema explicitly specified case class Person(name: String, age: Int) val peopleRdd = sc.textFile(...) // Assume RDD[Person] \uc704\uc758 \ucf54\ub4dc\ub97c manually build up a schema, dataFrame\uc744 \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc740 \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218 \uc788\ub2e4. // The schema is encoded in a string val schemastring = \"name age\" // Generate the schema based on the string of schema val fields = schemastring.split(\" \") .map(fieldName => StructField(fieldName, StringType, nullable = true)) val schema = StructType(fields) // Convert records of the RDD (people) to Rows val rowRDD = peopleRDD .map(_.split(\",\")) .map(attributes => Row(attributes(0), attributes(1).trim)) // Apply the schema to the RDD val peopleDF = spark.createDataFname(rowRDD, schema) (2) Create DataFrame by reading in a data source from file. Using the SparkSession object, you can read in semi-structured/structured data by using the read method. For example, to read in data and infer a schema from a JSON file: // 'spark' is the SparkSession object we created a few slides back val df = spark.read.json(\"examples/src/main/resources/people.json\") Semi-structured/Structured data sources Spark SQL can directly create DataFrames from: - JSON - CSV - Parquet - JDBC To see a list of all available methods for directly reading in semi-structured/structured data, see the latest API docs for DataFrameReader: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader SQL Literals DataFrame\uc774 \ub9cc\ub4e4\uc5b4 \uc84c\uc73c\uba74 \uc774\uc81c dataset\uc5d0 SQL syntax\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. A DataFrame called peopleDF, we just have to register our DataFrame as a temporary SQL view first: // Register the DataFrame as a SQL temporary view peopleDF.createOrReplaceTempView(\"people\") // This essentially gives a name to our DataFrame in SQL // so we can refer to it in an SQL FROM statement // SQL literals can be passed to Spark SQL's sql method val adultsDF = spark.sql(\"SELECT * FROM people WHERE age > 17\") The SQL statements available to you are largely what's available in HiveQL. This includes standard SQL statements such as: SELECT FROM WHERE COUNT HAVING GROUP BY ORDER BY SORT BY DISTINCT JOIN (LEFT|RIGHT|FULL) OUTER JOIN Subqueries: SELECT col FROM ( SELECT a + b AS col from t1) t2 Supported Spark SQL syntax: https://docs.datastax.eom/en/datastax_enterprise/4.6/datastax_enterprise/spark/sparkSqlSupportedSyntax.html For a HiveQL cheatsheet: https://hortonworks.com/blog/hive-cheat-sheet-for-sql-users/ For an updated list of supported Hive features in Spark SQL, the official Spark SQL docs enumerate: https://spark.apache.org/docs/latest/sql-programming-guide.html#supported-hive-features \ub354\uc6b1 \ud765\ubbf8\ub85c\uc6b4 \uc608\uc81c\ub97c \ud55c\ubc88 \uc0b4\ud3b4 \ubcf4\uc790 case class Employee(id: Int, fname: String, Iname: String, age: Int, city: String) // DataFrame with schema defined in Employee case class val employeeDF = sc.parallelize(...).toDF val sydneyEmployeesDF = spark.sql(\"\"\"SELECT id, lname FROM employees WHERE city = \"Sydney\" ORDER BY id\"\"\") DataFrames Spark's RDDs\uc640 \uad00\ub828\ub41c API\ub97c \uc81c\uacf5\ud55c\ub2e4 \uc790\ub3d9\uc801\uc73c\ub85c aggresively optimized \ud560 \uc218 \uc788\ub2e4. untyped \uc774\ub2e4. = scala compiler\uac00 type check\ub97c \ud558\uc9c0 \uc54a\ub294\ub2e4. show() pretty-prints DataFrame in tabular form. Shows first 20 elements. case class Employee(id: Int, fname: String, lname: String, age: Int, city: String) val employeeDF = sc.parallelize(...).toDF employeeDF.show( ) //+---+-----+-------+---+--------+ // | id|fname| |name|age| city| | // +---+-----+-------+---+--------+ // | 12| Joe| Smith| 38|New York| // |563|Sally| Owens| 48|New York| // |645|Slate|Markham| 28| Sydney| // |221|David| Walker| 21| Sydney| // +---+-----+-------+---+--------+ printSchema() prints the schema of your DataFrame in a tree format. case class Employee(id: Int, fname: String, lname: String, age: Int, city: String) val employeeDF = sc.parallelize(...).toDF employeeDF.printschema( ) // root // |-- id: integer (nullable = true) // |-- fname: string (nullable = true) // |-- lname: string (nullable = true) // |-- age: integer (nullable = true) // |-- city: string (nullable = true) def select(col: String, cols: String*): DataFrame // selects a set of named columns and returns a new DataFrame with these // columns as a result. def agg(expr: Column, exprs: Column*): DataFrame // performs aggregations on a series of columns and returns a new DataFrame // with the calculated output. def groupBy(col1: String, cols: String*): DataFrame // simplified // groups the DataFrame using the specified columns. Intended to be used before an aggregation. def join(right: DataFrame): DataFrame // simplified // inner join with another DataFrame Other transformations include: filter, limit, orderBy, where, as, sort, union, drop, amongst others. You can select and work with columns in three ways: 1. Using $-notation // $-notation requires: import spark.implicits._ df.filter($\"age\" > 18) 2. Referring to the Dataframe df.filter(df(\"age\")> 18)) 3. Using SQL query string df.filter(\"age > 18\") case class Employee(id: Int, fname: String, lname: String, age: Int, city: String) val employeeDF = sc.parallelize(...).toDF val sydneyEmployeesDF = employeeDF.select(\"id\", \"lname\") .where(\"city == 'Sydney'\") .orderBy( \"id\") // sydneyEmployeesDF: // +---+-------+ // | id| lname| // |221| Walker| // |645|Markham| // +---+-------+ \uc544\ub798 \ub450 \ucf54\ub4dc\uc758 \uacb0\uacfc\ub294 \ub3d9\uc77c\ud558\ub2e4. val over30 = employeeDF.filter(\"age > 30\").show() val over30 = employeeDF.where(\"age > 30\").show() employeeDF.filter(($\"age\" > 25) && ($\"city\" === \"Sydney\")).show() groupBy function\uc740 RelationalGroupedDataset \uc744 \ub9ac\ud134\ud55c\ub2e4. \uc774\ub294 count, sum, max, min, avg \uac19\uc740 \ud568\uc218\ub4e4\uc744 \ud3ec\ud568\ud55c\ub2e4. df.groupBy($\"attribute1\") .agg(sum($\"attribute2\")) df.groupBy($\"attribute1\") .count($\"attribute2\") case class Listing(street: String, zip: Int, price: Int) val listingsDF = . . . // DataFrame of Listings import org.apache.spark.sql.functions._ val mostExpensiveDF = listingsDF.groupBy($\"zip\") .max(\"price\") val leastExpensiveDF = listingsDF.groupBy($\"zip\") .min( \"price\") case class Post( authorID: Int, subforum: String, likes: Int, date: String) val postsDF = ... // DataFrame of Posts val rankedDF = postsDF.groupBy($\"authorID\", $\"subforum\") .agg(count($\"authorID\")) // new DF with columns authorID, subforum, count(authorID) .orderBy($\"subforum\", $\"count(authorID)\".desc) // postsDF: // +--------+--------+-----+----+ // |authorID|subforum|likes|datel // +--------+--------+-----+----+ // 1| design| 2| | // 1| debate| 0| | // 2| debate| 0| | // 3| debate| 23| | // 1| design| 1| | // 1| design| 0| | // 2| design| 0| | // 2| debate| 0| | // +--------+--------+-----+----+ // rankedDF: // +--------+--------+---------------+ // |authorID|subforum|count(authorID)| // +--------+--------+---------------+ // | 2| debate| 2| // | 1| debate| 1| // | 3| debate| 1| // | 1| design| 3| // | 2| design| 1| // +--------+--------+---------------+ Sometimes you may have a data set with null or NaN values. In these cases it's often desirable to do one of the following: drop rows/records with unwanted values like null or \"NaN\" replace certain values with a constant Dropping records with unwanted values: - drop() drops rows that contain null or NaN values in any column and returns a new DataFrame. - drop(\"all\") drops rows that contain null or NaN values in all columns and returns a new DataFrame. - drop(Array(\"id\", \"name\")) drops rows that contain null or NaN values in the specified columns and returns a new DataFrame. Replacing unwanted values: - fill(0) replaces all occurrences of null or NaN in numeric columns with specified value and returns a new DataFrame. - fill(Map(\"minBalance\" -> 0)) replaces all occurrences of null or NaN in specified column with specified value and returns a new DataFrame. - replace(Array(\"id\"),Map(1234 -> 8923)) replaces specified value (1234)in specified column (id) with specified replacement value (8923)and returns a new DataFrame. Like RDDs, DataFrames also have their own set of actions. collect(): Array[Row] Returns an array that contains all of Rows in this DataFrame. count(): Long Returns the number of rows in the DataFrame. first(): Row/head(): Row Returns the first row in the DataFrame. show(): Unit Displays the top 20 rows of DataFrame in a tabular form. take(n: Int): Array[Row] Returns the first n rows in the DataFrame. Joins on DataFrames are similar to those on Pair RDDs, with the one major usage difference that, since DataFrames aren\u2019t key/value pairs, we have to specify which columns we should join on. Several types of joins are available: - inner, outer, left_outer, right_outer, leftsemi. Performing joins: Given two DataFrames, df1 and df2 each with a column/attribute called id, we can perform an inner join as follows: df1.join(df2, $\"df1.id\" === $\"df2.id\") It\u2019s possible to change the join type by passing an additional string parameter to join specifying which type of join to perform. E.g., df1.join(df2, $\"df1.id\" $\"df2.id\", \"right_outer\") case class Abo(id: Int, v: (String, String)) case class Loc(id: Int, v: String) val as = List(Abo(101, (\"Ruetli\", \"AG\")), Abo(102, (\"Brelaz\", \"DemiTarif\")), Abo(103, (\"Gress\", \"DemiTarifVisa\")), Abo(104, (\"Schatten\", \"DemiTarif\"))) val abosDF = sc.parallelize(as).toDF val ls = List(Loc(101, \"Bern\"), Loc(101, \"Thun\"), Loc(102, \"Lausanne\"), Loc(102, \"Geneve\"), Loc(102, \"Nyon\"), Loc(103, \"Zurich\"), Loc(103, \"St-Gallen\"), Loc(103, \"Chur\")) val locationsDF = sc.parallelize(ls).toDF // abosDF: // +-\u2014+----------------------+ // |id| v| // +-\u2014+----------------------+ // |101| [Ruetli, AG]| // |102| [Brelaz, DemiTarif]| // |103|[Gress, DemiTarifv...| // |104|[Schatten, DemiTarif]| // +-\u2014+-------------------+ // locationsDF: // +---+---------+ // |id| v| // |101| Bern| // |101| Thun| // |102| Lausanne| // |102| Geneve| // |102| Nyon| // |103| Zurich| // |103|St-Gallen| // |103| Chur| // +---+---------+ val abosDF = sc.parallelize(as).toDF val locationsDF = sc.parallelize(ls).toDF val trackedCustomersDF = abosDF.join(locationsDF, abosDF(\"id\") === locationsDF(\"id\")) // trackedCustomersDF: // +---+--------------------+---+---------+ // | id| v| id| v| // +---+--------------------+---+---------+ // |101| [Ruetli,AG]|101| Bern| // |101| [Ruetli,AG]|101| Thun| // |103|[Gress,DemiTarifV...|103| Zurich| // |103|[Gress,DemiTarifV...|103|St-Gallen| // |103|[Gress,DemiTarifV...|103| Chur| // |102| [Brelaz,DemiTarif]|102| Lausanne| // |102| [Brelaz,DemiTarif]|102| Geneve| // |102| [Brelaz,DemiTarif]|102| Nyon| // +---+----------------------+---+---------+ val abosWithOptionalLocationsDF = abosDF.join(locationsDF, abosDF(\"id\") === locationsDF(\"id\"), \"left_outer\") // abosWithOptionalLocationsDF: // +---+--------------------+----+---------+ // | id| v| id| v| // +---+--------------------+----+---------+ // |101| [Ruetli,AG]| 101| Bern| // |101| [Ruetli,AG]| 101| Thun| // |103|[Gress,DemiTarifV...| 103| Zurich| // |103|[Gress,DemiTarifV...| 103|St-Gallen| // |103|[Gress,DemiTarifV...| 103| Chur| // |102| [Brelaz,DemiTarif]| 102| Lausanne| // |102| [Brelaz,DemiTarif]| 102| Geneve| // |102| [Brelaz,DemiTarif]| 102| Nyon| // |104|[Schatten,DemiTarif]|null| Nyon| // +---+----------------------+---+---------+ As expected, customer 104 has returned! :-) case class Demographic(id: Int, age: Int, codingBootcamp: Boolean, country: String, gender: String, isEthnicMinority: Boolean, servedlnMilitary: Boolean) val demographicsDF = sc.textfile(...).toDF // DataFrame of Demographic case class Finances(id: Int, hasDebt: Boolean, hasFinancialDependents: Boolean, hasStudentLoans: Boolean, income: Int) val financesDF = sc.textfile(...).toDF // DataFrame of Finances demographicsDF.join(financesDF, demographicsDF(\"ID\") === financesDF(\"ID\"), \"inner\") .filter($\"HasDebt\" && $\"HasFinancialDependents\") .filter($\"CountryLive\" === \"Switzerland\") .count \uc704\uc758 join\uacfc filter\uac00 \ubd99\uc740 \ucf54\ub4dc\ub97c \ubcf4\uba74 \ub9cc\uc57d df\uac00 \uc544\ub2cc \uc77c\ubc18 \ub370\uc774\ud130\uc600\uc744 \uacbd\uc6b0 join\uc744 \uba3c\uc800 \ud558\ub0d0 filter\ub97c \uba3c\uc800 \ud558\ub0d0\uc5d0 \ub530\ub77c \uc131\ub2a5\ucc28\uc774\uac00 \ud06c\uac8c \ub2ec\ub77c\uc9c4\ub2e4. \ud558\uc9c0\ub9cc df\uc5d0\uc11c\ub294 \uc790\ub3d9\uc801\uc73c\ub85c optimization\ud574\uc8fc\uae30 \ub54c\ubb38\uc5d0 \uc774\uc5d0 \ub300\ud574 \uac71\uc815\ud560 \ud544\uc694\uac00 \uc5c6\ub2e4. How is this possible? Recall that Spark SQL comes with two specialized backend components: Catalyst, query optimizer. Tungsten, off-heap serializer. Limitations of DataFrames Untyped! \ucc3e\uace0\uc790 \ud558\ub294 column\uc774 \uc5c6\uc744 \uc2dc compile\uc740 \ub418\uc9c0\ub9cc \uc2e4\ud589\ud574\ubcf4\uba74 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Limited Data Types If your data can\u2019t be expressed by case classes/Products and standard Spark SQL data types, it may be difficult to ensure that a Tungsten encoder exists for your data type. E.g., you have an application which already uses some kind of complicated regular Scala class. Requires Semi-Structured/Structured Data \ub9cc\uc57d your unstructured data\uac00 schema\ub97c \uac00\uc9c4 \ud615\ud0dc\ub85c reformulated\ub420 \uc218 \uc5c6\ub2e4\uba74 RDDs\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ub354 \uc88b\ub2e4. case class Listing(street: String, zip: Int, price: Int) val listingsDF = ... // DataFrame of Listings import org.apache.spark.sql.functions._ val averagePricesDF = listingsDF.groupBy($\"zip\") .avg(\"price\") // averagePrices: Array[org.apache.spark.sql.Row] averagePrices.head.schema.printTreeString() // root // 1-- zip: integer (nullable = true) // 1-- avg(price): double (nullable = true) val averagePricesAgain = averagePrices.map { row => (row(0).aslnstanceOf[Int], row(1).aslnstanceOf[Double]) } // mostExpensiveAgain: Array[(Int, Double)] DataFrames are actually Datasets. type DataFrame = DataSet[Row] Datasets can be thought of as typed distributed collections of data. Dataset API unifies the DataFrame and RDD APIs. Mix and match! Datasets require strucutred/semi-structured data. Schemas and Encoders core part of Datasets. // We can freely mix APls! listingsDS.groupByKey(l => l.zip) // looks like groupByKey on RDDs! .agg(avg($\"price\").as[Double]) // looks like our DataFrame operators! Datasets Datasets are a something in the middle between DataFrames and RDDs - You can still use relational DataFrame operations as we learned in previous sessions on Datasets. - Datasets add more typed operations that can be used as well. - Datasets let you use higher-order functions like map, flatMap, filter again! Datasets can be used when you want a mix of functional and relational transformations while benefiting from some of the optimizations on DataFrames. And we've almost got a type safe API as well. Creating Datasets From a DataFrame. Just use the toDS convenience method. myDF.toDS // requires import spark.implicits._ Note that often it's desirable to read in data from JSON from a file, which can be done with the read method on the SparkSession object like we saw in previous sessions, and then converted to a Dataset: val myDS = spark.read.json(\"people.json\").as[Person] From an RDD. Just use the toDS convenience method. myRDD.toDS // requires import spark.implicits._ From common Scala types. Just use the toDS convenience method. List(\"yay\", \"ohnoes\", \"hooray!\").toDS // requires import spark.implicits._ Recall the Column type from DataFrames. On Datasets, typed operations tend to act on TypedColumn instead. Remember untyped transformations from DataFrames? The Dataset API includes both untyped and typed transformations. untyped transformations the transformations we learned on DataFrames. typed transformations typed variants of many DataFrame transformations + additional transformations such as RDD-like higher-order functions map, flatMap, etc. These APls are integrated. You can call a map on a DataFrame and get back a Dataset, for example. - Caveat: not every operation you know from RDDs are available on Datasets, and not all operations look 100% the same on Datasets as they did on RDDs. But remember, you may have to explicitly provide type information when going from a DataFrame to a Dataset via typed transformations. val keyValuesDF = List((3, \"Me\"), (1, \"Thi\"), (2, \"Se\"), (3, \"ssa\") (3, \"-\")), (2, \"cre\") (2, \"t\")).toDF val res = keyValuesDF.map(row => row(0).asinstanceOf[Int] + 1) // Ew... Common (Typed) Transformation on Datasets map map U : Dataset[U] flatMap flatMap U : Dataset[U] filter filter(pred: T => Bool ean): Dataset[T] distinct distinct(): Dataset[T] groupByKey groupByKey K : KeyValueGroupedDataset[K, T] coalesce coalesce(numPartitions: Int): Dataset[T] Apply a function to each element in the Dataset and return a Dataset of the contents of the iterators returned. repartition repartition(numPartitions: Int): Dataset[T] Apply predicate function to each element in the Dataset and return a Dataset of elements that have passed the predicate condition, pred. Grouped Operations on Datasets Like on DataFrames, Datasets have a special set of aggregation operations meant to be used after a call to groupByKey on a Dataset. calling groupByKey on a Dataset returns a KeyValueGroupedDataset KeyValueGroupedDatasetcontains a number of aggregation operations which return Datasets How to group & aggregate on Datasets? 1. Call groupByKey on a Dataset, get back a KeyValueGroupedDataset. 2. Use an aggregation operation on KeyValueGroupedDataset (return Datasets) Some KeyValueGroupedDataset Aggregation Operations reduceGroups reduceGroups(f: (V, V) => V): Dataset[(K, V)] Reduces the elements of each group of data using the spec\u00adified binary function. The given function must be commutative and associative or the result may be non-deterministic. agg agg U : Dataset[(K, U)] Computes the given aggregation, returning a Dataset of tuples for each unique key and the result of computing this aggregation over all elements in the group. someDS.agg(avg($\"column\")) // error! untyped regular Column\uc744 TypedColumn\uc73c\ub85c \ubcc0\uacbd\ud574\uc918\uc57c \ud55c\ub2e4. someDS.agg(avg($\"column\").as[Double]) mapGroups mapGroups U : Dataset[U] Applies the given function to each group of data. For each unique group, the function will be passed the group key and an iterator that contains all of the elements in the group. The function can return an element of arbitrary type which wi11 be returned as a new Dataset. flatMapGroups flatMapGroups U : Dataset[U] Applies the given function to each group of data. For each unique group, the function will be passed the group key and an iterator that contains all of the elements in the group. The function can return an iterator containing elements of an arbitrary type which will be returned as a new Dataset. Dataset API\ub97c \uc0b4\ud3b4\ubcf4\uba74 RDD\uc5d0\ub294 \uc788\uc9c0\ub9cc \uc5ec\uae30\uc5d4 reduceByKey\uac00 \uc5c6\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. val keyValues = List((3,\"Me\"),(1 ,\"Thi\"),(2,\"Se\"),(3,\"ssa\"),(1 ,\"sisA\"),(3,\"ge:\"),(3,\"-)\"),(2,\"ere\"),(2,\"t\")) val keyValuesDS = keyValues.toDS keyValuesDS.groupByKey(p => p._1) .mapGroups((k, vs) => (k, vs.foldleft(\"\")((acc, p) => ace + p._2))) // +---+----------+ // | _1| _2| // +---+----------+ // | 1| ThisisA| // | 3|Message:-)| // | 2| Secret| // +---+----------+ keyValuesDS.groupByKey(p => p._1) .mapValues(p => p._2) .reduceGroups((acc, str) => ace + str) Aggregators A class that helps you generically aggregate data. Kind of like the aggregate method we saw on RDDs. class Aggregator[-IN, BUF, OUT] IN is the input type to the aggregator. When using an aggregator after groupByKey, this is the type that represents the value in the key/value pair. BUF is the intermediate type during aggregation. OUT is the type of the output of the aggregation . This is how implement our own Aggregator: val myAgg = new Aggregator[IN, BUF, OUT] { def zero: BUF = ... // The initial value. def reduce(b: BUF, a: IN): BUF = ... // Add an element to the running total def merge(b1 : BUF, b2: BUF): BUF = ... // Merge intermediate values. def finish(b: BUF): OUT = ... // Return the final result. }.toColumn val keyValues = List((3,\"Me\"),(1 ,\"Thi\"),(2,\"Se\"),(3,\"ssa\"),(1 ,\"sisA\"),(3,\"ge:\"),(3,\"-)\"),(2,\"ere\"),(2,\"t\")) val keyValuesDS = keyValues.toDS val strConcat = new Aggregator[(Int, String) , String, String] { def zero: String = \"\" def reduce(b: String, a: (Int, String)): String = b + a._2 def merge(bl : String, b2: String) : String = bl + b2 def finish(r: String) : String = r override def bufferEncoder: Encoder[String] = Encoders.STRING override def outputEncoder: Encoder[String] = Encoders.STRING }.toColumn keyValuesDS.groupByKey(pair => pair._1) .agg(strConcat.as[String]).show // +------+--------------------+ // | value|anon$1(scala.Tuple2)| // +------+--------------------+ // | 1| ThisisA| // | 3| Message:-)| // | 2| Secret| // +------+--------------------+ Encoders Encoders are what convert your data between JVM objects and Spark SQL's specialized internal (tabular) representation. They're required by all Datasets! Common Dataset Actions collect(): Array[T] Returns an array that contains all of Rows in this Dataset. count(): Long Returns the number of rows in the Dataset. first(): T/head(): T Returns the first row in this Dataset. foreach(f: T => Unit): Unit Applies a function f to all rows. reduce(f: (T, T) => T): T Reduces the elements of this Dataset using the specified binary function. show(): Unit Displays the top 20 rows of Dataset in a tabular form. take(n: Int): Array[T] Returns the first n rows in the Dataset. Limitations of Datasets Catalyst Can't Optimize All Operations Limited Data Types Requires Semi-Structured/Structured Data When to use Datasets vs DataFrames vs RDDs? Use Datasets when... - you have structured / semi-structured data - you want typesafety - you need to work with functional APls - you need good performance, but it doesn't have to be the best Use DataFrames when... - you have structured / semi-structured data - you want the best possible performance, automatically optimized for you Use RDDs when... - you have unstructured data - you need to fine-tune and manage low-level details of ROD computations - you have complex data types that cannot be serialized with Encoders","title":"Coursera/bigdata-analysis-spark"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#bigdata","text":"","title":"bigdata"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#week1","text":"","title":"Week1"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#why-scala-why-spark","text":"\uc2a4\uce7c\ub77c\ub85c \uad6c\ud604\ub41c Spark\ub294 leveraging \ud568\uc73c\ub85c\uc11c scale up \ud558\uae30 \uc27d\uac8c \ud574\uc900\ub2e4. Spark\ub294 large-scale data processing framework\uc774\ub2e4. Hadoop\ub3c4 \uc88b\uc740 \uc120\ud0dd\uc774\ub2e4. Spark\ub294 more expressive \ud558\ub2e4. flatMap filter\ub4f1 \ub2e4\uc591\ud55c collection \ud568\uc218\ub97c \uc4f8 \uc218 \uc788\ub2e4. Performant \ud558\ub2e4. Good for data science. iteration\uc744 \uac00\ub2a5\ud558\uac8c \ud574\uc900\ub2e4. Hadoop\uc740 \uc774\ub97c \uad6c\ud604\ud558\uae30 \ud798\ub4e4\ub2e4. Distributed data parallelism\uc740 Shared Memory data parallelism\uacfc \ub2e4\ub974\ub2e4. Shared Memory Split the data Workers/threads independently operate on the data shards in parallel. Combine when done (if necessary) Distributed Split the data over several nodes. Nodes independently operate on the data shards in parallel. Combine when done (if necessary) \ud558\uc9c0\ub9cc network latency between workers\ub97c \uac71\uc815\ud574\uc57c \ud55c\ub2e4. Distributed data parallelism\uc744 \uc0ac\uc6a9\ud568\uc73c\ub85c\uc11c collections abstraction\uc744 \uc720\uc9c0\ud560 \uc218 \uc788\ub2e4. Apache Spark\ub294 distributed data-parallel programming\uc744 \uc704\ud55c framework\uc774\ub2e4. Spark\ub294 Resilient Distributed Datasets(RDDs)\ub77c\uace0 \ubd88\ub9ac\ub294 distributed data parallel model\uc744 implelments\ud55c\ub2e4. \uc774\uc804\uc5d0 \ubc30\uc6e0\ub358 parallel\ub4e4\uc740 1 machine \ub0b4\uc5d0\uc11c single multicore / multi-processor\ub97c \uc774\uc6a9\ud55c Data parallelism\uc774\uc5c8\ub2e4. \uc774\uc81c\ub294 \uc774\ub97c Multiple machine\uc73c\ub85c \ud655\uc7a5\ud55c\ub2e4. \uc774\ub85c \uc778\ud574 \uace0\ub824\ud574\uc57c \ud560 \uc0ac\ud56d\ub4e4\uc774 \uc788\ub2e4. Partial Failure Latency Spark\ub294 \uc774 2\uac00\uc9c0 \uc774\uc288\ub97c \uc798 \ud574\uacb0\ud574\uc900\ub2e4. Spark\ub294 fault-tolerance\uc640 handling latency\ub97c functional programming \ubc29\uc2dd\uc73c\ub85c \ud574\uacb0\ud55c\ub2e4. Idea : Keep all data immutable and in-memory. All operations on data are just functional transformations, like regular Scala collections. Fault tolerance is achieved by replaying functional transformations over original dataset. Result : Spark has been shown to be l00x more performant than Hadoop, while adding even more expressive APls. RDD\ub294 immutable sequential \ud639\uc740 parallel Scala collections\uc640 \uc720\uc0ac\ud558\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc740 Combinators\ub97c \uc81c\uacf5\ud55c\ub2e4. map flatMap filter reduce fold aggregate RDD \ub97c \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc740 \ub450 \uac00\uc9c0\uac00 \uc788\ub2e4. Transforming an existing RDD From a SparkContext (or SparkSession) object. Scala\uc758 collections\uc5d0\ub294 Transformer\uc640 Accessors\uac00 \uc788\ub2e4. Transformer : Return new collections as results e.g. map, filter, flatMap, groupBy map(f: A => B): Traversable[B] Accessor : Return single values as results e.g. reduce, fold, aggregate. reduce(op: (A, A) => A): A \uc774\uc640 \uc720\uc0ac\ud558\uac8c Spark\uc5d0\uc11c\ub294 RDD\uc5d0 Transformation\uacfc Actions\ub97c \uc815\uc758\ud55c\ub2e4. Transformations\ub294 lazy\ud558\uace0 actions\ub294 eager\ud558\ub2e4\ub294 \uac83\uc744 \ubc18\ub4dc\uc2dc \uae30\uc5b5\ud574\uc57c \ud55c\ub2e4. Transformations: Return new RDDs as results. They are lazy, their result RDD is not immediately computed. Actions: Compute a result based on an RDD, and either returned or saved to an external storage system (e.g. HDFS). They are eager, their result is immediately computed. \uc774\uac8c \uc911\uc694\ud55c \uc774\uc720\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. Laziness/eagerness is how we can limit network communication using the programming model. \uc544\ub798\uc758 \uc608\ub97c \uc0b4\ud3b4\ubcf4\uc790 \uc5ec\uae30\uc5d0\uc11c sc\ub294 SparkContext\ub97c \uc758\ubbf8\ud55c\ub2e4. val largelist: List[String] = ... val wordsRdd = sc.parallelize(largelist) val lengthsRdd = wordsRdd.map(_.length) val totalChars = lengthsRdd.reduce(_ + _) 1,2,3\ubc88 \uc9f8 \uc904\uc758 \ucf54\ub4dc\uae4c\uc9c0 \uc2e4\ud589\ud588\uc744 \ub54c cluster\uc5d0\ub294 \uc5b4\ub5a4 \ubcc0\ud654\uac00 \uc77c\uc5b4\ub0a0\uae4c? \ub2f5\uc740 \uc544\ubb34\uc77c\ub3c4 \uc77c\uc5b4\ub098\uc9c0 \uc54a\ub294\ub2e4 \uc774\ub2e4. \uc65c\ub0d0\ud558\uba74 \uc774 \uac83\ub4e4\uc740 \ubaa8\ub450 transfomrations\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. 4\ubc88\uc9f8 \uc904\uc758 \ucf54\ub4dc\uc5d0 \uac00\uc11c\uc57c actions\uac00 \uc788\uae30\uc5d0 \ud3c9\uac00\ub41c\ub2e4.","title":"Why Scala? Why spark?"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#common-transformations-in-the-wild","text":"\uc544\ub798\uc5d0 \uc788\ub294 \ud568\uc218\ub4e4\uc740 \ubaa8\ub450 RDD\ub97c \ub9ac\ud134\ud558\ubbc0\ub85c transformation\uc774\ub2e4. = Lazy\ud558\ub2e4!! map: map[B](f: A => B): RDD[B] flatMap: flatMap[B](f: A => TraversableOnce[B]): RDD[B] filter: filter(pred: A => Boolean): RDD[A] distinct: distinct(): RDD[B] , \uc911\ubcf5\uc774 \uc81c\uac70\ub41c RDD\ub97c \ub9ac\ud134\ud55c\ub2e4.","title":"Common Transformations in the Wild"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#common-actions-in-the-wild","text":"\ub9ac\ud134 \ud0c0\uc785\uc774 RDD \ud639\uc740 action\uc774\ub2e4. = Eager\ud558\ub2e4!! collect: collect(): Array[T] , RDD\uc758 \ubaa8\ub4e0 elements\ub97c \ub9ac\ud134\ud55c\ub2e4. count: count(): Long , RDD\uc758 elements \uac1c\uc218\ub97c \ub9ac\ud134\ud55c\ub2e4. take: take(num: Int): Array[T] , RDD \uc911 \uccab \ubc88\uc9f8\ubd80\ud130 num \uae4c\uc9c0\uc758 elements\ub97c \ub9ac\ud134\ud55c\ub2e4. reduce: reduce(op: (A, A) => A): A , RDD\uc758 elements\ub4e4\uc744 op \ud568\uc218\ub85c combine\ud558\uace0 \uadf8 \uacb0\uacfc\ub97c \ub9ac\ud134\ud55c\ub2e4. foreach: foreach(f: T => Unit): Unit , RDD\uc758 \uac01 element\uc5d0 function\uc744 \uc801\uc6a9\ud55c\ub2e4. RDD[String]\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4 gigabytes\uc758 log\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud558\ub2e4. \uadf8\ub807\ub2f4 2016\ub144 12\uc6d4\uc5d0 \ubc1c\uc0dd\ud55c ERROR \ub85c\uadf8\uc758 \uac1c\uc218\ub294 \uc5b4\ub5bb\uac8c \uce74\uc6b4\ud2b8 \ud560\uae4c? val lastYearslogs: RDD[String] = ... val numDecErrorlogs = lastYearslogs.filter( lg=> lg.contains(\"2016-12\") && lg.contains(\"error\") ) .count() \uc774\ub7f0 Laziness\ub85c \uc778\ud574 \uc5bb\uc744 \uc218 \uc788\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4. val lastYearslogs: RDD[String] = ... val firstlogsWithErrors = lastYearslogs.filter(_.contains(\"ERROR\")) .take(10) \uc704\uc640 \uac19\uc774 10\uac1c\ub9cc \ucde8\ud558\ub824 \ud560\ub54c \ubaa8\ub4e0 \ub370\uc774\ud130\ub97c \uc21c\ud68c\ud560 \ud544\uc694 \uc5c6\uc774 10\uac1c\ub97c \uc5bb\ub294 \uc21c\uac04 \uc885\ub8cc\ub97c \ud560 \uc218 \uc788\ub2e4.","title":"Common Actions in the Wild"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#transformations-on-two-rdds","text":"RDDs also support set-like operations, like union and intersection. Two-RDD transformations combine two RDDs are combined into one. \ub9ac\ud134\ud0c0\uc785\uc774 \ud56d\uc0c1 RDD \uc774\ubbc0\ub85c Lazy\ud558\ub2e4! union: union(other: RDD[T]): RDD[T] intersection: intersection(other: RDD[T]): RDD[T] subtract: subtract(other: RDD[T]): RDD[T] cartesian: cartesian[U](other: RDD[U]): RDD[(T, U)] / \ub2e4\ub978 RDD\uc640\uc758 Cartesian product \ub97c \ub9ac\ud134\ud55c\ub2e4","title":"Transformations on Two RDDs"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#other-useful-rod-actions","text":"Scala collections\uc5d0\ub294 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294 Spark\uc5d0\ub9cc \uc874\uc7ac\ud558\ub294 \ubd84\uc0b0 \ucc98\ub9ac\uc5d0 \uc7a5\uc810\uc774 \uc788\ub294 actions\ub3c4 \uc788\ub2e4 \uc544\ub798\uc758 \ud568\uc218\ub4e4\uc740 \ubc18\ud658\ud0c0\uc785\uc774 RDD\uac00 \uc544\ub2c8\uae30\ub54c\ubb38\uc5d0 actions\uc774\ub2e4. = Eager\ud558\ub2e4. takeSample: takeSample(withRepl: Boolean, num: Int): Array[T] / Return an array with a random sample of num elements of the dataset, with or without replacement. takeOrdered: takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] / Return the first n elements of the ROD using either their natural order or a custom comparator. saveAsTextFile: saveAsTextFile(path: String): Unit / Write the elements of the dataset as a text file in the local filesystem or HDFS. saveAsSequenceFile: saveAsSequenceFile(path: String): Unit / Write the elements of the dataset as a Hadoop Se\u00adquenceFile in the local filesystem or HDFS.","title":"Other Useful ROD Actions"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#why-is-spark-good-for-data-science","text":"\ub300\ubd80\ubd84\uc758 data science probles\ub294 iteration\uc744 \ud3ec\ud568\ud55c\ub2e4. \uc704 \uadf8\ub9bc\uc5d0\uc11c \uc704\ucabd\uc740 Hadoop, \uc544\ub798\ucabd\uc740 Spark\uc774\ub2e4. \ubcf4\uba74 \uc54c \uc218 \uc788\ub4ef\uc774 Iteration\uc774 \uc77c\uc5b4\ub0a0\ub54c\ub9c8\ub2e4 Hadoop\uc740 IO \ud0c0\uc784\uc774 \uc99d\uac00\ud558\ub294 \ubc18\uba74 Spark\uc5d0\uc11c\ub294 memory computation\uc73c\ub85c\uc368 IO time\uc744 \uc904\uc600\ub2e4.","title":"Why is Spark Good for Data Science?"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#iteration-example-logistic-regression","text":"Logistic regression\uc740 classification\uc744 \uc704\ud55c iterative algorithm\uc774\ub2e4. \ub2e4\ub978 classification algorithms\uc640 \uac19\uc774 classifier's weights\ub294 training dataset\uc5d0 \uae30\ubc18\ud558\uc5ec \ubc18\ubcf5\uc801\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8 \ub41c\ub2e4. val points = sc.textFile(...).map(parsePoint) var w = Vector.zeros(d) for (i <- 1 to numIterations) { val gradients = points.map { p => (1 / (1 + exp(-p.y * w.dot(p.x))) - 1) * p.y * p.y }.reduce(_ + _) w = -= alpha * gradient } RDD\ub294 \uae30\ubcf8\uc801\uc73c\ub85c action\uc744 run\ud560 \ub54c\ub9c8\ub2e4 recomputed \ub41c\ub2e4. \uc774\ub294 data\uac00 \ub9ce\uc544\uc9c8\uc218\ub85d expensive \ud574\uc9c4\ub2e4. Spark\ub294 memory\uc5d0 \uce90\uc2dc\ub97c \ucee8\ud2b8\ub864 \ud560 \uc218 \uc788\ub3c4\ub85d \uc9c0\uc6d0\ud574\uc900\ub2e4. RDD\ub97c \uba54\ubaa8\ub9ac\uc5d0 \uce90\uc2dc\ud558\uae30 \uc704\ud574\uc120 persist() \ud639\uc740 cache() \ub97c \uc0ac\uc6a9\ud558\uba74 \ub41c\ub2e4. val lastYearsLogs: RDD[String] = ... val logsWithErrors = lastYearsLogs.filter(_.contains(\"ERROR\")) .persist() val firstLogsWithErrors = logsWithErrors.take(10) val numErrors = logsWithErrors.count() \ub9cc\uc57d logsWithErrors\uc5d0 persist\ub97c \ucd94\uac00\ud558\uc9c0 \uc54a\uc558\ub354\ub77c\uba74(\uce90\uc2f1\ud558\uc9c0 \uc54a\uc558\ub354\ub77c\uba74) firstLogsWithErrors\ub97c \uacc4\uc0b0\ud560 \ub54c\uc640 numErrors\ub97c \uacc4\uc0b0\ud560 \ub54c \ucd1d 2\ubc88 \uacc4\uc0b0 \ud588\uc5b4\uc57c \ud560 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uce90\uc2f1\uc744 \ud588\uae30 \ub54c\ubb38\uc5d0 \ud55c \ubc88\uc758 logsWithErrors \uacc4\uc0b0\uc774 \uc774\ub8e8\uc5b4\uc9c4\ub2e4. \uc774\ub97c \uc704\uc5d0 Logsitic Regression\ucabd\uc5d0 \uc801\uc6a9\ud558\ub824\uba74 points \uc120\uc5b8 \ub9c8\uc9c0\ub9c9\uc5d0 .persist()\ub97c \ucd94\uac00\ud574\uc8fc\uba74 \ub41c\ub2e4. \ub370\uc774\ud130\ub97c persist\ud558\ub294 \ubc29\ubc95\uc740 \uc5ec\ub7ec \uac00\uc9c0\uac00 \uc788\ub2e4. in memory as regular Java objects on disk as regular Java objects in memory as serialized Java objects (more compact) on disk as serialized Java objects (more compact) both in memory and on disk (spill over to disk to avoid re-computation) cache() Shorthand for using the default storage level, which is in memory only as regular Java objects. persist() Persistence can be customized with this method. Pass the storage level you\u2019d like as a parameter to persist. Key takeway: Despite similar-looking API to Scala Collections, the deferred semantics of Spark's RDDs are very unlike Scala Collections. One of the most common performance bottlenecks of newcomers to Spark arises from unknowingly reevaluating several transformations when caching could be used. Spark\ub294 Lazy\ub97c \ubc1c\uacac\ud558\uba74 \uc5b4\ub5bb\uac8c \ud558\uba74 optimization\ud560 \uc218 \uc788\uc744\uc9c0 analyze\ud55c\ub2e4.","title":"Iteration, Example: Logistic Regression"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#excerciese","text":"object WikipediaRanking extends WikipediaRankingInterface { val langs = List( \"JavaScript\", \"Java\", \"PHP\", \"Python\", \"C#\", \"C++\", \"Ruby\", \"CSS\", \"Objective-C\", \"Perl\", \"Scala\", \"Haskell\", \"MATLAB\", \"Clojure\", \"Groovy\") // true\ub85c \uac12\uc744 \uc124\uc815\ud55c\ub2e4\ub294 \uac83\uc740 local\ub85c \ub9cc\ub4e0\ub2e4\ub294 \uac83\uc774\ub2e4. // appName\uc744 \uc124\uc815\ud55c \uac83\uc740 SparkContext\ub97c \ub9cc\ub4e4\uae30 \uc704\ud568\uc774\ub2e4. val conf: SparkConf = new SparkConf(true).setAppName(\"myApp\") // local[2]\ub85c \ub9cc\ub4e0 \uac83\uc740 \ucc98\uc74c\uc5d0 sc\ub97c \ub9cc\ub4dc\ub294 \uacfc\uc815\uc5d0\uc11c \uc624\ub958\uac00 \ubc1c\uc0dd\ud588\uace0 \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 stackoverflow\uc5d0\uc11c \ucc3e\uc740 \ubc29\ubc95\uc774\ub2e4. // new SparkContext(conf)\ub85c \ud588\ub354\ub2c8 \uc624\ub958\uac00 \ubc1c\uc0dd\ud588\ub2e4. val sc: SparkContext = new SparkContext(\"local[2]\", \"myApp\", conf) // Hint: use a combination of `sc.parallelize`, `WikipediaData.lines` and `WikipediaData.parse` val wikiRdd: RDD[WikipediaArticle] = sc.parallelize(WikipediaData.lines.map(line => WikipediaData.parse(line))) /** Returns the number of articles on which the language `lang` occurs. * Hint1: consider using method `aggregate` on RDD[T]. * Hint2: consider using method `mentionsLanguage` on `WikipediaArticle` */ def occurrencesOfLang(lang: String, rdd: RDD[WikipediaArticle]): Int = rdd .aggregate(0)((b: Int,a: WikipediaArticle) => if (WikipediaArticle(a.title, a.text).mentionsLanguage(lang)) b + 1 else b , (b, a) => b + a) /* (1) Use `occurrencesOfLang` to compute the ranking of the languages * (`val langs`) by determining the number of Wikipedia articles that * mention each language at least once. Don't forget to sort the * languages by their occurrence, in decreasing order! * * Note: this operation is long-running. It can potentially run for * several seconds. */ def rankLangs(langs: List[String], rdd: RDD[WikipediaArticle]): List[(String, Int)] = langs.map(lang => (lang, occurrencesOfLang(lang, rdd))).sortWith(_._2 >_._2) /* Compute an inverted index of the set of articles, mapping each language * to the Wikipedia pages in which it occurs. */ def makeIndex(langs: List[String], rdd: RDD[WikipediaArticle]): RDD[(String, Iterable[WikipediaArticle])] = rdd.flatMap(rd => langs.filter(lang => rd.mentionsLanguage(lang)).map(lang => (lang, rd))).groupByKey() /* (2) Compute the language ranking again, but now using the inverted index. Can you notice * a performance improvement? * * Note: this operation is long-running. It can potentially run for * several seconds. */ def rankLangsUsingIndex(index: RDD[(String, Iterable[WikipediaArticle])]): List[(String, Int)] = { index.mapValues(d => d.size) .collect() .sortWith((before, after) => before._2 > after._2) .toList } /* (3) Use `reduceByKey` so that the computation of the index and the ranking are combined. * Can you notice an improvement in performance compared to measuring *both* the computation of the index * and the computation of the ranking? If so, can you think of a reason? * * Note: this operation is long-running. It can potentially run for * several seconds. */ def rankLangsReduceByKey(langs: List[String], rdd: RDD[WikipediaArticle]): List[(String, Int)] = rdd.flatMap(rd => for { lang <- langs if rd.mentionsLanguage(lang) } yield (lang, 1)) .reduceByKey((x, y) => x + y) .collect() .sortWith((before, after) => before._2 > after._2) .toList }","title":"Excerciese"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#week2","text":"distributed Spark\uc5d0\uc11c reduce \uac19\uc774 Reduction Operation\uc774\ub780 \ubb34\uc5c7\uc778\uac00? - foldLeft, reduceRight \ub4f1\uacfc \uac19\uc774 - collection\uc744 \uc21c\ud658\ud558\uba74\uc11c - neighboring element\ub97c combine \ud558\uba74\uc11c - collection together\ub97c \ud574\uc11c - single element result\ub97c \ub9cc\ub4e4\uc5b4 \ub0b4\ub294 \uac83\uc774\ub2e4. Spark\uc5d0 RDD\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\ub294 Reduction Operations\ub294 \uc544\ub798\uc640 \uac19\ub2e4. 1. fold 2. reduce 3. aggregate \ub9cc\uc57d reduction\uc744 \ud558\uba74\uc11c \ub9ac\ud134 \ud0c0\uc785\uc744 \ubc14\uafb8\uace0 \uc2f6\ub2e4\uba74 aggregate\ub97c \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4. foldLeft\uc640 foldRight\uc740 \ubd88\uac00\ub2a5\ud558\ub2e4. \ub9cc\uc57d \uc21c\ucc28\uc801\uc73c\ub85c \ucc98\ub9ac\ud574\uc11c \uc774\ub97c \uad6c\ud604\ud560 \uc218 \uc788\uc9c0 \uc54a\uc744\uae4c? \ub77c\ub294 \uc0dd\uac01\uc774 \ub4e4\uc218\ub3c4 \uc788\uc9c0\ub9cc distributed system\uc5d0\uc11c \uc5b4\ub5a4 \uc77c\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \ucc98\ub9ac \ud55c\ub2e4\ub294 \uac83\uc740 \ud798\ub4e4\ub2e4. aggregate\ub294 reduction operator\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ub41c\ub2e4. large-scale data\ub97c \ub2e4\ub8f0 \ub54c data type\uc744 \uc904\uc774\ub294 \uac83\uc740 \uc911\uc694\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. big data processing\uc5d0 \uc788\uc5b4 most common operation on data\ub294 key-value pairs \uc774\ub2e4. \uad6c\uae00\uc5d0\uc11c\ub294 key-value pairs\ub97c manipulating\ud558\uae30 \uc704\ud574 MapReduce \ub97c \ub514\uc790\uc778 \ud588\ub2e4. Spark\uc5d0\uc11c\ub294 distributed key-value pairs\ub97c Pair RDDs \ub77c\uace0 \ubd80\ub978\ub2e4. element type\uc774 pair\uc778 RDD\uac00 \ub9cc\ub4e4\uc5b4\uc9c0\uba74 Spark\ub294 \uc790\ub3d9\uc801\uc73c\ub85c \uc720\uc6a9\ud55c \uba87 \uac00\uc9c0 \uba54\uc18c\ub4dc\ub4e4\uc744 \ucd94\uac00\ud574\uc900\ub2e4. def groupByKey(): RDD[(K, Iterable[V])] def reduceByKey(func: (V, V) => V): RDD[(K, V)] def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] Pair RDD\ub97c \ub9cc\ub4dc\ub294 \uac00\uc7a5 \uc790\uc8fc\uc4f0\uc774\ub294 \ubc29\ubc95\uc740 \uc774\ubbf8 \uc874\uc7ac\ud558\ub294 non-pair RDD\ub85c \ubd80\ud130 \ub9cc\ub4dc\ub294 \uac83\uc774\ub2e4. RDD\uc5d0 map operation\uc744 \uc801\uc6a9\ud558\ub294 \uac83\uc774 \uc608\uac00 \ub420 \uc218 \uc788\ub2e4. val rdd: RDD[WikipediaPage] = // Has type: org.apache.spark.rdd.RDDE(String, String)] val pairRdd = rdd.map(page => (page.title, page.text)) \uc704\uc640 \uac19\uc774 pairRdd\ub97c \ub9cc\ub4e4\uba74 groupByKey, reduceByKey, join\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4.","title":"Week2"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#some-interesting-pair-rdds-operations","text":"Transformations(=Lazy) - groupByKey - reduceByKey - mapValues - keys - join - leftOuterJoin / rightOuterJoin Action(=Eager) - countByKey val ages = List(2, 52, 44, 23, 17, 14, 12, 82, 51 , 64) val grouped = ages.groupBy { age => if (age >= 18 && age < 65) \"adult\" else if (age < 18) \"child\" else \"senior\" } // grouped: scala.collection.immutable.MapEString,List[Int]] = // Map(senior -> List(82), adult -> List(52, 44, 23, 51, 64), // child -> List(2, 17, 14, 12)) gropuByKey \uc608\uc2dc) case class Event(organizer: String, name: String, budget: Int) val eventsRdd = sc.parallelize(...) .map(event => (event.organizer, event.budget)) val groupedRdd = eventsRdd.groupByKey() // TRICK QUESTION! As-is, it \"does\" nothing. It returns an unevaluated RDD (groupByKey\ub294 lazy\ud558\uae30 \ub54c\ubb38\uc774\ub2e4.) groupedRdd.collect().foreach(println) // (Prime Sound,CompactBuffer(42000)) reduceByKey \uc608\uc2dc) - reduceByKey\ub294 groupByKey\ub97c \ud558\uace0 reduce\ub97c \ud558\ub294 \uac83\ubcf4\ub2e4 more efficient\ud558\ub2e4. - key\uc5d0\ub294 \uad00\uc2ec\uc774 \uc5c6\uace0 value\uc5d0\ub9cc operate\ud55c\ub2e4. case class Event(organizer: String, name: String, budget: Int) val eventsRdd = sc.parallelize(...) .map(event => (event.organizer, event.budget)) val budgetsRdd = eventsRdd.reduceByKey(_+_) reducedRdd.collect().foreach(println) // (Prime Sound,42000) // (Sportorg,36400) // (Innotech,320000) // (Association Balelec,50000) mapValues\uc608\uc2dc) def mapValues[U](f: V => U): RDD[(K, U)] // \uc704\uc758 \uc815\uc758\ub294 \uc544\ub798\uc640 \uac19\uc774 \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4. rdd.map { case (x, y): (x, func(y)) } countByKey (def countByKey(): Map[K, Long]) simply counts the number of elements per key in a Pair RDD, returning a normal Scala Map (remember, it\u2019s an action!) mapping from keys to counts. val intermediate = eventsRdd.mapValues(b => (b, 1)) .reduceByKey((v1, v2) => (v1._1 + v2._1, v1._2 + v2._2)) // intermediate: RDD[(String, (Int, Int))] val avgBudgets = intermediate.mapValues { case (budget, numberOfEvents) => budget / numberOfEvents } avgBudgets.collect().foreach(println) // (Prime Sound,42000) // (Sportorg,12133) // (Innotech,106666) // (Association Balelec,50000) keys \uc608\uc2dc) - keys (def keys: RDD[K]) Return an RDD with the keys of each tuple. - Transformation\uc774\ub2e4. = RDD\ub97c \ub9ac\ud134\ud55c\ub2e4. case class Visitor(ip: String, timestamp: String, duration: String) val visits: RDD[Visitor] = sc.textfile(...) // keys\uc640 distinct\ub294 transformation\uc774\uae30 \ub54c\ubb38\uc5d0 count\ub97c \uc774\uc6a9\ud574 // \uacc4\uc0b0\uc744 \ud55c\ub2e4. val numllniqueVisits = visits.keys.distinct().count() // numllniqueVisits: Long = 3391 join\ub3c4 pari RDD\uc5d0\ub9cc \uc801\uc6a9\uac00\ub2a5\ud558\uae30 \ub54c\ubb38\uc5d0 normal RDD\uc5d0\ub294 \uc801\uc6a9 \ubd88\uac00\ub2a5\ud558\ub2e4. join\uc740 \ud55c \ub9c8\ub514\ub85c 2\uac1c\uc758 pair RDD\ub97c 1\uac1c\uc758 pair RDD\ub85c \ud569\uce58\ub294 \uac83\uc774\ub2e4. val as = List( (101, (\"Ruetli\", AG)), (102, (\"Brelaz\", DemiTarif)), (103, (\"Gress\", DemiTarifVisa)), (104, (\"Schatten\", DemiTarif))) val abos = sc.parallelize(as) val ls = List( (101, \"Bern\"), (101, \"Thun\"), (102, \"Lausanne\"), (102, \"Geneve\"), (102, \"Nyon\"), (103, \"Zurich\"), (103, \"St-Gallen\"), (103, \"Chur\") ) val locations = sc.parallelize(ls)","title":"Some interesting Pair RDDs operations"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#inner-join","text":"Inner joins return a new RDD containing combined pairs whose keys are present in both input RDDs. val trackedCustomers = abos.join(locations) // trackedCustomers: RDD[(Int, ((String, Abonnement), String))] trackedCustomers.collect().foreach(println) // (101,((Ruetli,AG),Bern)) // (101,((Ruetli,AG),Thun)) // (102,((Brelaz,DemiTarif),Nyon)) // (102,((Brelaz,DemiTarif),Lausanne)) // (102,((Brelaz,DemiTarif),Geneve)) // (103,((Gress,DemiTarifVisa),St-Gallen)) // (103,((Gress,DemiTarifVisa),Chur)) // (103,((Gress,DemiTarifVisa),Zurich)) \uc704\uc758 \uc608\ub97c \ubcf4\uba74 as\uc5d0\uc11c 104\uac00 \uc0ac\ub77c\uc84c\ub2e4. location\uc5d0\ub294 key 104\uac00 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4.","title":"inner join"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#outer-join","text":"Outer joins return a new RDD containing combined pairs whose keys don\u2019t have to be present in both input RDDs. def leftOuterJoinEW](other: RDD[(K, W)]): RDD[(K, (V, OptionEW]))] def rightOuterJoinEW](other: RDD[(K, W)]): RDD[(K, (Option[V], W))] val abosWithOptionalLocations = abos.leftOuterJoin(locations) abosWithOptionalLocations.collect().foreach(println) // (101,((Ruetli,AG),Some(Thun))) // (101,((Ruetli,AG),Some(Bern))) // (102,((Brelaz,DemiTarif),Some(Geneve))) // (102,((Brelaz,DemiTarif),Some(Nyon))) // (102,((Brelaz,DemiTarif),Some(Lausanne))) // (103,((Gress,DemiTarifVisa),Some(Zurich))) // (103,((Gress,DemiTarifVisa),Some(St-Gallen))) // (103,((Gress,DemiTarifVisa),Some(Chur))) // (104,((Schatten,DemiTarif),None)) val customersWithLocationDataAndOptionalAbos = abos.rightOuterJoin(locations) // RDD[(Int, (Option[(String, Abonnement)], String))] customersWithLocationDataAndOptionalAbos.collect().foreach(println) // (101,(Some((Ruetli,AG)),Bern)) // (101,(Some((Ruetli,AG)),Thun)) // (102,(Some((Brelaz,DemiTarif)),Lausanne)) // (102,(Some((Brelaz,DemiTarif)),Geneve)) // (102,(Some((Brelaz,DemiTarif)),Nyon)) // (103,(Some((Gress,DemiTarifVisa)),Zurich)) // (103,(Some((Gress,DemiTarifVisa)),St-Gallen)) // (103,(Some((Gress,DemiTarifVisa))\uff0cChur))","title":"Outer join"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#week3","text":"Remember our data is distributed! val pairs = sc.parallelize( List( (1, \"one\"), (2, \"two\"), (3, \"three\") ) ) pairs.groupByKey() // res2: org.apache.spark.rdd.RDD[(Int, Iterable[String])] // = ShuffledRDD[16] at groupByKey at <console>:37 We typically have to move data from one node to another to be \"grouped with\" its key. Doing this is called \"shuffling\". Shuffles can be an enormous hit to because it means that Spark must send data from one node to another. Why? Latency! Goal: calculate how many trips, and how much money was spent by each individual customer over the course of the month. val purchasesRdd: RDD[CFFPurchaseJ = sc.textFile(...) // Returns: Array[(Int, (Int, Double))] val purchasesPerMonth = purchasesRdd .map(p => (p.customerld, p.price)) // Pair RDD .groupByKey() // groupByKey returns RDD[(K, Iterable[VJ)J .map(p => (p._1, (p._2.size, p._2.sum))) .collect() \uc704\uc758 \uadf8\ub9bc\uc5d0\uc11c \ub178\ub780\uc0c9 \uc601\uc5ed\uc774 shuffle\uc774 \uc77c\uc5b4\ub098\ub294 \uacf3\uc774\ub2e4. \uc544\ub798\ucc98\ub7fc reduceByKey\ub85c optimize\ud560 \uc218 \uc788\ub2e4. val purchasesPerMonth = purchasesRdd .map(p => (p.customerld, (1, p.price))) // Pair ROD .reduceByKey((v1, v2) => (v1._1 + v2._1, v1._2 + v2._2)) .collect() \uc704 \uadf8\ub9bc\ucc98\ub7fc network\ub97c \ud1b5\ud558\uae30 \uc804\uc5d0 \uba3c\uc800 reduce\ub97c \ud574\uc11c \ub354 \ud6a8\uc728\uc801\uc774\ub2e4.(\ub354 \uc801\uac8c \uc8fc\uace0\ubc1b\uae30 \ub54c\ubb38) \uadf8\ub807\ub2e4\uba74 Spark\ub294 \uc5b4\ub5bb\uac8c data\ub97c partitioning(\ud544\uc694\ud55c machine\uc5d0 \ub370\uc774\ud130\ub97c \ubcf4\ub0b4\ub294 \uac83) \ud560\uae4c? \ub2e4\uc74c \ubd80\ud130 \uc54c\uc544\ubcf4\uc790 Partitioning\uc740 key base\uc774\uae30 \ub54c\ubb38\uc5d0 pair RDD \uc5d0\uc11c\ub9cc \ub3d9\uc791\ud55c\ub2e4. number of partitions\ub294 configurable\ud558\uba70 \uae30\ubcf8\uc740 total number of cores on all executor nodes\uc774\ub2e4. Spark\uc5d0\uc11c \uac00\ub2a5\ud55c partitioning\uc740 2\uac00\uc9c0\uac00 \uc788\ub2e4 1. Hash partitioning 2. Range partitioning","title":"Week3"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#hash-partitioning","text":"val purchasesPerCust = purchasesRdd .map(p => (p.customerld, p.price)) // Pair RDD .groupByKey() groupByKey first computes per tuple (k, v) its partition p: p = k.hashCode() % numPartitions Then, all tuples in the same partition p are sent to the machine hosting p. hash partitioning attempts to spread data evenly across partitions based on the key.","title":"Hash partitioning"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#range-partitioning","text":"Pair RDDs may contain keys that have an ordering defined. For such RDDs, range partitioning may be more efficient. Using a range partitioner, keys are partitioned according to: an ordering for keys a set of sorted ranges of keys Property: tuples with keys in the same range appear on the same machine. Consider a Pair RDD, with keys [8, 96, 240, 400, 401, 800], and a desired number of partitions of 4. Furthermore, suppose that hashCode() is the identity (n.hashCode() == n). p = n.hashCode() % numPartitions => p = n % numPartitions \uc640 \uac19\uc740 \uacb0\ub860\uc774 \ub41c\ub2e4. \uadf8\ub807\uae30\uc5d0 \uc774\ub97c \uc801\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ubd84\uc0b0\ub41c\ub2e4. partition 0: [8, 96, 240, 400, 800] partition 1: [401] partition 2: [] partition 3: [] The result is a very unbalanced distribution which hurts performance. Using range partitioning the distribution can be improved significantly: - Assumptions: (a) keys non-negative, (b) 800 is biggest key in the RDD. - Set of ranges: [1 , 200], [201 , 400], [401 , 600], [601 , 800] In this case, range partitioning distributes the keys as follows among the partitions: - partition 0: [8, 96] - partition 1: [240, 400] - partition 2: [401J - partition 3: [800] The resulting partitioning is much more balanced How do we set a partitioning for our data? There are two ways to create RDDs with specific partitionings: Call partitionBy on an RDD, providing an explicit Partitioner. Using transformations that return RDDs with specific partitioners. val pairs = purchasesRdd.map(p => (p.customerld, p.price)) val tunedPartitioner = new RangePartitioner(8, pairs) val partitioned = pairs.partitionBy(tunedPartitioner).persist() Important: the result of partitionBy should be persisted. Otherwise, the partitioning is repeatedly applied (involving shuffling!) each time the partitioned RDD is used. Partitioner from parent RDD: Pair RDDs that are the result of a transformation on a partitioned Pair RDD typically is configured to use the hash partitioner that was used to construct it. Automatically-set partitioners: Some operations on RDDs automatically result in an RDD with a known partitioner - for when it makes sense. For example, by default, when using sortByKey, a RangePartitioner is used. Further, the default partitioner when using groupByKey, is a HashPartitioner, as we saw earlier. Operations on Pair RDDs that hold to (and propagate) a partitioner: - cogroup - groupWith - join - leftOuterJoin - rightOuterJoin - groupByKey - reduceByKey - foldByKey - combineByKey - partitionBy - sort - mapValues (if parent has a partitioner) - flatMapValues (if parent has a partitioner) - filter (if parent has a partitioner) All other operations will produce a result without a partitioner. map\uc740 \uc65c lose the partitioner\ud560\uae4c? \uc65c\ub0d0\ud558\uba74 \ud0a4\ub97c \ubc14\uafc0\uc218\ub3c4 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. rdd.map((k: String, v: Int)=> (\"doh!\" v)) \ud558\uc9c0\ub9cc mapValues\ub294 \ud0a4\ub97c \ubc14\uafb8\uc9c0 \uc54a\ub294\ub2e4. = partitioner\uac00 \uc720\uc9c0\ub41c\ub2e4. => Pair RDD\ub97c \ub2e4\ub8f0 \ub550 mapValues\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \ucd5c\uc6b0\uc120 \uace0\ub824\ud574\ub77c. userData\uac00 \ud070 \ub370\uc774\ud130\uc774\uace0 events\uac00 \uc791\uc740 \ub370\uc774\ud130 \uc77c \ub54c \uc544\ub798 \ucf54\ub4dc\ub97c \uc0b4\ud3b4\ubcf4\uc790. val sc = new SparkContext(...) val userData = sc.sequenceFile[UserID, Userlnfo](\"hdfs://...\").persist() def processNewlogs(logFileName: String) { val events = sc.sequenceFile[UserID, Linklnfo](logFileName) val joined = userData.join(events) //ROD of (UserID, (Userlnfo, Linklnfo)) val offTopicVisits = joined.filter { case (userld, (userlnfo, linklnfo)) => //Expand the tuple !userlnfo.topics.contains(linklnfo.topic) }.count() println(''Number of visits to non-subscribed topics: '' + offTopicVisits) } \uc704\uc640 \uac19\uc740 \ucf54\ub4dc\uac00 \uc788\uc744\ub54c processNewlogs\ub97c \uc2e4\ud589\ud560 \ub54c\ub9c8\ub2e4 join\uc774 \ubd88\ub9ac\uae30 \ub54c\ubb38\uc5d0 keys\uac00 dataset\uc5d0 \uc5b4\ub5bb\uac8c partitioned \ub418\uc5b4\uc788\ub294\uc9c0 \uc54c \uc218 \uc5c6\ub2e4. => \ube44\ud6a8\uc728\uc801\uc774\ub2e4. By default, this operation will hash all the keys of both datasets, sending elements with the same key hash across the network to the same machine, and then join together the elements with the same key on that machine. Even though userData doesn't change! \uadf8\ub798\uc11c \uc544\ub798\uc640 \uac19\uc774 \uc218\uc815\ud55c\ub2e4. val userData = sc.sequenceFile[UserID, Userlnfo](\"hdfs://...\") .partitionBy(new HashPartitioner(100)) // Create 100 partitions .persist() Since we called partitionBy when building userData, Spark will now know that it is hash-partitioned, and calls to join on it will take advantage of this information. In particular, when we call userData.join(events), Spark will shuffle only the events ROD, sending events with each particular UserID to the machine that contains the corresponding hash partition of userData You can also figure out whether a shuffle has been planned/executed via: The return type of certain transformations, e.g., org.apache.spark.rdd.RDD[(String, Int)]= ShuffledRDD[366J Using function toDebugString to see its execution plan: partitioned .reduceByKey((v1, v2) => (v1._1 + v2._1, v1._2 + v2._2)) .toDebugString // res9: String= (8) MapPartitionsRDD[l6221J at reduceByKey at <console>:49 [] shuffle\uc744 \uc720\ubc1c\ud558\ub294 operations - cogroup - groupWith - join - leftOuterJoin - rightOuterJoin - groupByKey - reduceByKey - combineByKey - d istinct - intersection - repartition - coalesce There are a few ways to use operations that might cause a shuffle and to still avoid much or all network shuffling. reduceByKey running on a pre-partitioned ROD will cause the values to be computed locally, requiring only the final reduced value has to be sent from the worker to the driver. join called on two RDDs that are pre-partitioned with the same partitioner and cached on the same machine will cause the join to be computed locally, with no shuffling across the network. How your data is organized on the cluster, and what operations you're doing with it matters! Rule of thumb: a shuffle can occur when the resulting RDD depends on other elements from the same RDD or another RDD. In fact, RDD dependencies encode when data must move across the network. Narrow Dependencies Each partition of the parent RDD is used by at most one partition of the child RDD. Fast! No shuffle necessary. Optimizations like pipelining possible. Wide Dependencies Each partition of the parent RDD may be depended on by multiple child partitions. Slow! Requires all or some data to be shuffled over the network. \uc5b4\ub5bb\uac8c dependencies\ub97c \ubcfc \uc218 \uc788\uc744\uae4c? dependencies method on RDDs. dependencies returns a sequence of Dependency objects, which are actually the dependencies used by Spark\u2019s sched\ub098ler to know how this RDD depends on other RDDs. val wordsRdd = sc.parallelize(largeList) val pairs = wordsRdd.map(c => (c, 1)) .groupByKey() .dependencies // pairs: SeqEorg.apache.spark.Dependency // List(org.apache.spark.ShuffleDependency@4294a23d) Narrow dependency objects: \u25ba OneToOneDependency \u25ba PruneDependency \u25ba RangeDependency Wide dependency objects: \u25ba ShuffleDependency 2\ubc88\uc9f8 \ubc29\ubc95 toDebugString method on RDDs. toDebugString prints out a visualization of the RDD\u2019s lineage, and other information pertinent to scheduling. For example, indentations in the output separate groups of narrow transformations that may be pipelined together with wide transformations that require shuffles. These groupings are called stages. val wordsRdd = sc.parallelize(largeList) val pairs = wordsRdd.map(c => (c, 1)) .groupByKey() .toDebugString //pairs: String = //(8) ShuffledRDD[219] at groupByKey at <console>:38 [] // +-(8) MapPartitionsRDD[218] at map at <console>:37 [] // | ParallelCollectionRDD[217] at parallelize at <console>:36 [] Recomputing missing partitions fast for narrow dependencies. But slow for wide dependencies!","title":"Range partitioning"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#weel4","text":"\ub611\uac19\uc740 \uacb0\uacfc\uac00 \ub098\uc624\ub294 \ucf54\ub4dc\uc774\ub354\ub77c\ub3c4 \uc5b4\ub5bb\uac8c \uc218\ud589\ud558\ub290\ub0d0\uc5d0 \ub530\ub77c runtime\uc774 \ud655\uc5f0\ud558\uac8c \ucc28\uc774\ub09c\ub2e4. Case1: innerjoin first demographics.join(finances).filter { p => p._2._1.country == \"Switzerland\" && p._2._2.hasFinancialDependents && p._2._2.hasDebt }.count Case2: filter first val filtered = finances.filter(p => p._2.hasFinancialDependents && p._2.hasDebt) demographics.filter(p => p._2.country == \"Switzerland\") .join(filtered) .count Case3: Cartesian Product val cartesian = demographics.cartesian(finances) cartesian.filter { case (pl, p2) => p1._1 == p2._l } .filter { case (pl, p2) => (pl._2.country == \"Switzerland\") && (p2._2.hasFinancialDependents) && (p2._2.hasDebt) }.count \uc2e4\ud589 \uc2dc\uac04\uc740 \uc5b4\ub5bb\uac8c \ub420\uae4c? 150,000 people\uc5d0 \ub300\ud574\uc11c \uacc4\uc0b0\uc744 \ud588\uc744 \ub54c 1\ubc88\uc740 4.97\ucd08 2\ubc88\uc740 1.35\ucd08\uac00 \uac78\ub838\ub2e4. \ubc18\uba74 3\ubc88\uc740 \uc57d 4\ubd84\uc774 \uac78\ub838\ub2e4. \uadf8\ub9cc\ud07c Cartesian Product\ub294 \uc131\ub2a5\uc774 \uc88b\uc9c0 \uc54a\ub2e4. \ub530\ub77c\uc11c \uac00\ub2a5\ud558\uba74 Cartesian Product\ub294 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294 \uac83\uc774 \uc88b\ub2e4. \uadf8\ub807\ub2e4\uba74 \uac1c\ubc1c\uc790\ub294 \ub9e4\ubc88 \uc774\ub7f0 \uc131\ub2a5\uc744 \uc608\uc0c1\ud558\uba74\uc11c \ucf54\ub529\uc744 \ud574\uc57c \ud560\uae4c? => DataType\uc5d0 \ub530\ub77c \ub2e4\ub974\ub2e4. \uc6b0\ub9ac\uac00 \uadf8\uac04 \ub2e4\ub904\uc654\ub358 \ub370\uc774\ud130 \ud0c0\uc785\ub4e4\uc740 unstructured data\uc600\ub2e4. text or json, xml ... \uc774 \uacbd\uc6b0\uc5d4 Spark\uac00 \ub0b4\ubd80\ub97c \ub4e4\uc5ec\ub2e4 \ubcf4\uae30 \ud798\ub4e4\uae30\uc5d0 optimization\uc774 \uc548\ub41c\ub2e4. \ud558\uc9c0\ub9cc DB\uc640 \uac19\uc774 structured data\uc758 \uacbd\uc6b0\uc5d0\ub294 Spark\uac00 \ub370\uc774\ud130 \uad6c\uc870\ub97c \uc54c \uc218 \uc788\uace0 \uc608\uce21\ud560 \uc218 \uc788\uae30\uc5d0 aggressive optimization\uc774 \uac00\ub2a5\ud558\ub2e4. => Spark SQL \uc774 \uc774\ub97c \uac00\ub2a5\ud558\uac8c \ud574\uc900\ub2e4.!","title":"Weel4"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#spark-sql","text":"Spark SQL\uc758 goal\uc740 1. Spark programs(on RDD)\uc640 external data source with a friendly API \uac04\uc758 relational processing\uc744 \uc9c0\uc6d0\ud558\ub294 \uac83\uc774\ub2e4. 2. database\ub97c research\ud558\ub294\ub370 high performance\ub97c \ubc1c\ud718\ud55c\ub2e4. 3. semi-structured data\uc640 external databases\ub97c easily support \ud55c\ub2e4. Spark SQL\uc740 Spark\uc758 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc911 \ud558\ub098\ub85c\uc368 3\uac1c\uc758 main API\uac00 \uc788\ub2e4. - SQL literal syntax - DataFrames - Datasets \ub610\ud55c 2\uac1c\uc758 \ud2b9\ubcc4\ud55c backend component\uac00 \uc788\ub2e4. - Catalyst, query optimizer - Tungsten, off-heap serializer","title":"Spark SQL"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#relational-queriessql","text":"Everything about SQL is structured. In fact, SQL stands for structural query language. There are a set of fixed data types. Int, Long, String, etc. There are fixed set of operations. SELECT, WHERE, GROUP BY, etc. Research and industry surrounding relational databases has focused on exploiting this rigidness to get all kinds of performance speedups. DataFrame is Spark SQL\u2019s core abstraction. (Conceptually equivalent to a table in a relational database.) DataFrames are, conceptually, RDDs full of records with a known schema . Unlike RDDs though, DataFrames require some kind of schema info! DataFrames are untyped! That is, the Scala compiler doesn\u2019t check the types in its schema! ( RDD[T] \ub85c \uc4f0\ub294 \ubc18\uba74 DataFrame \uc740 \ub4a4\uc5d0 \ud0c0\uc785\uc774 \uc548\ubd99\ub294\ub2e4.) DataFrames contain Rows which can contain any schema. Transformations on DataFrames are also known as untyped transformations","title":"Relational Queries(SQL)"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#sparksession","text":"To get started using Spark SQL, everything starts with the SparkSession import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(\"My App\") //.config(\"spark.some.config.option\", \"some-value\") .getOrCreate()","title":"SparkSession"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#creating-dataframes","text":"DataFrames can be created in two ways: From an existing RDD. Either with schema inference, or with an explicit schema. Reading in a specific data source from file. Common structured or semi-structured formats such as JSON.","title":"Creating DataFrames"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#1a-create-dataframe-from-rdd-schema-reflectively-inferred","text":"val tupleRDD = ...// Assume RDD[(Int, String String, String)] val tupleDF = tupleRDD.toDF(\"id\", \"name\", \"city\", \"country\") // column names Note: ifyou use toDF without arguments, Spark will assign numbers as attributes (column names) to your DataFrame. If you already have an RDD containing some kind of case class instance, then Spark can infer the attributes from the case class's fields. case class Person(id: Int, name: String, city: String) val peopleRDD =...// Assume RDD[Person] val peopleDF = peopleRDD.toDF","title":"(1a) Create DataFrame from RDD, schema reflectively inferred"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#1b-create-dataframe-from-existing-rdd-schema-explicitly-specified","text":"case class Person(name: String, age: Int) val peopleRdd = sc.textFile(...) // Assume RDD[Person] \uc704\uc758 \ucf54\ub4dc\ub97c manually build up a schema, dataFrame\uc744 \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc740 \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218 \uc788\ub2e4. // The schema is encoded in a string val schemastring = \"name age\" // Generate the schema based on the string of schema val fields = schemastring.split(\" \") .map(fieldName => StructField(fieldName, StringType, nullable = true)) val schema = StructType(fields) // Convert records of the RDD (people) to Rows val rowRDD = peopleRDD .map(_.split(\",\")) .map(attributes => Row(attributes(0), attributes(1).trim)) // Apply the schema to the RDD val peopleDF = spark.createDataFname(rowRDD, schema)","title":"(1b) Create DataFrame from existing RDD, schema explicitly specified"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#2-create-dataframe-by-reading-in-a-data-source-from-file","text":"Using the SparkSession object, you can read in semi-structured/structured data by using the read method. For example, to read in data and infer a schema from a JSON file: // 'spark' is the SparkSession object we created a few slides back val df = spark.read.json(\"examples/src/main/resources/people.json\") Semi-structured/Structured data sources Spark SQL can directly create DataFrames from: - JSON - CSV - Parquet - JDBC To see a list of all available methods for directly reading in semi-structured/structured data, see the latest API docs for DataFrameReader: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader","title":"(2) Create DataFrame by reading in a data source from file."},{"location":"scala/bigdata-analysis-with-scala-and-spark/#sql-literals","text":"DataFrame\uc774 \ub9cc\ub4e4\uc5b4 \uc84c\uc73c\uba74 \uc774\uc81c dataset\uc5d0 SQL syntax\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. A DataFrame called peopleDF, we just have to register our DataFrame as a temporary SQL view first: // Register the DataFrame as a SQL temporary view peopleDF.createOrReplaceTempView(\"people\") // This essentially gives a name to our DataFrame in SQL // so we can refer to it in an SQL FROM statement // SQL literals can be passed to Spark SQL's sql method val adultsDF = spark.sql(\"SELECT * FROM people WHERE age > 17\") The SQL statements available to you are largely what's available in HiveQL. This includes standard SQL statements such as: SELECT FROM WHERE COUNT HAVING GROUP BY ORDER BY SORT BY DISTINCT JOIN (LEFT|RIGHT|FULL) OUTER JOIN Subqueries: SELECT col FROM ( SELECT a + b AS col from t1) t2 Supported Spark SQL syntax: https://docs.datastax.eom/en/datastax_enterprise/4.6/datastax_enterprise/spark/sparkSqlSupportedSyntax.html For a HiveQL cheatsheet: https://hortonworks.com/blog/hive-cheat-sheet-for-sql-users/ For an updated list of supported Hive features in Spark SQL, the official Spark SQL docs enumerate: https://spark.apache.org/docs/latest/sql-programming-guide.html#supported-hive-features \ub354\uc6b1 \ud765\ubbf8\ub85c\uc6b4 \uc608\uc81c\ub97c \ud55c\ubc88 \uc0b4\ud3b4 \ubcf4\uc790 case class Employee(id: Int, fname: String, Iname: String, age: Int, city: String) // DataFrame with schema defined in Employee case class val employeeDF = sc.parallelize(...).toDF val sydneyEmployeesDF = spark.sql(\"\"\"SELECT id, lname FROM employees WHERE city = \"Sydney\" ORDER BY id\"\"\")","title":"SQL Literals"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#dataframes","text":"Spark's RDDs\uc640 \uad00\ub828\ub41c API\ub97c \uc81c\uacf5\ud55c\ub2e4 \uc790\ub3d9\uc801\uc73c\ub85c aggresively optimized \ud560 \uc218 \uc788\ub2e4. untyped \uc774\ub2e4. = scala compiler\uac00 type check\ub97c \ud558\uc9c0 \uc54a\ub294\ub2e4. show() pretty-prints DataFrame in tabular form. Shows first 20 elements. case class Employee(id: Int, fname: String, lname: String, age: Int, city: String) val employeeDF = sc.parallelize(...).toDF employeeDF.show( ) //+---+-----+-------+---+--------+ // | id|fname| |name|age| city| | // +---+-----+-------+---+--------+ // | 12| Joe| Smith| 38|New York| // |563|Sally| Owens| 48|New York| // |645|Slate|Markham| 28| Sydney| // |221|David| Walker| 21| Sydney| // +---+-----+-------+---+--------+ printSchema() prints the schema of your DataFrame in a tree format. case class Employee(id: Int, fname: String, lname: String, age: Int, city: String) val employeeDF = sc.parallelize(...).toDF employeeDF.printschema( ) // root // |-- id: integer (nullable = true) // |-- fname: string (nullable = true) // |-- lname: string (nullable = true) // |-- age: integer (nullable = true) // |-- city: string (nullable = true) def select(col: String, cols: String*): DataFrame // selects a set of named columns and returns a new DataFrame with these // columns as a result. def agg(expr: Column, exprs: Column*): DataFrame // performs aggregations on a series of columns and returns a new DataFrame // with the calculated output. def groupBy(col1: String, cols: String*): DataFrame // simplified // groups the DataFrame using the specified columns. Intended to be used before an aggregation. def join(right: DataFrame): DataFrame // simplified // inner join with another DataFrame Other transformations include: filter, limit, orderBy, where, as, sort, union, drop, amongst others. You can select and work with columns in three ways: 1. Using $-notation // $-notation requires: import spark.implicits._ df.filter($\"age\" > 18) 2. Referring to the Dataframe df.filter(df(\"age\")> 18)) 3. Using SQL query string df.filter(\"age > 18\") case class Employee(id: Int, fname: String, lname: String, age: Int, city: String) val employeeDF = sc.parallelize(...).toDF val sydneyEmployeesDF = employeeDF.select(\"id\", \"lname\") .where(\"city == 'Sydney'\") .orderBy( \"id\") // sydneyEmployeesDF: // +---+-------+ // | id| lname| // |221| Walker| // |645|Markham| // +---+-------+ \uc544\ub798 \ub450 \ucf54\ub4dc\uc758 \uacb0\uacfc\ub294 \ub3d9\uc77c\ud558\ub2e4. val over30 = employeeDF.filter(\"age > 30\").show() val over30 = employeeDF.where(\"age > 30\").show() employeeDF.filter(($\"age\" > 25) && ($\"city\" === \"Sydney\")).show() groupBy function\uc740 RelationalGroupedDataset \uc744 \ub9ac\ud134\ud55c\ub2e4. \uc774\ub294 count, sum, max, min, avg \uac19\uc740 \ud568\uc218\ub4e4\uc744 \ud3ec\ud568\ud55c\ub2e4. df.groupBy($\"attribute1\") .agg(sum($\"attribute2\")) df.groupBy($\"attribute1\") .count($\"attribute2\") case class Listing(street: String, zip: Int, price: Int) val listingsDF = . . . // DataFrame of Listings import org.apache.spark.sql.functions._ val mostExpensiveDF = listingsDF.groupBy($\"zip\") .max(\"price\") val leastExpensiveDF = listingsDF.groupBy($\"zip\") .min( \"price\") case class Post( authorID: Int, subforum: String, likes: Int, date: String) val postsDF = ... // DataFrame of Posts val rankedDF = postsDF.groupBy($\"authorID\", $\"subforum\") .agg(count($\"authorID\")) // new DF with columns authorID, subforum, count(authorID) .orderBy($\"subforum\", $\"count(authorID)\".desc) // postsDF: // +--------+--------+-----+----+ // |authorID|subforum|likes|datel // +--------+--------+-----+----+ // 1| design| 2| | // 1| debate| 0| | // 2| debate| 0| | // 3| debate| 23| | // 1| design| 1| | // 1| design| 0| | // 2| design| 0| | // 2| debate| 0| | // +--------+--------+-----+----+ // rankedDF: // +--------+--------+---------------+ // |authorID|subforum|count(authorID)| // +--------+--------+---------------+ // | 2| debate| 2| // | 1| debate| 1| // | 3| debate| 1| // | 1| design| 3| // | 2| design| 1| // +--------+--------+---------------+ Sometimes you may have a data set with null or NaN values. In these cases it's often desirable to do one of the following: drop rows/records with unwanted values like null or \"NaN\" replace certain values with a constant Dropping records with unwanted values: - drop() drops rows that contain null or NaN values in any column and returns a new DataFrame. - drop(\"all\") drops rows that contain null or NaN values in all columns and returns a new DataFrame. - drop(Array(\"id\", \"name\")) drops rows that contain null or NaN values in the specified columns and returns a new DataFrame. Replacing unwanted values: - fill(0) replaces all occurrences of null or NaN in numeric columns with specified value and returns a new DataFrame. - fill(Map(\"minBalance\" -> 0)) replaces all occurrences of null or NaN in specified column with specified value and returns a new DataFrame. - replace(Array(\"id\"),Map(1234 -> 8923)) replaces specified value (1234)in specified column (id) with specified replacement value (8923)and returns a new DataFrame. Like RDDs, DataFrames also have their own set of actions. collect(): Array[Row] Returns an array that contains all of Rows in this DataFrame. count(): Long Returns the number of rows in the DataFrame. first(): Row/head(): Row Returns the first row in the DataFrame. show(): Unit Displays the top 20 rows of DataFrame in a tabular form. take(n: Int): Array[Row] Returns the first n rows in the DataFrame. Joins on DataFrames are similar to those on Pair RDDs, with the one major usage difference that, since DataFrames aren\u2019t key/value pairs, we have to specify which columns we should join on. Several types of joins are available: - inner, outer, left_outer, right_outer, leftsemi. Performing joins: Given two DataFrames, df1 and df2 each with a column/attribute called id, we can perform an inner join as follows: df1.join(df2, $\"df1.id\" === $\"df2.id\") It\u2019s possible to change the join type by passing an additional string parameter to join specifying which type of join to perform. E.g., df1.join(df2, $\"df1.id\" $\"df2.id\", \"right_outer\") case class Abo(id: Int, v: (String, String)) case class Loc(id: Int, v: String) val as = List(Abo(101, (\"Ruetli\", \"AG\")), Abo(102, (\"Brelaz\", \"DemiTarif\")), Abo(103, (\"Gress\", \"DemiTarifVisa\")), Abo(104, (\"Schatten\", \"DemiTarif\"))) val abosDF = sc.parallelize(as).toDF val ls = List(Loc(101, \"Bern\"), Loc(101, \"Thun\"), Loc(102, \"Lausanne\"), Loc(102, \"Geneve\"), Loc(102, \"Nyon\"), Loc(103, \"Zurich\"), Loc(103, \"St-Gallen\"), Loc(103, \"Chur\")) val locationsDF = sc.parallelize(ls).toDF // abosDF: // +-\u2014+----------------------+ // |id| v| // +-\u2014+----------------------+ // |101| [Ruetli, AG]| // |102| [Brelaz, DemiTarif]| // |103|[Gress, DemiTarifv...| // |104|[Schatten, DemiTarif]| // +-\u2014+-------------------+ // locationsDF: // +---+---------+ // |id| v| // |101| Bern| // |101| Thun| // |102| Lausanne| // |102| Geneve| // |102| Nyon| // |103| Zurich| // |103|St-Gallen| // |103| Chur| // +---+---------+ val abosDF = sc.parallelize(as).toDF val locationsDF = sc.parallelize(ls).toDF val trackedCustomersDF = abosDF.join(locationsDF, abosDF(\"id\") === locationsDF(\"id\")) // trackedCustomersDF: // +---+--------------------+---+---------+ // | id| v| id| v| // +---+--------------------+---+---------+ // |101| [Ruetli,AG]|101| Bern| // |101| [Ruetli,AG]|101| Thun| // |103|[Gress,DemiTarifV...|103| Zurich| // |103|[Gress,DemiTarifV...|103|St-Gallen| // |103|[Gress,DemiTarifV...|103| Chur| // |102| [Brelaz,DemiTarif]|102| Lausanne| // |102| [Brelaz,DemiTarif]|102| Geneve| // |102| [Brelaz,DemiTarif]|102| Nyon| // +---+----------------------+---+---------+ val abosWithOptionalLocationsDF = abosDF.join(locationsDF, abosDF(\"id\") === locationsDF(\"id\"), \"left_outer\") // abosWithOptionalLocationsDF: // +---+--------------------+----+---------+ // | id| v| id| v| // +---+--------------------+----+---------+ // |101| [Ruetli,AG]| 101| Bern| // |101| [Ruetli,AG]| 101| Thun| // |103|[Gress,DemiTarifV...| 103| Zurich| // |103|[Gress,DemiTarifV...| 103|St-Gallen| // |103|[Gress,DemiTarifV...| 103| Chur| // |102| [Brelaz,DemiTarif]| 102| Lausanne| // |102| [Brelaz,DemiTarif]| 102| Geneve| // |102| [Brelaz,DemiTarif]| 102| Nyon| // |104|[Schatten,DemiTarif]|null| Nyon| // +---+----------------------+---+---------+ As expected, customer 104 has returned! :-) case class Demographic(id: Int, age: Int, codingBootcamp: Boolean, country: String, gender: String, isEthnicMinority: Boolean, servedlnMilitary: Boolean) val demographicsDF = sc.textfile(...).toDF // DataFrame of Demographic case class Finances(id: Int, hasDebt: Boolean, hasFinancialDependents: Boolean, hasStudentLoans: Boolean, income: Int) val financesDF = sc.textfile(...).toDF // DataFrame of Finances demographicsDF.join(financesDF, demographicsDF(\"ID\") === financesDF(\"ID\"), \"inner\") .filter($\"HasDebt\" && $\"HasFinancialDependents\") .filter($\"CountryLive\" === \"Switzerland\") .count \uc704\uc758 join\uacfc filter\uac00 \ubd99\uc740 \ucf54\ub4dc\ub97c \ubcf4\uba74 \ub9cc\uc57d df\uac00 \uc544\ub2cc \uc77c\ubc18 \ub370\uc774\ud130\uc600\uc744 \uacbd\uc6b0 join\uc744 \uba3c\uc800 \ud558\ub0d0 filter\ub97c \uba3c\uc800 \ud558\ub0d0\uc5d0 \ub530\ub77c \uc131\ub2a5\ucc28\uc774\uac00 \ud06c\uac8c \ub2ec\ub77c\uc9c4\ub2e4. \ud558\uc9c0\ub9cc df\uc5d0\uc11c\ub294 \uc790\ub3d9\uc801\uc73c\ub85c optimization\ud574\uc8fc\uae30 \ub54c\ubb38\uc5d0 \uc774\uc5d0 \ub300\ud574 \uac71\uc815\ud560 \ud544\uc694\uac00 \uc5c6\ub2e4. How is this possible? Recall that Spark SQL comes with two specialized backend components: Catalyst, query optimizer. Tungsten, off-heap serializer.","title":"DataFrames"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#limitations-of-dataframes","text":"Untyped! \ucc3e\uace0\uc790 \ud558\ub294 column\uc774 \uc5c6\uc744 \uc2dc compile\uc740 \ub418\uc9c0\ub9cc \uc2e4\ud589\ud574\ubcf4\uba74 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Limited Data Types If your data can\u2019t be expressed by case classes/Products and standard Spark SQL data types, it may be difficult to ensure that a Tungsten encoder exists for your data type. E.g., you have an application which already uses some kind of complicated regular Scala class. Requires Semi-Structured/Structured Data \ub9cc\uc57d your unstructured data\uac00 schema\ub97c \uac00\uc9c4 \ud615\ud0dc\ub85c reformulated\ub420 \uc218 \uc5c6\ub2e4\uba74 RDDs\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ub354 \uc88b\ub2e4. case class Listing(street: String, zip: Int, price: Int) val listingsDF = ... // DataFrame of Listings import org.apache.spark.sql.functions._ val averagePricesDF = listingsDF.groupBy($\"zip\") .avg(\"price\") // averagePrices: Array[org.apache.spark.sql.Row] averagePrices.head.schema.printTreeString() // root // 1-- zip: integer (nullable = true) // 1-- avg(price): double (nullable = true) val averagePricesAgain = averagePrices.map { row => (row(0).aslnstanceOf[Int], row(1).aslnstanceOf[Double]) } // mostExpensiveAgain: Array[(Int, Double)] DataFrames are actually Datasets. type DataFrame = DataSet[Row] Datasets can be thought of as typed distributed collections of data. Dataset API unifies the DataFrame and RDD APIs. Mix and match! Datasets require strucutred/semi-structured data. Schemas and Encoders core part of Datasets. // We can freely mix APls! listingsDS.groupByKey(l => l.zip) // looks like groupByKey on RDDs! .agg(avg($\"price\").as[Double]) // looks like our DataFrame operators!","title":"Limitations of DataFrames"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#datasets","text":"Datasets are a something in the middle between DataFrames and RDDs - You can still use relational DataFrame operations as we learned in previous sessions on Datasets. - Datasets add more typed operations that can be used as well. - Datasets let you use higher-order functions like map, flatMap, filter again! Datasets can be used when you want a mix of functional and relational transformations while benefiting from some of the optimizations on DataFrames. And we've almost got a type safe API as well.","title":"Datasets"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#creating-datasets","text":"From a DataFrame. Just use the toDS convenience method. myDF.toDS // requires import spark.implicits._ Note that often it's desirable to read in data from JSON from a file, which can be done with the read method on the SparkSession object like we saw in previous sessions, and then converted to a Dataset: val myDS = spark.read.json(\"people.json\").as[Person] From an RDD. Just use the toDS convenience method. myRDD.toDS // requires import spark.implicits._ From common Scala types. Just use the toDS convenience method. List(\"yay\", \"ohnoes\", \"hooray!\").toDS // requires import spark.implicits._ Recall the Column type from DataFrames. On Datasets, typed operations tend to act on TypedColumn instead. Remember untyped transformations from DataFrames? The Dataset API includes both untyped and typed transformations. untyped transformations the transformations we learned on DataFrames. typed transformations typed variants of many DataFrame transformations + additional transformations such as RDD-like higher-order functions map, flatMap, etc. These APls are integrated. You can call a map on a DataFrame and get back a Dataset, for example. - Caveat: not every operation you know from RDDs are available on Datasets, and not all operations look 100% the same on Datasets as they did on RDDs. But remember, you may have to explicitly provide type information when going from a DataFrame to a Dataset via typed transformations. val keyValuesDF = List((3, \"Me\"), (1, \"Thi\"), (2, \"Se\"), (3, \"ssa\") (3, \"-\")), (2, \"cre\") (2, \"t\")).toDF val res = keyValuesDF.map(row => row(0).asinstanceOf[Int] + 1) // Ew...","title":"Creating Datasets"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#common-typed-transformation-on-datasets","text":"map map U : Dataset[U] flatMap flatMap U : Dataset[U] filter filter(pred: T => Bool ean): Dataset[T] distinct distinct(): Dataset[T] groupByKey groupByKey K : KeyValueGroupedDataset[K, T] coalesce coalesce(numPartitions: Int): Dataset[T] Apply a function to each element in the Dataset and return a Dataset of the contents of the iterators returned. repartition repartition(numPartitions: Int): Dataset[T] Apply predicate function to each element in the Dataset and return a Dataset of elements that have passed the predicate condition, pred.","title":"Common (Typed) Transformation on Datasets"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#grouped-operations-on-datasets","text":"Like on DataFrames, Datasets have a special set of aggregation operations meant to be used after a call to groupByKey on a Dataset. calling groupByKey on a Dataset returns a KeyValueGroupedDataset KeyValueGroupedDatasetcontains a number of aggregation operations which return Datasets How to group & aggregate on Datasets? 1. Call groupByKey on a Dataset, get back a KeyValueGroupedDataset. 2. Use an aggregation operation on KeyValueGroupedDataset (return Datasets)","title":"Grouped Operations on Datasets"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#some-keyvaluegroupeddataset-aggregation-operations","text":"reduceGroups reduceGroups(f: (V, V) => V): Dataset[(K, V)] Reduces the elements of each group of data using the spec\u00adified binary function. The given function must be commutative and associative or the result may be non-deterministic. agg agg U : Dataset[(K, U)] Computes the given aggregation, returning a Dataset of tuples for each unique key and the result of computing this aggregation over all elements in the group. someDS.agg(avg($\"column\")) // error! untyped regular Column\uc744 TypedColumn\uc73c\ub85c \ubcc0\uacbd\ud574\uc918\uc57c \ud55c\ub2e4. someDS.agg(avg($\"column\").as[Double]) mapGroups mapGroups U : Dataset[U] Applies the given function to each group of data. For each unique group, the function will be passed the group key and an iterator that contains all of the elements in the group. The function can return an element of arbitrary type which wi11 be returned as a new Dataset. flatMapGroups flatMapGroups U : Dataset[U] Applies the given function to each group of data. For each unique group, the function will be passed the group key and an iterator that contains all of the elements in the group. The function can return an iterator containing elements of an arbitrary type which will be returned as a new Dataset. Dataset API\ub97c \uc0b4\ud3b4\ubcf4\uba74 RDD\uc5d0\ub294 \uc788\uc9c0\ub9cc \uc5ec\uae30\uc5d4 reduceByKey\uac00 \uc5c6\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. val keyValues = List((3,\"Me\"),(1 ,\"Thi\"),(2,\"Se\"),(3,\"ssa\"),(1 ,\"sisA\"),(3,\"ge:\"),(3,\"-)\"),(2,\"ere\"),(2,\"t\")) val keyValuesDS = keyValues.toDS keyValuesDS.groupByKey(p => p._1) .mapGroups((k, vs) => (k, vs.foldleft(\"\")((acc, p) => ace + p._2))) // +---+----------+ // | _1| _2| // +---+----------+ // | 1| ThisisA| // | 3|Message:-)| // | 2| Secret| // +---+----------+ keyValuesDS.groupByKey(p => p._1) .mapValues(p => p._2) .reduceGroups((acc, str) => ace + str)","title":"Some KeyValueGroupedDataset Aggregation Operations"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#aggregators","text":"A class that helps you generically aggregate data. Kind of like the aggregate method we saw on RDDs. class Aggregator[-IN, BUF, OUT] IN is the input type to the aggregator. When using an aggregator after groupByKey, this is the type that represents the value in the key/value pair. BUF is the intermediate type during aggregation. OUT is the type of the output of the aggregation . This is how implement our own Aggregator: val myAgg = new Aggregator[IN, BUF, OUT] { def zero: BUF = ... // The initial value. def reduce(b: BUF, a: IN): BUF = ... // Add an element to the running total def merge(b1 : BUF, b2: BUF): BUF = ... // Merge intermediate values. def finish(b: BUF): OUT = ... // Return the final result. }.toColumn val keyValues = List((3,\"Me\"),(1 ,\"Thi\"),(2,\"Se\"),(3,\"ssa\"),(1 ,\"sisA\"),(3,\"ge:\"),(3,\"-)\"),(2,\"ere\"),(2,\"t\")) val keyValuesDS = keyValues.toDS val strConcat = new Aggregator[(Int, String) , String, String] { def zero: String = \"\" def reduce(b: String, a: (Int, String)): String = b + a._2 def merge(bl : String, b2: String) : String = bl + b2 def finish(r: String) : String = r override def bufferEncoder: Encoder[String] = Encoders.STRING override def outputEncoder: Encoder[String] = Encoders.STRING }.toColumn keyValuesDS.groupByKey(pair => pair._1) .agg(strConcat.as[String]).show // +------+--------------------+ // | value|anon$1(scala.Tuple2)| // +------+--------------------+ // | 1| ThisisA| // | 3| Message:-)| // | 2| Secret| // +------+--------------------+","title":"Aggregators"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#encoders","text":"Encoders are what convert your data between JVM objects and Spark SQL's specialized internal (tabular) representation. They're required by all Datasets!","title":"Encoders"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#common-dataset-actions","text":"collect(): Array[T] Returns an array that contains all of Rows in this Dataset. count(): Long Returns the number of rows in the Dataset. first(): T/head(): T Returns the first row in this Dataset. foreach(f: T => Unit): Unit Applies a function f to all rows. reduce(f: (T, T) => T): T Reduces the elements of this Dataset using the specified binary function. show(): Unit Displays the top 20 rows of Dataset in a tabular form. take(n: Int): Array[T] Returns the first n rows in the Dataset.","title":"Common Dataset Actions"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#limitations-of-datasets","text":"Catalyst Can't Optimize All Operations Limited Data Types Requires Semi-Structured/Structured Data","title":"Limitations of Datasets"},{"location":"scala/bigdata-analysis-with-scala-and-spark/#when-to-use-datasets-vs-dataframes-vs-rdds","text":"Use Datasets when... - you have structured / semi-structured data - you want typesafety - you need to work with functional APls - you need good performance, but it doesn't have to be the best Use DataFrames when... - you have structured / semi-structured data - you want the best possible performance, automatically optimized for you Use RDDs when... - you have unstructured data - you need to fine-tune and manage low-level details of ROD computations - you have complex data types that cannot be serialized with Encoders","title":"When to use Datasets vs DataFrames vs RDDs?"},{"location":"scala/fpd-in-scala/","text":"fdd-in-scala Week1-1: Recap: Functions and Pattern Matching Pattern Matching\uc758 \uc608 def show(json: JSON): String = json match { case JSeq(elems) => \"[\" + (elems map show mkString \", \") + \"]\" case JObj(bindings) => val assocs = bindings map { case (key, value) => \"\\\"\" + key + \"\\\": \" + show(value) } \"{\" + (assocs mkString \", \") + \"}\" case JNum(num) => num.toString case JStr(str) => \"\\\"\" + str + \"\\\"\" case JBool(b) => b.toString case JNull => \"null\" } scala\uc5d0\uc11c \ubaa8\ub4e0 concrete type\uc740 class or trait \uc774\ub2e4. function\ub3c4 \uc608\uc678\uac00 \uc544\ub2c8\ub2e4. JBinding => String \uc740 scala.Function1[JBinding, String] \uc758 \ucd95\uc57d\ud615\uc774\ub2e4. scala.Function1\uc740 trait\uc774\uace0 JBinding\uacfc String\uc740 type arguments\uc774\ub2e4. Partial Function another subtype of function, special type\uc774\ub2e4. function\uacfc \ub3d9\uc77c\ud558\uac8c apply\ub97c \uac00\uc9c0\uba74\uc11c isDefinedAt \uc744 \ucd94\uac00\ub85c \uac16\ub294\ub2e4. trait PartialFunction[-A, +R] extends Function1[-A, +R] { def apply(x: A): R def isDefinedAt(x: A): Boolean } val f1: String => String = { case \"ping\" => \"pong\"} f1(\"ping\") // pong f1(\"abc\") // MatchError!!! val f: PartialFunction[String, String] = { case \"ping\" => \"pong\" } f.isDefinedAt(\"ping\") // true f.isDefinedAt(\"pong\") // false \ub9cc\uc57d PartialFunction type\uc774 \uae30\ub300\ub41c\ub2e4\uba74 Scala Compiler\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud655\uc7a5\ud55c\ub2e4. { case \"ping\" => \"pong\" } as follows: new PartialFunction[String, String] { def apply(x: String) = x match { case \u201dping\u201d => \u201dpong\u201d } def isDefinedAt(x: String) = x match { case \u201dping\u201d => true case _ => false } } Excercise1 val f: PartialFunction[List[Int], String] = { case Nil => \u201done\u201d case x :: y :: rest => \u201dtwo\u201d } f.isDefinedAt(List(1, 2, 3)) // true val g: PartialFunction[List[Int], String] = { case Nil => \u201done\u201d case x :: rest => rest match { case Nil => \u201dtwo\u201d } } g.isDefinedAt(List(1,2,3)) // ture g(List(1,2,3)) // Match Error!!! \uc704\uc5d0\uc11c \ubcf4\ub4ef\uc774 isDefinedAt \uc740 outmost matching block\ub9cc \uac80\uc99d\ud574\uc900\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 g\uc5d0\uc11c\ub294 true\uac00 \ub9ac\ud134 \ub418\ub294 \uac83\uc774\ub2e4. \uc2e4\uc81c\ub85c \uc0ac\uc6a9\uc744 \ud574\ubcf4\uba74 case Nil \ubc16\uc5d0 case\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Week1-2: Recap: Collections All collection types share a common set of general methods. Core Methods: - map - flatMap - filter and also - foldLeft - foldRight abstract class List[+T] { def map[U](f: T => U): List[U] = this match { case x :: xs => f(x) :: xs.map(f) case Nil => Nil } } abstract class List[+T] { def flatMap[U](f: T => List[U]): List[U] = this match { case x :: xs => f(x) ++ xs.flatMap(f) case Nil => Nil } } map\uacfc\uc758 \ub2e4\ub978\uc810 1. map\uacfc \ub2ec\ub9ac f\uac00 List[U]\ub97c \ub9ac\ud134\ud55c\ub2e4. 2. ++\ub85c List\ub97c concat\ud55c\ub2e4.(List\ub07c\ub9ac\uc758 concat\uc774\ubbc0\ub85c) Week3-1: Type-Directed Programming \uc9c0\uae08\uae4c\uc9c0 \ubd24\ub4ef\uc774 compiler\ub294 values\ub85c \ubd80\ud130 types\uc744 \uc720\ucd94\ud560 \uc218 \uc788\ub2e4. val x = 12 compiler\ub294 x\ub97c Int\ub85c \uc720\ucd94\ud55c\ub2e4. \uc65c\ub0d0\ud558\uba74 \uac12\uc774 12\uc774\ubbc0\ub85c \uc544\ub798\uc640 \uac19\uc774 \ubcf5\uc7a1\ud55c \ud45c\ud604\uc5d0\uc11c\ub3c4 \uc774\ub294 \uc801\uc6a9\ub41c\ub2e4. val y = x + 3 compiler\ub294 y \ub610\ud55c Int\ub85c \uc720\ucd94\ud55c\ub2e4. \uc774\ubc88\uc5d0\ub294 \ubc18\ub300\ub85c compiler\uac00 types\ub85c \ubd80\ud130 values\ub97c \uc720\ucd94\ud558\ub294 \uacfc\uc815\uc744 \ubcfc \uac83\uc774\ub2e4. \uc65c \uc774\uac83\uc774 \uc720\uc6a9\ud558\ub0d0? \ud655\uc2e4\ud55c \ud558\ub098\ub294 compiler\uac00 value\ub97c \ucc3e\uc544\uc11c \uc904 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\ubc88 \ub808\uc2a8\uc758 \ub098\uba38\uc9c0\ub294 \uc774\ub7f0 \uba54\uce74\ub2c8\uc998\uc758 motivation\uc744 \uc18c\uac1c\ud558\uace0 \ub2e4\uc74c \ubc88 \ub808\uc2a8\uc740 how to use it\uc744 \uc124\uba85\ud560 \uac83\uc774\ub2e4. Motivating Example parameter\ub85c List[Int]\ub97c \ubc1b\uc544\uc11c \uc815\ub82c\ud55c \uacb0\uacfc\ub97c List[Int]\ub85c \ub9ac\ud134\ud558\ub294 \ud568\uc218\ub97c \uc0dd\uac01\ud574\ubcf4\uc790. def sort(xs: List[Int]): List[Int] = { ... ... if (x < y) ... ... } \uc0c1\uc138 \ucf54\ub4dc\ub294 \uc5ec\uae30\uc5d0\uc11c \ud544\uc694\uac00 \uc5c6\uae30\uc5d0 \uc0dd\ub7b5\ud588\ub2e4. \uc704 \ucf54\ub4dc\ub294 Int\uc5d0 \ub300\ud574\uc11c\ub9cc \uc801\uc6a9 \uac00\ub2a5\ud558\ubbc0\ub85c general\ud558\uac8c \ubaa8\ub4e0 \ud0c0\uc785\uc5d0 \ub300\ud574\uc11c\ub3c4 \ub3d9\uc791\ud558\uac8c \ud558\uace0 \uc2f6\ub2e4. \uc774\uc5d0 \ub300\ud55c straightforward approach\ub294 polymorphic type\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. def sort[A](xs: List[A]): List[A] = ... \ud558\uc9c0\ub9cc \uc774\uac83\ub9cc\uc73c\ub85c\ub294 \ubd80\uc871\ud558\ub2e4. \uc65c\ub0d0\ud558\uba74 \uac01 type\ubcc4\ub85c compare\ub97c \ub2e4\ub974\uac8c \ud574\uc57c \ud558\uae30 \ub54c\ubb38\uc774\ub2e4. \uadf8\ub798\uc11c \uc774\ubc88\uc5d4 \uac01 compare \ud568\uc218\ub97c parameter\ub85c \ubc1b\ub3c4\ub85d \ud574\ubcf4\uc790. def sort[A](xs: List[A])(lessThan: (A, A) => Boolean): List[A] = { ... ... if (lessThan(x, y)) ... ... } \uadf8\ub807\uac8c \ub418\uba74 \uc544\ub798\uc640 \uac19\uc774 \uac00\ub2a5\ud558\ub2e4' val xs = List(-5, 6, 3, 2, 7) val strings = List(\"apple\", \"pear\", \"orange\", \"pineapple\") sort(xs)((x, y) => x < y) sort(strings)((s1, s2) => s1.compareTo(s2) < 0) Refactoring With Ordering scala\ub294 standard library \uc5d0\uc11c comparing \ud558\ub294 \ud568\uc218\ub97c \uae30\ubcf8\uc73c\ub85c \uc81c\uacf5\ud55c\ub2e4. package scala.math trait Ordering[A] { def compare(a1: A, a2: A): Int def lt(a1: A, a2: A): Boolean = compare(a1, a2) <= 0 ... } compare \ud568\uc218\ub294 2\uac1c\uc758 parameter\ub97c \ubc1b\uc544\uc11c \uccab \ubc88\uc9f8 \uac12\uc774 \ud074 \uacbd\uc6b0 \uc591\uc218, \uc791\uc744 \uacbd\uc6b0 \uc74c\uc218, \ub3d9\uc77c\ud55c \uacbd\uc6b0 0\uc744 \ub9ac\ud134\ud55c\ub2e4. \uc774\ub97c \uc0ac\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ubcc0\uacbd \uac00\ub2a5\ud558\ub2e4. def sort[A](xs: List[A])(ord: Ordering[A]): List[A] = { ... ... if (ord.lt(x, y)) ... ... } import scala.math.Ordering sort(xs)(Ordering.Int) sort(strings)(Ordering.String) \uc5ec\uae30\uc5d0\uc11c \uc0ac\uc6a9 \uc911\uc778 Int\uc640 String\uc740 types \uc774 \uc544\ub2c8\uace0 values \uc784\uc744 \uc54c\uc544\uc57c \ud55c\ub2e4. scala\uc5d0\uc11c\ub294 types\uacfc values\uc5d0 \ub3d9\uc77c\ud55c symbol\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uac00\ub2a5\ud558\ub2e4. object Ordering { val Int = new Ordering[Int] { def compare(x: Int, y: Int) = if (x > y) 1 else if (x < y) -1 else 0 } } Reducing Boilerplate \uc9c0\uae08\uae4c\uc9c0 \uc815\uc758\ud55c \uac83\uc744 \ub530\ub974\uba74 \uc798 \ub3d9\uc791\ud55c\ub2e4. \ud558\uc9c0\ub9cc \ubaa8\ub4e0 \uacbd\uc6b0\uc5d0 \ub300\ud574 boilerplate\uac00 \uc874\uc7ac\ud558\uac8c \ub41c\ub2e4. Int\ub97c \ube44\uad50\ud560 \ub54c\ub9c8\ub2e4 Ordering.Int \ub97c \ubc18\ubcf5\uc801\uc73c\ub85c \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4. sort(xs)(Ordering.Int) sort(ys)(Ordering.Int) sort(strings)(Ordering.String) Implicit Parameters implicit \uc744 \uba85\uc2dc\ud568\uc73c\ub85c\uc11c compiler\uac00 argument ord \ub97c support\ub97c \ud558\uac8c \ud560 \uc218 \uc788\ub2e4. def sort[A](xs: List[A])(implicit ord: Ordering[A]): List[A] = ... sort(xs) sort(ys) sort(strings) \uc704\uc640 \uac19\uc774 \ud558\uba74 \ucef4\ud30c\uc77c\ub7ec\uac00 value\uc5d0 \ub9de\ucdb0 type\uc744 \uacb0\uc815\ud55c\ub2e4. \ucef4\ud30c\uc77c\ub7ec\uac00 \uc218\ud589\ud558\ub294 \uacfc\uc815\uc744 \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\uc790. sort(xs) xs \uac00 List[Int] \ud0c0\uc785\uc774\ubbc0\ub85c \ucef4\ud30c\uc77c\ub7ec\ub294 \uc704\uc758 \ucf54\ub4dc\ub97c \uc544\ub798\uc640 \uac19\uc774 \ubcc0\ud658\ud55c\ub2e4. sort[Int](xs) \uadf8\ub9ac\uace0 \ucef4\ud30c\uc77c\ub7ec\ub294 candidate definition\uc911 Ordering[Int] \ud0c0\uc785\uc5d0 \ub9de\ub294 \uac83\uc744 \ucc3e\ub294\ub2e4. \uc704\uc758 \ucf00\uc774\uc2a4\uc5d0\uc11c\ub294 Ordering.Int\uc640 only matching\ub418\uace0 \ucef4\ud30c\uc77c\ub7ec\ub294 method sort\ub85c \uc774\ub97c \uc804\ub2ec\ud55c\ub2e4. sort[Int](xs)(Ordering.Int) candidate values\uac00 \uc5b4\ub5bb\uac8c \uc815\uc758\ub3c4\uc5b4\uc788\ub294 \uc9c0\ub97c \uc0b4\ud3b4 \ubcf4\uae30 \uc804\uc5d0 implicit \ud0a4\uc6cc\ub4dc\uc5d0 \ub300\ud574 \uc790\uc138\ud788 \uc54c\uc544\ubcf4\uc790. method\ub294 \uc624\uc9c1 \ud558\ub098\uc758 implicit parameter list\ub97c \uac00\uc9c8 \uc218 \uc788\uc73c\uba70 \uc774\ub294 \ub9c8\uc9c0\ub9c9 paramter\uac00 \ub418\uc57c \ud55c\ub2e4. At call site, the arguments of the given clause are usually left out, although it is possible to explicitly pass them: // Argument inferred by the compiler sort(xs) // Explicit argument sort(xs)(Ordering.Int.reverse) Candidates for Implicit Parameters \ucef4\ud30c\uc77c\ub7ec\uac00 type T\uc5d0 \ub300\ud574 \uc5b4\ub5a4 candidate definition\ub97c \ucc3e\uc744\uae4c? \ucef4\ud30c\uc77c\ub7ec\ub294 \uc544\ub798 definition\uc744 \ucc3e\ub294\ub2e4. have type T, are marked implicit, are visible at the point of the function call, or are defined in a companion object associated with T. most specific\ud55c \uc815\uc758\ub97c \ucc3e\uac8c \ub418\uba74 \uadf8\uac83\uc744 \uc0ac\uc6a9\ud558\uace0 \uc5c6\ub2e4\uba74 error\ub97c report\ud55c\ub2e4. implicit Definition implicit definition\uc774\ub780 implicit \ud0a4\uc6cc\ub4dc\uc640 \ud568\uaed8 \uc815\uc758\ub41c \uac83\uc744 \ub9d0\ud55c\ub2e4. object Ordering { implicit val Int: Ordering[Int] = ... } \uc704\uc758 \ucf54\ub4dc\ub294 Int\ub77c\ub294 \uc774\ub984\uc744 \uac00\uc9c4 Ordering[Int] \ud0c0\uc785\uc758 implicit value\ub97c \uc815\uc758\ud55c \uac83\uc774\ub2e4. Any val, lazy val, def, or object definition can be marked implicit. \ub9c8\uc9c0\ub9c9\uc73c\ub85c implicit definitions\ub294 type parameters\uc640 implicit parameters\ub97c \uac00\uc9c8 \uc218 \uc788\ub2e4. implicit def orderingPair[A, B](implicit orderingA: Ordering[A], orderingB: Ordering[B] ): Ordering[(A, B)] = ... Implicit Search Scope type T\uc758 implicit value\ub97c \ucc3e\uae30 \uc704\ud574 \uccab \ubc88\uc9f8\ub85c visible(inherited, imported, or defined in an enclosing scope)\ud55c \ubaa8\ub4e0 implicit definitions\ub97c \ucc3e\ub294\ub2e4. \ub9cc\uc57d \ucef4\ud30c\uc77c\ub7ec\uac00 lexcial scope\uc5d0\uc11c implicit instance\uc640 \ub9e4\uce6d\ub418\ub294 type T\ub97c \ucc3e\uc9c0 \ubabb\ud558\uba74, T\uc640 \uad00\ub828\ub41c companion objects\uc5d0\uc11c \uc774\uc5b4\uc11c \ucc3e\ub294\ub2e4. (companion objects\uc640 types\ub294 other types\uc640 \uc5f0\uad00\uc788\ub2e4.) A companion object is an object that has the same name as a type. \uc608\ub85c object scala.math.Ordering is the companion of the type scala.math.Ordering. The types associated with a type T are: if T has parent types T\u2081 with T\u2082 ... with T\u2099, the union of the parts of T\u2081, ... T\u2099 as well as T itself, if T is a parameterized type S[T\u2081, T\u2082, ..., T\u2099], the union of the parts of S and T\u2081, ..., T\u2099, otherwise, just T itself. As an example, consider the following type hierarchy: trait Foo[A] trait Bar[A] extends Foo[A] trait Baz[A] extends Bar[A] trait X trait Y extends X \ub9cc\uc57d Bar[Y] \ud0c0\uc785\uc758 implicit value\uac00 \ud544\uc694\ud558\ub2e4\uba74 compiler\ub294 \uc544\ub798\uc640 \uac19\uc740 companion object\uc5d0\uc11c implicit definition\uc744 \ucc3e\uc744 \uac83\uc774\ub2e4. Bar, because it is a part of Bar[Y], Y, because it is a part of Bar[Y], Foo, because it is a parent type of Bar, and X, because it is a parent type of Y. However, the Baz companion object will not be visited. Implicit Search Process search process\ub294 no candidate found \ud639\uc740 \ub9e4\uce6d\ub418\ub294 \ucd5c\uc18c\ud55c \ud558\ub098\uc758 candidate\ub97c \uacb0\uacfc\ub97c \ub9cc\ub4e4\uc5b4 \ub0b8\ub2e4. \ub9cc\uc57d no no available implicit definition matching \uc774\ub77c\uba74 \uc5d0\ub7ec\uac00 repot \ub41c\ub2e4. scala> def f(implicit n: Int) = () scala> f ^ error: could not find implicit value for parameter n: Int \ubc18\ub300\ub85c \ub458 \uc774\uc0c1\uc758 implicit definition\uc774 eligibale \ud558\ub2e4\uba74 ambiguity\uac00 report \ub41c\ub2e4. scala> implicit val x: Int = 0 scala> implicit val y: Int = 1 scala> def f(implicit n: Int) = () scala> f ^ error: ambiguous implicit values: both value x of type => Int same type\uc5d0 \ub9e4\uce6d\ub418\ub294 several implicit definitions\uac00 \uc788\uc5b4\ub3c4 \ud558\ub098\ub97c \ud2b9\uc815 \ud560 \uc218 \uc788\ub2e4\uba74 \uad1c\ucc2e\ub2e4. A definition a: A is more specific than a definition b: B if: type A has more \u201cfixed\u201d parts, or, a is defined in a class or object which is a subclass of the class defining b. Let\u2019s see a few examples of priorities at work. Which implicit definition matches the Int implicit parameter when the following method f is called? implicit def universal[A]: A = ??? implicit def int: Int = ??? def f(implicit n: Int) = () f \uc704\uc758 \uacbd\uc6b0\uc5d0\uc11c universal\uc740 type paramter\ub97c \uc9c0\ub2c8\uace0 int\ub294 \uc544\ub2c8\uae30\uc5d0, int\uac00 more fixed parts\ub97c \uac16\uace0 \uc774\ub294 universal\ubcf4\ub2e4 \uba3c\uc800 \uace0\ub824\ub41c\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \ucef4\ud30c\uc77c\ub7ec\uac00 int\ub97c \uc120\ud0dd\ud568\uc5d0 \uc788\uc5b4 ambiguity\uac00 \uc5c6\ub2e4. \uc544\ub798\uc640 \uac19\uc774 \uc788\uc744 \ub54c implicit Int \ud30c\ub77c\ubbf8\ud130\ub97c \uac16\ub294 f method\ub294 \uc5b4\ub290 implicit definition\uc5d0 \ub9e4\uce58 \ub420\uae4c? trait A { implicit val x: Int = 0 } trait B extends A { implicit val y: Int = 1 def f(implicit n: Int) = () f } y\uac00 A\ub97c extend\ud558\ub294 trait\uc774\ubbc0\ub85c y\uac00 A\ubcf4\ub2e4 more specific \ud558\ub2e4. \uadf8\ub7ec\ubbc0\ub85c \ucef4\ud30c\uc77c\ub7ec\uac00 y\ub97c \uc120\ud0dd\ud558\ub294 \uac83\uc5d0 ambiguity\ub294 \uc5c6\ub2e4. Context Bounds Syntactic sugar allows the omission of the implicit parameter list: def printSorted[A: Ordering](as: List[A]): Unit = { println(sort(as)) } Type parameter A has one context bound: Ordering. This is equivalent to writing: def printSorted[A](as: List[A])(implicit ev1: Ordering[A]): Unit = { println(sort(as)) } More generally, a method definition such as: def f[A: U\u2081 ... : U\u2099](ps): R = ... Is expanded to: def f[A](ps)(implicit ev\u2081: U\u2081[A], ..., ev\u2099: U\u2099[A]): R = ... Implicit Query At any point in a program, one can query an implicit value of a given type by calling the implicitly operation: scala> implicitly[Ordering[Int]] res0: Ordering[Int] = scala.math.Ordering$Int$@73564ab0 Note that implicitly is not a special keyword, it is defined as a library operation: def implicitly[A](implicit value: A): A = value Summary In this lesson we have introduced the concept of type-directed programming, a language mechanism that infers values from types. There has to be a unique (most specific) implicit definition matching the queried type for it to be selected by the compiler. Implicit values are searched in the enclosing lexical scope (imports, parameters, inherited members) as well as in the implicit scope of the queried type. The implicit scope of type is made of implicit values defined in companion objects of types associated with the queried type. Week3-2: Type Classes In the previous lectures we have seen a particular pattern of code combining parameterized types and implicits. We have defined a parameterized type Ordering[A], implicit instances of that type for concrete types A, and implicit parameters of type Ordering[A]: trait Ordering[A] { def compare(a1: A, a2: A): Int } object Ordering { implicit val Int: Ordering[Int] = new Ordering[Int] { def compare(x: Int, y: Int) = if (x < y) -1 else if (x > y) 1 else 0 } implicit val String: Ordering[String] = new Ordering[String] { def compare(s: String, t: String) = s.compareTo(t) } } def sort[A: Ordering](xs: List[A]): List[A] = ... We say that Ordering is a type class. Type classes provide yet another form of polymorphism. The method sort can be called with lists containing elements of any type A for which there is an implicit value of type Ordering[A]. At compile-time, the compiler resolves the specific Ordering implementation that matches the type of the list elements. Retroactive Extension Type classes let us add new features to data types without changing the original definition of these data types. For instance, consider the following Rational type, modeling a rational number: /** A rational number * @param num Numerator * @param denom Denominator */ case class Rational(num: Int, denom: Int) We can add the capability \"to be compared\" to the type Rational by defining an implicit instance of type Ordering[Rational]: object RationalOrdering { implicit val orderingRational: Ordering[Rational] = new Ordering[Rational] { def compare(q: Rational, r: Rational): Int = q.num * r.denom - r.num * q.denom } } Laws So far, we have shown how to implement instances of a type class, for some specific types (Int, String, and Rational). Now, let\u2019s have a look at the other side: how to use (and reason about) type classes. For example, the sort function is written in terms of the Ordering type class, whose implementation is itself defined by each specific instance, and is therefore unknown at the time the sort function is written. If an Ordering instance implementation is incorrect, then the sort function becomes incorrect too! To prevent this from happening, type classes are often accompanied by laws, which describe properties that instances must satisfy, and that users of type classes can rely on. Can you think of properties that instances of the type class Ordering must satisfy, so that we can be confident that the method sort won\u2019t be broken? Instances of the Ordering[A] type class must satisfy the following properties: inverse: the sign of the result of comparing x and y must be the inverse of the sign of the result of comparing y and x, transitive: if a value x is lower than y and that y is lower than z, then x must also be lower than z, consistent: if two values x and y are equal, then the sign of the result of comparing x and z should be the same as the sign of the result of comparing y and z. The authors of a type class should think about such kind of laws and they should provide ways for instance implementers to check that these laws are satisfied. Example of Type Class: Ring Let\u2019s see how we can define a type class modeling a ring structure. A ring is an algebraic structure defined as follows according to Wikipedia: In mathematics, a ring is one of the fundamental algebraic structures used in abstract algebra. It consists of a set equipped with two binary operations that generalize the arithmetic operations of addition and multiplication. Through this generalization, theorems from arithmetic are extended to non-numerical objects such as polynomials, series, matrices and functions. This structure is so common that, by abstracting over the ring structure, developers could write programs that could then be applied to various domains (arithmetic, polynomials, series, matrices and functions). A ring is a set equipped with two binary operations, + and *, satisfying the following laws (called the ring axioms): (a + b) + c = a + (b + c) + is associative a + b = b + a + is commutative a + 0 = a 0 is the additive identity a + -a = 0 -a is the additive inverse of a (a * b) * c = a * (b * c) * is associative a * 1 = a 1 is the multiplicative identity a * (b + c) = a * b + a * c left distributivity (b + c) * a = b * a + c * a right distributivity Here is how we can define a ring type class in Scala: trait Ring[A] { def plus(x: A, y: A): A def mult(x: A, y: A): A def inverse(x: A): A def zero: A def one: A } Here is how we define an instance of Ring[Int]: object Ring { implicit val ringInt: Ring[Int] = new Ring[Int] { def plus(x: Int, y: Int): Int = x + y def mult(x: Int, y: Int): Int = x * y def inverse(x: Int): Int = -x def zero: Int = 0 def one: Int = 1 } } Finally, this is how we would define a function that checks that the + associativity law is satisfied by a given Ring instance: def plusAssociativity[A](x: A, y: A, z: A)(implicit ring: Ring[A]): Boolean = ring.plus(ring.plus(x, y), z) == ring.plus(x, ring.plus(y, z)) Note: in practice, the standard library already provides a type class Numeric, which models a ring structure. Summary In this lesson we have identified a new programming pattern: type classes. Type classes provide a form of polymorphism: they can be used to implement algorithms that can be applied to various types. The compiler selects the type class implementation for a specific type at compile-time. A type class definition is a trait that takes type parameters and defines operations that apply to these types. Generally, a type class definition is accompanied by laws, checking that implementations of their operations are correct. Week3-3: Conditional Implicit Definitions In this lesson, we will see that implicit definitions can themselves take implicit parameters. Let\u2019s start with an example. Consider how we order two String values: is \"abc\" lexicographically before \"abd\"? To answer this question, we need to compare all the characters of the String values, element-wise: is a before a? No. is b before b? No. is c before d? Yes! We conclude that \"abc\" is before \"abd\". So, we compare two sequences of characters with an algorithm that compares the characters of the sequences element-wise. Said otherwise, we can define an ordering relation for sequence of characters based on the ordering relation for characters. Can we generalize this process to sequences of any element type A for which there is an implicit Ordering[A] instance? The signature of such an Ordering[List[A]] definition takes an implicit parameter of type Ordering[A]: implicit def orderingList[A](implicit ord: Ordering[A]): Ordering[List[A]] For reference, a complete implementation is shown below. You can see that at some point in the algorithm we call the operation compare of the ord parameter: implicit def orderingList[A](implicit ord: Ordering[A]): Ordering[List[A]] = new Ordering[List[A]] { def compare(xs: List[A], ys: List[A]) = (xs, ys) match { case (x :: xsTail, y :: ysTail) => val c = ord.compare(x, y) if (c != 0) c else compare(xsTail, ysTail) case (Nil, Nil) => 0 case (_, Nil) => 1 case (Nil, _) => -1 } } With this definition, we can sort a list of list of numbers, for example: scala> val xss = List(List(1, 2, 3), List(1), List(1, 1, 3)) res0: List[List[Int]] = List(List(1, 2, 3), List(1), List(1, 1, 3)) scala> sort(xss) res1: List[List[Int]] = List(List(1), List(1, 1, 3), List(1, 2, 3)) But let\u2019s take a step back. We haven\u2019t defined an instance of Ordering[List[Int]] and yet we have been able to sort a list of List[Int] elements! How did the compiler manage to provide such an instance to us? This happened in several steps. First, we called sort(xss). The compiler fixed the type parameter A of the method to List[Int], based on the type of the argument xss, as if we had written: sort[List[Int]](xss) Then, the compiler searched for an implicit definition of type Ordering[List[Int]]. It found that our orderingList definition could be a match under the condition that it could also find an implicit definition of type Ordering[Int], which it eventually found. Finally, the compiler inserted the following arguments for us: sort[List[Int]](xss)(orderingList(Ordering.Int)) In this case, the compiler combined two implicit definitions (orderingList and Ordering.Int) before terminating. In general, though, an arbitrary number of implicit definitions can be combined until the search hits a \u201cterminal\u201d definition. Consider for instance these four implicit definitions: implicit def a: A = ... implicit def aToB(implicit a: A): B = ... implicit def bToC(implicit b: B): C = ... implicit def cToD(implicit c: C): D = ... We can then ask the compiler to summon a value of type D: implicitly[D] The compiler finds that there is a candidate definition, cToD, that can provide such a D value, under the condition that it can also find an implicit definition of type C. Again, it finds that there is a candidate definition, bToC, that can provide such a C value, under the condition that it can also find an implicit definition of type B. Once again, it finds that there is candidate definition, aToB, that can provide such a B value, under the condition that it can also find an implicit value of type A. Finally, it finds a candidate definition for type A and the algorithm terminates! At the beginning of this lesson, we showed that by using implicit parameters the compiler could infer simple arguments for us. We have now reached a point where we can appreciate that the compiler can infer more complex arguments (by inferring arguments of arguments!). It not only significantly reduces code verbosity, it also alleviates developers from implementing parts of their programs, which are summoned by the compiler based on their type (hence the name \u201ctype-directed programming\u201d). In practice, complex fragments of programs such as serializers and deserializers of data types can be summoned by the compiler. Recursive Implicit Definitions What happens if we write an implicit definition that depends on itself? trait X implicit def loop(implicit x: X): X = x implicitly[X] The compiler detects that it keeps searching for an implicit definition of the same type and returns an error: error: diverging implicit expansion for type X starting with method loop Note: it is possible to write recursive implicit definitions by making sure that the search always terminates, but this is out of the scope of this lesson. Example: Sort by Multiple Criteria Consider a situation where we want to compare several movies. Each movie has a title, a rating (in number of \u201cstars\u201d), and a duration (in minutes): case class Movie(title: String, rating: Int, duration: Int) val movies = Seq( Movie(\"Interstellar\", 9, 169), Movie(\"Inglourious Basterds\", 8, 140), Movie(\"Fight Club\", 9, 139), Movie(\"Zodiac\", 8, 157) ) We want to sort movies by rating first, and then by duration. To achieve this, a first step is to change our sort function to take as parameter the sort criteria in addition to the elements to sort: def sort[A, B](elements: Seq[A])(critera: A => B)(implicit ord: Ordering[B] ): Seq[A] = ... The sort algorithm remains the same except that instead of comparing the elements together, we compare the criteria applied to each element. With this function, here is how we can sort movies by title: sort(movies)(_.title) And here is how we can sort them by rating: sort(movies)(_.rating) Each time the sort function is called, its ordering parameter is inferred by the compiler based on the type of the criteria (String and then Int, in the above examples). However, our initial problem was to sort the movies by multiple criteria. We would like to sort first by rating and then by duration: sort(movies)(movie => (movie.rating, movie.duration)) The type of the criteria is now a tuple type (Int, Int). Unfortunately, the compiler is unable to infer the corresponding ordering parameter. We need to define how simple orderings can be combined together to get an ordering for multiple criteria. We do so by defining the following implicit ordering: implicit def orderingPair[A, B](implicit orderingA: Ordering[A], orderingB: Ordering[B] ): Ordering[(A, B)] = ... This definition provides an ordering for pairs of type (A, B) given orderings for types A and B. The complete implementation is the following: implicit def orderingPair[A, B](implicit orderingA: Ordering[A], orderingB: Ordering[B] ): Ordering[(A, B)] = new Ordering[(A, B)] { def compare(pair1: (A, B), pair2: (A, B)): Int = { val firstCriteria = orderingA.compare(pair1._1, pair2._1) if (firstCriteria != 0) firstCriteria else orderingB.compare(pair1._2, pair2._2) } } We first compare the two values according to the first criteria, and if they are equal we compare them according to the second criteria. With this definition, the compiler is now able to infer the ordering for the following call: sort(movies)(movie => (movie.rating, movie.duration)) Here is the same call where the inferred parameter is explicitly written: sort(movies)(movie => (movie.rating, movie.duration))( orderingPair(Ordering.Int, Ordering.Int) ) Note that in the standard library the sort function that we have defined here is already available as a method sortBy on collections. Summary In this lesson, we have seen that: implicit definitions can also take implicit parameters, an arbitrary number of implicit definitions can be chained until a terminal definition is reached. Wee5: Timely Effects Lecture 5.1 - Imperative Event Handling: The Observer Pattern The Observer Pattern\uc740 model\uc758 \ubcc0\ud654\uc5d0 \ub530\ub77c views\uac00 \ubcc0\uacbd\ub418\ub294 \uac83\uc774\ub2e4. publish / subscribe mode/view/controller(MVC) \ub77c\uace0\ub3c4 \ubd88\ub9b0\ub2e4. trait Publisher { private var subscribers: Set[Subscriber] = Set() def subscribe(subscriber: Subscriber): Unit = subscribers += subscriber def unsubscribe(subscriber: Subscriber): Unit = subscribers -= subscriber def publish(): Unit = subscribers.foreach(_.handler(this)) } trait Subscriber { def handler(pub: Publisher) } class BankAccount extends Publisher { private var balance = 0 def currentBalance: Int = balance def deposit(amount: Int): Unit = if (amount > 0) { balance = balance + amount publish() } def withdraw(amount: Int): Unit = if (0 < amount && amount <= balance) { balance = balance - amount publish() } else throw new Error(\"insufficient funds\") } class Consolidator(observed: List[BankAccount]) extends Subscriber { observed.foreach(_.subscribe(this)) private var total: Int = _ compute() private def compute() = total = observed.map(_.currentBalance).sum def handler(pub: Publisher) = compute() def totalBalance = total } val a = new BankAccount val b = new BankAccount val c = new Consolidator(List(a, b)) c.totalBalance a deposit 20 c.totalBalance b deposit 30 c.totalBalance \uc704 \ucf54\ub4dc\ub294 BankAccount\ub97c Observer Pattern\uc73c\ub85c \uad6c\ud604\ud55c \uac83\uc774\ub2e4. Observer Pattern\uc758 \uc7a5\uc810\uc740 - view\ub97c state\ub85c\ubd80\ud130 \ubd84\ub9ac\ud560 \uc218 \uc788\ub2e4. - \uc8fc\uc5b4\uc9c4 state\ub85c \uc5ec\ub7ec \uac1c\uc758 views\ub97c \ub9cc\ub4e4 \uc218 \uc788\ub2e4. - set up\uc774 \uac04\ub2e8\ud558\ub2e4. \ub2e8\uc810\uc740 - handler\uac00 Unit-typed\uc5ec\uc11c \uba85\ub839\ud615 \uc2a4\ud0c0\uc77c\uc774\ub2e4. - \ub9ce\uc740 moving parts\uac00 co-ordinated \ub418\uc5b4\uc57c \ud55c\ub2e4. - Concurrency\uac00 \ubb38\uc81c\ub97c \ubcf5\uc7a1\ud558\uac8c \ub9cc\ub4e0\ub2e4. - View\uac00 state\uc5d0 \uac15\ud558\uac8c bound\ub418\uc11c \uc989\uc2dc update\uac00 \uc77c\uc5b4\ub09c\ub2e4. \uac00\ub054 view\uc640 state\uac04\uc5d0 looser asynchronous relationship\uc744 \ub9cc\ub4e4\uace0 \uc2f6\uc744\ub54c \ub2e8\uc810\uc73c\ub85c \uc791\uc6a9\ud55c\ub2e4. Lecture 5.2 - Functional Reactive Programming What is FRP Reactive Programming\uc740 in time\uc5d0 \uc77c\uc5b4\ub09c \uc774\ubca4\ud2b8\ub4e4\uc758 sequence\uc5d0 reacting\ud558\ub294 \uac83\uc774\ub2e4. Functional view: event sequence\ub97c signal\ub85c \ud569\uce60 \uc218 \uc788\ub2e4. signal\uc740 \uacc4\uc18d\ud574\uc11c \ubcc0\ud558\ub294 value\uc774\ub2e4. mutable state\ub97c \uacc4\uc18d update \ud558\ub294 \ub300\uc2e0 \uc774\ubbf8 \uc788\ub294 signal\uc744 new\ub85c \uc815\uc758\ud560 \uc218 \uc788\ub2e4. Example: Mouse Positions Event-based view: \ub9c8\uc6b0\uc2a4\uac00 \uc6c0\uc9c1\uc77c \ub54c\ub9c8\ub2e4 MouseMoved(toPos: Position) \uc774 fired \ub41c\ub2e4. FRP view: mousePosition: Signal[Position] \ud604\uc7ac \ub9c8\uc6b0\uc2a4 \uc704\uce58\ub97c \ud45c\ud604\ud558\ub294 signal\uc774 \uc788\ub2e4. Fundamental Signal Operations 2\uac1c\uc758 \uae30\ubcf8 operation\uc774 \uc788\ub2e4. 1. \ud604\uc7ac signal\uc758 value\ub97c \uc5bb\ub294 operation. \uc6b0\ub9ac\uac00 \uc815\uc758\ud560 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c\ub294 () \ub85c \ud45c\ud604\ud55c\ub2e4. mousePosition() 2. define a signal in terms of other signal. \uc6b0\ub9ac\uac00 \uc815\uc758\ud560 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c\ub294 Singal \uc0dd\uc131\uc790\ub85c \ud45c\ud604\ud55c\ub2e4. scala def inReactangle(LL: Position, UR: Position): Signal[Boolean] = Signal { val pos = mousePosition() LL <= pos && pos <= UR } Constant Signals \ud56d\uc0c1 same value\ub97c \uac16\ub294 signal\uc744 \uc815\uc758\ud560 \uc218 \uc788\ub2e4. val sig = Signal(3) // the signal that is always 3 \uc2dc\uac04\uc5d0 \ub530\ub77c \ubcc0\ud558\ub294 signal\uc744 \uc5b4\ub5bb\uac8c \uc815\uc758\ud560 \uac83\uc778\uac00? - mousePosition\uac19\uc774 \uc678\ubd80\uc5d0 \uc815\uc758\ub41c signal\uc744 map\uc73c\ub85c \uc21c\ud68c\ud558\ub294 \ubc29\ubc95\uc774 \uc788\ub2e4. - \ub610\ub294 Var \ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc774 \uc788\ub2e4. Signal\uc758 Value\ub294 immutable\ud558\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \ubcc0\uacbd\ub420 \uc218 \uc788\ub294 Signal\uc758 subclass\uc778 Var\ub97c \uad6c\ud604\ud560 \uac83\uc774\ub2e4. Var\ub294 value\ub97c \ud604\uc7ac\uc758 \uac12\uc73c\ub85c \ubcc0\uacbd\ud574\uc8fc\ub294 \"update\" operation\uc744 \uac16\ub294\ub2e4. val sig = Var(3) sig.update(5) scala\uc5d0\uc11c update\ub294 assignment\ub85c \uc4f8 \uc218 \uc788\ub2e4. \uc608\ub97c \ub4e4\uc5b4 arr\uc774\ub77c\ub294 \uc774\ub984\uc758 array\uac00 \uc788\ub2e4\uace0 \ud560 \ub54c arr(i) = 0 // \uc740 \uc544\ub798\uc640 \uac19\uc774 \ubcc0\ud658\ub41c\ub2e4. arr.update(i, 0) update method\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. class Array[T] { def update(idx: Int, value: T): Unit } \uc77c\ubc18\uc801\uc73c\ub85c f() = E\ub294 f.update(E)\ub85c \ucd95\uc57d\ud560 \uc218 \uc788\ub2e4. \uadf8\ub7ec\ubbc0\ub85c sig.update(5) \ub294 \uc544\ub798\uc640 \uac19\uc774 \ucd95\uc57d\ud560 \uc218 \uc788\ub2e4. sig() = 5 Var Signal\uc740 mutable variables\ucc98\ub7fc \ubcf4\uc778\ub2e4. sig() \ub294 \ud604\uc7ac \uac12\uc744 \uc5bb\ub294 \uac83\uc774\uba70 sig() = newValue \ub294 update\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc911\uc694\ud55c \ucc28\uc774\uc810\uc774 \uc788\ub2e4. Var\ub294 future points in time\uc5d0\uc11c \uc790\ub3d9\uc801\uc73c\ub85c \uacc4\uc0b0\ub418\ub294 \uac12\uc744 \uac00\uc9c8 \uc218 \uc788\ub2e4. \ub610\ud55c mutable variables\uc758 exists \ub9e4\ucee4\ub2c8\uc998\uc774 \uc5c6\uace0 \ubaa8\ub4e0 updates\ub97c \uc9c1\uc811 propagate\ud574\uc57c \ud55c\ub2e4. a = 2 b = 2 * a a = a + 1 b = 2 * a // a \uac12\uc774 \ubcc0\ud588\uc9c0\ub9cc b\uac12\uc774 \uc790\ub3d9\uc801\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8 \ub418\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 \ub2e4\uc2dc \ub123\uc5b4\uc918\uc57c \ud55c\ub2e4. a() = 2 b() = 2 * a() a() = 3 // a\uac00 3\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8 \ub410\uc73c\ubbc0\ub85c b\ub294 6\uc73c\ub85c \uc790\ub3d9 \uc5c5\ub370\uc774\ud2b8 \ub41c\ub2e4. \uc544\ub798\uc640 \uac19\uc740 \uacbd\uc6b0\ub294 \ucc98\ub9ac\ud560 \uc218 \uc5c6\ub2e4. s() = s() + 1 \uc774\ub294 s\uac00 \ud56d\uc0c1 \uc790\uae30 \uc790\uc2e0\ubcf4\ub2e4 1\ucee4\uc57c \ud55c\ub2e4\ub294 \uc758\ubbf8\uc774\ubbc0\ub85c \ubd88\uac00\ud558\ub2e4. Lecture 5.3 - A Simple FRP Implementation Thread-Local State global state\ub97c synchronization\ud558\ub824\uba74 concurrent access \ubb38\uc81c\uac00 \uc0dd\uae34\ub2e4. block\uc744 \uc0ac\uc6a9\ud558\uac8c \ub418\uba74 \ub290\ub824\uc9c0\uace0 dealock\uc758 \uc704\ud5d8\uc131\uc774 \uc788\ub2e4. \uc774\ub97c \ud574\uacb7\ud558\uae30 \uc704\ud55c \ubc29\ubc95\uc73c\ub85c global state\ub300\uc2e0 thread-local state\ub97c \ub458 \uc218 \uc788\ub2e4. thread-local state\ub780 \uac01 thread\uac00 variable\uc758 copy\ubcf8\uc5d0 \uc811\uadfc\ud55c\ub2e4\ub294 \uc758\ubbf8\uc774\ub2e4. \uadf8\ub798\uc11c global variable\uc744 \uc0ac\uc6a9\ud558\uc9c0\ub9cc thread\uc0ac\uc774\uc5d4 \uacf5\uc720\uac00 \uc548\ub41c\ub2e4. \uc774\ub97c scala\uc5d0\uc11c\ub294 scala.util.DynamicVariable\ub85c \uc9c0\uc6d0\ud55c\ub2e4.","title":"Coursera/fpd-in-scala"},{"location":"scala/fpd-in-scala/#fdd-in-scala","text":"","title":"fdd-in-scala"},{"location":"scala/fpd-in-scala/#week1-1-recap-functions-and-pattern-matching","text":"Pattern Matching\uc758 \uc608 def show(json: JSON): String = json match { case JSeq(elems) => \"[\" + (elems map show mkString \", \") + \"]\" case JObj(bindings) => val assocs = bindings map { case (key, value) => \"\\\"\" + key + \"\\\": \" + show(value) } \"{\" + (assocs mkString \", \") + \"}\" case JNum(num) => num.toString case JStr(str) => \"\\\"\" + str + \"\\\"\" case JBool(b) => b.toString case JNull => \"null\" } scala\uc5d0\uc11c \ubaa8\ub4e0 concrete type\uc740 class or trait \uc774\ub2e4. function\ub3c4 \uc608\uc678\uac00 \uc544\ub2c8\ub2e4. JBinding => String \uc740 scala.Function1[JBinding, String] \uc758 \ucd95\uc57d\ud615\uc774\ub2e4. scala.Function1\uc740 trait\uc774\uace0 JBinding\uacfc String\uc740 type arguments\uc774\ub2e4.","title":"Week1-1: Recap: Functions and Pattern Matching"},{"location":"scala/fpd-in-scala/#partial-function","text":"another subtype of function, special type\uc774\ub2e4. function\uacfc \ub3d9\uc77c\ud558\uac8c apply\ub97c \uac00\uc9c0\uba74\uc11c isDefinedAt \uc744 \ucd94\uac00\ub85c \uac16\ub294\ub2e4. trait PartialFunction[-A, +R] extends Function1[-A, +R] { def apply(x: A): R def isDefinedAt(x: A): Boolean } val f1: String => String = { case \"ping\" => \"pong\"} f1(\"ping\") // pong f1(\"abc\") // MatchError!!! val f: PartialFunction[String, String] = { case \"ping\" => \"pong\" } f.isDefinedAt(\"ping\") // true f.isDefinedAt(\"pong\") // false \ub9cc\uc57d PartialFunction type\uc774 \uae30\ub300\ub41c\ub2e4\uba74 Scala Compiler\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud655\uc7a5\ud55c\ub2e4. { case \"ping\" => \"pong\" } as follows: new PartialFunction[String, String] { def apply(x: String) = x match { case \u201dping\u201d => \u201dpong\u201d } def isDefinedAt(x: String) = x match { case \u201dping\u201d => true case _ => false } }","title":"Partial Function"},{"location":"scala/fpd-in-scala/#excercise1","text":"val f: PartialFunction[List[Int], String] = { case Nil => \u201done\u201d case x :: y :: rest => \u201dtwo\u201d } f.isDefinedAt(List(1, 2, 3)) // true val g: PartialFunction[List[Int], String] = { case Nil => \u201done\u201d case x :: rest => rest match { case Nil => \u201dtwo\u201d } } g.isDefinedAt(List(1,2,3)) // ture g(List(1,2,3)) // Match Error!!! \uc704\uc5d0\uc11c \ubcf4\ub4ef\uc774 isDefinedAt \uc740 outmost matching block\ub9cc \uac80\uc99d\ud574\uc900\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 g\uc5d0\uc11c\ub294 true\uac00 \ub9ac\ud134 \ub418\ub294 \uac83\uc774\ub2e4. \uc2e4\uc81c\ub85c \uc0ac\uc6a9\uc744 \ud574\ubcf4\uba74 case Nil \ubc16\uc5d0 case\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud55c\ub2e4.","title":"Excercise1"},{"location":"scala/fpd-in-scala/#week1-2-recap-collections","text":"All collection types share a common set of general methods. Core Methods: - map - flatMap - filter and also - foldLeft - foldRight abstract class List[+T] { def map[U](f: T => U): List[U] = this match { case x :: xs => f(x) :: xs.map(f) case Nil => Nil } } abstract class List[+T] { def flatMap[U](f: T => List[U]): List[U] = this match { case x :: xs => f(x) ++ xs.flatMap(f) case Nil => Nil } } map\uacfc\uc758 \ub2e4\ub978\uc810 1. map\uacfc \ub2ec\ub9ac f\uac00 List[U]\ub97c \ub9ac\ud134\ud55c\ub2e4. 2. ++\ub85c List\ub97c concat\ud55c\ub2e4.(List\ub07c\ub9ac\uc758 concat\uc774\ubbc0\ub85c)","title":"Week1-2: Recap: Collections"},{"location":"scala/fpd-in-scala/#week3-1-type-directed-programming","text":"\uc9c0\uae08\uae4c\uc9c0 \ubd24\ub4ef\uc774 compiler\ub294 values\ub85c \ubd80\ud130 types\uc744 \uc720\ucd94\ud560 \uc218 \uc788\ub2e4. val x = 12 compiler\ub294 x\ub97c Int\ub85c \uc720\ucd94\ud55c\ub2e4. \uc65c\ub0d0\ud558\uba74 \uac12\uc774 12\uc774\ubbc0\ub85c \uc544\ub798\uc640 \uac19\uc774 \ubcf5\uc7a1\ud55c \ud45c\ud604\uc5d0\uc11c\ub3c4 \uc774\ub294 \uc801\uc6a9\ub41c\ub2e4. val y = x + 3 compiler\ub294 y \ub610\ud55c Int\ub85c \uc720\ucd94\ud55c\ub2e4. \uc774\ubc88\uc5d0\ub294 \ubc18\ub300\ub85c compiler\uac00 types\ub85c \ubd80\ud130 values\ub97c \uc720\ucd94\ud558\ub294 \uacfc\uc815\uc744 \ubcfc \uac83\uc774\ub2e4. \uc65c \uc774\uac83\uc774 \uc720\uc6a9\ud558\ub0d0? \ud655\uc2e4\ud55c \ud558\ub098\ub294 compiler\uac00 value\ub97c \ucc3e\uc544\uc11c \uc904 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\ubc88 \ub808\uc2a8\uc758 \ub098\uba38\uc9c0\ub294 \uc774\ub7f0 \uba54\uce74\ub2c8\uc998\uc758 motivation\uc744 \uc18c\uac1c\ud558\uace0 \ub2e4\uc74c \ubc88 \ub808\uc2a8\uc740 how to use it\uc744 \uc124\uba85\ud560 \uac83\uc774\ub2e4.","title":"Week3-1: Type-Directed Programming"},{"location":"scala/fpd-in-scala/#motivating-example","text":"parameter\ub85c List[Int]\ub97c \ubc1b\uc544\uc11c \uc815\ub82c\ud55c \uacb0\uacfc\ub97c List[Int]\ub85c \ub9ac\ud134\ud558\ub294 \ud568\uc218\ub97c \uc0dd\uac01\ud574\ubcf4\uc790. def sort(xs: List[Int]): List[Int] = { ... ... if (x < y) ... ... } \uc0c1\uc138 \ucf54\ub4dc\ub294 \uc5ec\uae30\uc5d0\uc11c \ud544\uc694\uac00 \uc5c6\uae30\uc5d0 \uc0dd\ub7b5\ud588\ub2e4. \uc704 \ucf54\ub4dc\ub294 Int\uc5d0 \ub300\ud574\uc11c\ub9cc \uc801\uc6a9 \uac00\ub2a5\ud558\ubbc0\ub85c general\ud558\uac8c \ubaa8\ub4e0 \ud0c0\uc785\uc5d0 \ub300\ud574\uc11c\ub3c4 \ub3d9\uc791\ud558\uac8c \ud558\uace0 \uc2f6\ub2e4. \uc774\uc5d0 \ub300\ud55c straightforward approach\ub294 polymorphic type\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. def sort[A](xs: List[A]): List[A] = ... \ud558\uc9c0\ub9cc \uc774\uac83\ub9cc\uc73c\ub85c\ub294 \ubd80\uc871\ud558\ub2e4. \uc65c\ub0d0\ud558\uba74 \uac01 type\ubcc4\ub85c compare\ub97c \ub2e4\ub974\uac8c \ud574\uc57c \ud558\uae30 \ub54c\ubb38\uc774\ub2e4. \uadf8\ub798\uc11c \uc774\ubc88\uc5d4 \uac01 compare \ud568\uc218\ub97c parameter\ub85c \ubc1b\ub3c4\ub85d \ud574\ubcf4\uc790. def sort[A](xs: List[A])(lessThan: (A, A) => Boolean): List[A] = { ... ... if (lessThan(x, y)) ... ... } \uadf8\ub807\uac8c \ub418\uba74 \uc544\ub798\uc640 \uac19\uc774 \uac00\ub2a5\ud558\ub2e4' val xs = List(-5, 6, 3, 2, 7) val strings = List(\"apple\", \"pear\", \"orange\", \"pineapple\") sort(xs)((x, y) => x < y) sort(strings)((s1, s2) => s1.compareTo(s2) < 0)","title":"Motivating Example"},{"location":"scala/fpd-in-scala/#refactoring-with-ordering","text":"scala\ub294 standard library \uc5d0\uc11c comparing \ud558\ub294 \ud568\uc218\ub97c \uae30\ubcf8\uc73c\ub85c \uc81c\uacf5\ud55c\ub2e4. package scala.math trait Ordering[A] { def compare(a1: A, a2: A): Int def lt(a1: A, a2: A): Boolean = compare(a1, a2) <= 0 ... } compare \ud568\uc218\ub294 2\uac1c\uc758 parameter\ub97c \ubc1b\uc544\uc11c \uccab \ubc88\uc9f8 \uac12\uc774 \ud074 \uacbd\uc6b0 \uc591\uc218, \uc791\uc744 \uacbd\uc6b0 \uc74c\uc218, \ub3d9\uc77c\ud55c \uacbd\uc6b0 0\uc744 \ub9ac\ud134\ud55c\ub2e4. \uc774\ub97c \uc0ac\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ubcc0\uacbd \uac00\ub2a5\ud558\ub2e4. def sort[A](xs: List[A])(ord: Ordering[A]): List[A] = { ... ... if (ord.lt(x, y)) ... ... } import scala.math.Ordering sort(xs)(Ordering.Int) sort(strings)(Ordering.String) \uc5ec\uae30\uc5d0\uc11c \uc0ac\uc6a9 \uc911\uc778 Int\uc640 String\uc740 types \uc774 \uc544\ub2c8\uace0 values \uc784\uc744 \uc54c\uc544\uc57c \ud55c\ub2e4. scala\uc5d0\uc11c\ub294 types\uacfc values\uc5d0 \ub3d9\uc77c\ud55c symbol\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uac00\ub2a5\ud558\ub2e4. object Ordering { val Int = new Ordering[Int] { def compare(x: Int, y: Int) = if (x > y) 1 else if (x < y) -1 else 0 } }","title":"Refactoring With Ordering"},{"location":"scala/fpd-in-scala/#reducing-boilerplate","text":"\uc9c0\uae08\uae4c\uc9c0 \uc815\uc758\ud55c \uac83\uc744 \ub530\ub974\uba74 \uc798 \ub3d9\uc791\ud55c\ub2e4. \ud558\uc9c0\ub9cc \ubaa8\ub4e0 \uacbd\uc6b0\uc5d0 \ub300\ud574 boilerplate\uac00 \uc874\uc7ac\ud558\uac8c \ub41c\ub2e4. Int\ub97c \ube44\uad50\ud560 \ub54c\ub9c8\ub2e4 Ordering.Int \ub97c \ubc18\ubcf5\uc801\uc73c\ub85c \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4. sort(xs)(Ordering.Int) sort(ys)(Ordering.Int) sort(strings)(Ordering.String)","title":"Reducing Boilerplate"},{"location":"scala/fpd-in-scala/#implicit-parameters","text":"implicit \uc744 \uba85\uc2dc\ud568\uc73c\ub85c\uc11c compiler\uac00 argument ord \ub97c support\ub97c \ud558\uac8c \ud560 \uc218 \uc788\ub2e4. def sort[A](xs: List[A])(implicit ord: Ordering[A]): List[A] = ... sort(xs) sort(ys) sort(strings) \uc704\uc640 \uac19\uc774 \ud558\uba74 \ucef4\ud30c\uc77c\ub7ec\uac00 value\uc5d0 \ub9de\ucdb0 type\uc744 \uacb0\uc815\ud55c\ub2e4. \ucef4\ud30c\uc77c\ub7ec\uac00 \uc218\ud589\ud558\ub294 \uacfc\uc815\uc744 \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\uc790. sort(xs) xs \uac00 List[Int] \ud0c0\uc785\uc774\ubbc0\ub85c \ucef4\ud30c\uc77c\ub7ec\ub294 \uc704\uc758 \ucf54\ub4dc\ub97c \uc544\ub798\uc640 \uac19\uc774 \ubcc0\ud658\ud55c\ub2e4. sort[Int](xs) \uadf8\ub9ac\uace0 \ucef4\ud30c\uc77c\ub7ec\ub294 candidate definition\uc911 Ordering[Int] \ud0c0\uc785\uc5d0 \ub9de\ub294 \uac83\uc744 \ucc3e\ub294\ub2e4. \uc704\uc758 \ucf00\uc774\uc2a4\uc5d0\uc11c\ub294 Ordering.Int\uc640 only matching\ub418\uace0 \ucef4\ud30c\uc77c\ub7ec\ub294 method sort\ub85c \uc774\ub97c \uc804\ub2ec\ud55c\ub2e4. sort[Int](xs)(Ordering.Int) candidate values\uac00 \uc5b4\ub5bb\uac8c \uc815\uc758\ub3c4\uc5b4\uc788\ub294 \uc9c0\ub97c \uc0b4\ud3b4 \ubcf4\uae30 \uc804\uc5d0 implicit \ud0a4\uc6cc\ub4dc\uc5d0 \ub300\ud574 \uc790\uc138\ud788 \uc54c\uc544\ubcf4\uc790. method\ub294 \uc624\uc9c1 \ud558\ub098\uc758 implicit parameter list\ub97c \uac00\uc9c8 \uc218 \uc788\uc73c\uba70 \uc774\ub294 \ub9c8\uc9c0\ub9c9 paramter\uac00 \ub418\uc57c \ud55c\ub2e4. At call site, the arguments of the given clause are usually left out, although it is possible to explicitly pass them: // Argument inferred by the compiler sort(xs) // Explicit argument sort(xs)(Ordering.Int.reverse)","title":"Implicit Parameters"},{"location":"scala/fpd-in-scala/#candidates-for-implicit-parameters","text":"\ucef4\ud30c\uc77c\ub7ec\uac00 type T\uc5d0 \ub300\ud574 \uc5b4\ub5a4 candidate definition\ub97c \ucc3e\uc744\uae4c? \ucef4\ud30c\uc77c\ub7ec\ub294 \uc544\ub798 definition\uc744 \ucc3e\ub294\ub2e4. have type T, are marked implicit, are visible at the point of the function call, or are defined in a companion object associated with T. most specific\ud55c \uc815\uc758\ub97c \ucc3e\uac8c \ub418\uba74 \uadf8\uac83\uc744 \uc0ac\uc6a9\ud558\uace0 \uc5c6\ub2e4\uba74 error\ub97c report\ud55c\ub2e4.","title":"Candidates for Implicit Parameters"},{"location":"scala/fpd-in-scala/#implicit-definition","text":"implicit definition\uc774\ub780 implicit \ud0a4\uc6cc\ub4dc\uc640 \ud568\uaed8 \uc815\uc758\ub41c \uac83\uc744 \ub9d0\ud55c\ub2e4. object Ordering { implicit val Int: Ordering[Int] = ... } \uc704\uc758 \ucf54\ub4dc\ub294 Int\ub77c\ub294 \uc774\ub984\uc744 \uac00\uc9c4 Ordering[Int] \ud0c0\uc785\uc758 implicit value\ub97c \uc815\uc758\ud55c \uac83\uc774\ub2e4. Any val, lazy val, def, or object definition can be marked implicit. \ub9c8\uc9c0\ub9c9\uc73c\ub85c implicit definitions\ub294 type parameters\uc640 implicit parameters\ub97c \uac00\uc9c8 \uc218 \uc788\ub2e4. implicit def orderingPair[A, B](implicit orderingA: Ordering[A], orderingB: Ordering[B] ): Ordering[(A, B)] = ...","title":"implicit Definition"},{"location":"scala/fpd-in-scala/#implicit-search-scope","text":"type T\uc758 implicit value\ub97c \ucc3e\uae30 \uc704\ud574 \uccab \ubc88\uc9f8\ub85c visible(inherited, imported, or defined in an enclosing scope)\ud55c \ubaa8\ub4e0 implicit definitions\ub97c \ucc3e\ub294\ub2e4. \ub9cc\uc57d \ucef4\ud30c\uc77c\ub7ec\uac00 lexcial scope\uc5d0\uc11c implicit instance\uc640 \ub9e4\uce6d\ub418\ub294 type T\ub97c \ucc3e\uc9c0 \ubabb\ud558\uba74, T\uc640 \uad00\ub828\ub41c companion objects\uc5d0\uc11c \uc774\uc5b4\uc11c \ucc3e\ub294\ub2e4. (companion objects\uc640 types\ub294 other types\uc640 \uc5f0\uad00\uc788\ub2e4.) A companion object is an object that has the same name as a type. \uc608\ub85c object scala.math.Ordering is the companion of the type scala.math.Ordering. The types associated with a type T are: if T has parent types T\u2081 with T\u2082 ... with T\u2099, the union of the parts of T\u2081, ... T\u2099 as well as T itself, if T is a parameterized type S[T\u2081, T\u2082, ..., T\u2099], the union of the parts of S and T\u2081, ..., T\u2099, otherwise, just T itself. As an example, consider the following type hierarchy: trait Foo[A] trait Bar[A] extends Foo[A] trait Baz[A] extends Bar[A] trait X trait Y extends X \ub9cc\uc57d Bar[Y] \ud0c0\uc785\uc758 implicit value\uac00 \ud544\uc694\ud558\ub2e4\uba74 compiler\ub294 \uc544\ub798\uc640 \uac19\uc740 companion object\uc5d0\uc11c implicit definition\uc744 \ucc3e\uc744 \uac83\uc774\ub2e4. Bar, because it is a part of Bar[Y], Y, because it is a part of Bar[Y], Foo, because it is a parent type of Bar, and X, because it is a parent type of Y. However, the Baz companion object will not be visited.","title":"Implicit Search Scope"},{"location":"scala/fpd-in-scala/#implicit-search-process","text":"search process\ub294 no candidate found \ud639\uc740 \ub9e4\uce6d\ub418\ub294 \ucd5c\uc18c\ud55c \ud558\ub098\uc758 candidate\ub97c \uacb0\uacfc\ub97c \ub9cc\ub4e4\uc5b4 \ub0b8\ub2e4. \ub9cc\uc57d no no available implicit definition matching \uc774\ub77c\uba74 \uc5d0\ub7ec\uac00 repot \ub41c\ub2e4. scala> def f(implicit n: Int) = () scala> f ^ error: could not find implicit value for parameter n: Int \ubc18\ub300\ub85c \ub458 \uc774\uc0c1\uc758 implicit definition\uc774 eligibale \ud558\ub2e4\uba74 ambiguity\uac00 report \ub41c\ub2e4. scala> implicit val x: Int = 0 scala> implicit val y: Int = 1 scala> def f(implicit n: Int) = () scala> f ^ error: ambiguous implicit values: both value x of type => Int same type\uc5d0 \ub9e4\uce6d\ub418\ub294 several implicit definitions\uac00 \uc788\uc5b4\ub3c4 \ud558\ub098\ub97c \ud2b9\uc815 \ud560 \uc218 \uc788\ub2e4\uba74 \uad1c\ucc2e\ub2e4. A definition a: A is more specific than a definition b: B if: type A has more \u201cfixed\u201d parts, or, a is defined in a class or object which is a subclass of the class defining b. Let\u2019s see a few examples of priorities at work. Which implicit definition matches the Int implicit parameter when the following method f is called? implicit def universal[A]: A = ??? implicit def int: Int = ??? def f(implicit n: Int) = () f \uc704\uc758 \uacbd\uc6b0\uc5d0\uc11c universal\uc740 type paramter\ub97c \uc9c0\ub2c8\uace0 int\ub294 \uc544\ub2c8\uae30\uc5d0, int\uac00 more fixed parts\ub97c \uac16\uace0 \uc774\ub294 universal\ubcf4\ub2e4 \uba3c\uc800 \uace0\ub824\ub41c\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \ucef4\ud30c\uc77c\ub7ec\uac00 int\ub97c \uc120\ud0dd\ud568\uc5d0 \uc788\uc5b4 ambiguity\uac00 \uc5c6\ub2e4. \uc544\ub798\uc640 \uac19\uc774 \uc788\uc744 \ub54c implicit Int \ud30c\ub77c\ubbf8\ud130\ub97c \uac16\ub294 f method\ub294 \uc5b4\ub290 implicit definition\uc5d0 \ub9e4\uce58 \ub420\uae4c? trait A { implicit val x: Int = 0 } trait B extends A { implicit val y: Int = 1 def f(implicit n: Int) = () f } y\uac00 A\ub97c extend\ud558\ub294 trait\uc774\ubbc0\ub85c y\uac00 A\ubcf4\ub2e4 more specific \ud558\ub2e4. \uadf8\ub7ec\ubbc0\ub85c \ucef4\ud30c\uc77c\ub7ec\uac00 y\ub97c \uc120\ud0dd\ud558\ub294 \uac83\uc5d0 ambiguity\ub294 \uc5c6\ub2e4.","title":"Implicit Search Process"},{"location":"scala/fpd-in-scala/#context-bounds","text":"Syntactic sugar allows the omission of the implicit parameter list: def printSorted[A: Ordering](as: List[A]): Unit = { println(sort(as)) } Type parameter A has one context bound: Ordering. This is equivalent to writing: def printSorted[A](as: List[A])(implicit ev1: Ordering[A]): Unit = { println(sort(as)) } More generally, a method definition such as: def f[A: U\u2081 ... : U\u2099](ps): R = ... Is expanded to: def f[A](ps)(implicit ev\u2081: U\u2081[A], ..., ev\u2099: U\u2099[A]): R = ...","title":"Context Bounds"},{"location":"scala/fpd-in-scala/#implicit-query","text":"At any point in a program, one can query an implicit value of a given type by calling the implicitly operation: scala> implicitly[Ordering[Int]] res0: Ordering[Int] = scala.math.Ordering$Int$@73564ab0 Note that implicitly is not a special keyword, it is defined as a library operation: def implicitly[A](implicit value: A): A = value","title":"Implicit Query"},{"location":"scala/fpd-in-scala/#summary","text":"In this lesson we have introduced the concept of type-directed programming, a language mechanism that infers values from types. There has to be a unique (most specific) implicit definition matching the queried type for it to be selected by the compiler. Implicit values are searched in the enclosing lexical scope (imports, parameters, inherited members) as well as in the implicit scope of the queried type. The implicit scope of type is made of implicit values defined in companion objects of types associated with the queried type.","title":"Summary"},{"location":"scala/fpd-in-scala/#week3-2-type-classes","text":"In the previous lectures we have seen a particular pattern of code combining parameterized types and implicits. We have defined a parameterized type Ordering[A], implicit instances of that type for concrete types A, and implicit parameters of type Ordering[A]: trait Ordering[A] { def compare(a1: A, a2: A): Int } object Ordering { implicit val Int: Ordering[Int] = new Ordering[Int] { def compare(x: Int, y: Int) = if (x < y) -1 else if (x > y) 1 else 0 } implicit val String: Ordering[String] = new Ordering[String] { def compare(s: String, t: String) = s.compareTo(t) } } def sort[A: Ordering](xs: List[A]): List[A] = ... We say that Ordering is a type class. Type classes provide yet another form of polymorphism. The method sort can be called with lists containing elements of any type A for which there is an implicit value of type Ordering[A]. At compile-time, the compiler resolves the specific Ordering implementation that matches the type of the list elements.","title":"Week3-2: Type Classes"},{"location":"scala/fpd-in-scala/#retroactive-extension","text":"Type classes let us add new features to data types without changing the original definition of these data types. For instance, consider the following Rational type, modeling a rational number: /** A rational number * @param num Numerator * @param denom Denominator */ case class Rational(num: Int, denom: Int) We can add the capability \"to be compared\" to the type Rational by defining an implicit instance of type Ordering[Rational]: object RationalOrdering { implicit val orderingRational: Ordering[Rational] = new Ordering[Rational] { def compare(q: Rational, r: Rational): Int = q.num * r.denom - r.num * q.denom } }","title":"Retroactive Extension"},{"location":"scala/fpd-in-scala/#laws","text":"So far, we have shown how to implement instances of a type class, for some specific types (Int, String, and Rational). Now, let\u2019s have a look at the other side: how to use (and reason about) type classes. For example, the sort function is written in terms of the Ordering type class, whose implementation is itself defined by each specific instance, and is therefore unknown at the time the sort function is written. If an Ordering instance implementation is incorrect, then the sort function becomes incorrect too! To prevent this from happening, type classes are often accompanied by laws, which describe properties that instances must satisfy, and that users of type classes can rely on. Can you think of properties that instances of the type class Ordering must satisfy, so that we can be confident that the method sort won\u2019t be broken? Instances of the Ordering[A] type class must satisfy the following properties: inverse: the sign of the result of comparing x and y must be the inverse of the sign of the result of comparing y and x, transitive: if a value x is lower than y and that y is lower than z, then x must also be lower than z, consistent: if two values x and y are equal, then the sign of the result of comparing x and z should be the same as the sign of the result of comparing y and z. The authors of a type class should think about such kind of laws and they should provide ways for instance implementers to check that these laws are satisfied.","title":"Laws"},{"location":"scala/fpd-in-scala/#example-of-type-class-ring","text":"Let\u2019s see how we can define a type class modeling a ring structure. A ring is an algebraic structure defined as follows according to Wikipedia: In mathematics, a ring is one of the fundamental algebraic structures used in abstract algebra. It consists of a set equipped with two binary operations that generalize the arithmetic operations of addition and multiplication. Through this generalization, theorems from arithmetic are extended to non-numerical objects such as polynomials, series, matrices and functions. This structure is so common that, by abstracting over the ring structure, developers could write programs that could then be applied to various domains (arithmetic, polynomials, series, matrices and functions). A ring is a set equipped with two binary operations, + and *, satisfying the following laws (called the ring axioms): (a + b) + c = a + (b + c) + is associative a + b = b + a + is commutative a + 0 = a 0 is the additive identity a + -a = 0 -a is the additive inverse of a (a * b) * c = a * (b * c) * is associative a * 1 = a 1 is the multiplicative identity a * (b + c) = a * b + a * c left distributivity (b + c) * a = b * a + c * a right distributivity Here is how we can define a ring type class in Scala: trait Ring[A] { def plus(x: A, y: A): A def mult(x: A, y: A): A def inverse(x: A): A def zero: A def one: A } Here is how we define an instance of Ring[Int]: object Ring { implicit val ringInt: Ring[Int] = new Ring[Int] { def plus(x: Int, y: Int): Int = x + y def mult(x: Int, y: Int): Int = x * y def inverse(x: Int): Int = -x def zero: Int = 0 def one: Int = 1 } } Finally, this is how we would define a function that checks that the + associativity law is satisfied by a given Ring instance: def plusAssociativity[A](x: A, y: A, z: A)(implicit ring: Ring[A]): Boolean = ring.plus(ring.plus(x, y), z) == ring.plus(x, ring.plus(y, z)) Note: in practice, the standard library already provides a type class Numeric, which models a ring structure.","title":"Example of Type Class: Ring"},{"location":"scala/fpd-in-scala/#summary_1","text":"In this lesson we have identified a new programming pattern: type classes. Type classes provide a form of polymorphism: they can be used to implement algorithms that can be applied to various types. The compiler selects the type class implementation for a specific type at compile-time. A type class definition is a trait that takes type parameters and defines operations that apply to these types. Generally, a type class definition is accompanied by laws, checking that implementations of their operations are correct.","title":"Summary"},{"location":"scala/fpd-in-scala/#week3-3-conditional-implicit-definitions","text":"In this lesson, we will see that implicit definitions can themselves take implicit parameters. Let\u2019s start with an example. Consider how we order two String values: is \"abc\" lexicographically before \"abd\"? To answer this question, we need to compare all the characters of the String values, element-wise: is a before a? No. is b before b? No. is c before d? Yes! We conclude that \"abc\" is before \"abd\". So, we compare two sequences of characters with an algorithm that compares the characters of the sequences element-wise. Said otherwise, we can define an ordering relation for sequence of characters based on the ordering relation for characters. Can we generalize this process to sequences of any element type A for which there is an implicit Ordering[A] instance? The signature of such an Ordering[List[A]] definition takes an implicit parameter of type Ordering[A]: implicit def orderingList[A](implicit ord: Ordering[A]): Ordering[List[A]] For reference, a complete implementation is shown below. You can see that at some point in the algorithm we call the operation compare of the ord parameter: implicit def orderingList[A](implicit ord: Ordering[A]): Ordering[List[A]] = new Ordering[List[A]] { def compare(xs: List[A], ys: List[A]) = (xs, ys) match { case (x :: xsTail, y :: ysTail) => val c = ord.compare(x, y) if (c != 0) c else compare(xsTail, ysTail) case (Nil, Nil) => 0 case (_, Nil) => 1 case (Nil, _) => -1 } } With this definition, we can sort a list of list of numbers, for example: scala> val xss = List(List(1, 2, 3), List(1), List(1, 1, 3)) res0: List[List[Int]] = List(List(1, 2, 3), List(1), List(1, 1, 3)) scala> sort(xss) res1: List[List[Int]] = List(List(1), List(1, 1, 3), List(1, 2, 3)) But let\u2019s take a step back. We haven\u2019t defined an instance of Ordering[List[Int]] and yet we have been able to sort a list of List[Int] elements! How did the compiler manage to provide such an instance to us? This happened in several steps. First, we called sort(xss). The compiler fixed the type parameter A of the method to List[Int], based on the type of the argument xss, as if we had written: sort[List[Int]](xss) Then, the compiler searched for an implicit definition of type Ordering[List[Int]]. It found that our orderingList definition could be a match under the condition that it could also find an implicit definition of type Ordering[Int], which it eventually found. Finally, the compiler inserted the following arguments for us: sort[List[Int]](xss)(orderingList(Ordering.Int)) In this case, the compiler combined two implicit definitions (orderingList and Ordering.Int) before terminating. In general, though, an arbitrary number of implicit definitions can be combined until the search hits a \u201cterminal\u201d definition. Consider for instance these four implicit definitions: implicit def a: A = ... implicit def aToB(implicit a: A): B = ... implicit def bToC(implicit b: B): C = ... implicit def cToD(implicit c: C): D = ... We can then ask the compiler to summon a value of type D: implicitly[D] The compiler finds that there is a candidate definition, cToD, that can provide such a D value, under the condition that it can also find an implicit definition of type C. Again, it finds that there is a candidate definition, bToC, that can provide such a C value, under the condition that it can also find an implicit definition of type B. Once again, it finds that there is candidate definition, aToB, that can provide such a B value, under the condition that it can also find an implicit value of type A. Finally, it finds a candidate definition for type A and the algorithm terminates! At the beginning of this lesson, we showed that by using implicit parameters the compiler could infer simple arguments for us. We have now reached a point where we can appreciate that the compiler can infer more complex arguments (by inferring arguments of arguments!). It not only significantly reduces code verbosity, it also alleviates developers from implementing parts of their programs, which are summoned by the compiler based on their type (hence the name \u201ctype-directed programming\u201d). In practice, complex fragments of programs such as serializers and deserializers of data types can be summoned by the compiler.","title":"Week3-3: Conditional Implicit Definitions"},{"location":"scala/fpd-in-scala/#recursive-implicit-definitions","text":"What happens if we write an implicit definition that depends on itself? trait X implicit def loop(implicit x: X): X = x implicitly[X] The compiler detects that it keeps searching for an implicit definition of the same type and returns an error: error: diverging implicit expansion for type X starting with method loop Note: it is possible to write recursive implicit definitions by making sure that the search always terminates, but this is out of the scope of this lesson.","title":"Recursive Implicit Definitions"},{"location":"scala/fpd-in-scala/#example-sort-by-multiple-criteria","text":"Consider a situation where we want to compare several movies. Each movie has a title, a rating (in number of \u201cstars\u201d), and a duration (in minutes): case class Movie(title: String, rating: Int, duration: Int) val movies = Seq( Movie(\"Interstellar\", 9, 169), Movie(\"Inglourious Basterds\", 8, 140), Movie(\"Fight Club\", 9, 139), Movie(\"Zodiac\", 8, 157) ) We want to sort movies by rating first, and then by duration. To achieve this, a first step is to change our sort function to take as parameter the sort criteria in addition to the elements to sort: def sort[A, B](elements: Seq[A])(critera: A => B)(implicit ord: Ordering[B] ): Seq[A] = ... The sort algorithm remains the same except that instead of comparing the elements together, we compare the criteria applied to each element. With this function, here is how we can sort movies by title: sort(movies)(_.title) And here is how we can sort them by rating: sort(movies)(_.rating) Each time the sort function is called, its ordering parameter is inferred by the compiler based on the type of the criteria (String and then Int, in the above examples). However, our initial problem was to sort the movies by multiple criteria. We would like to sort first by rating and then by duration: sort(movies)(movie => (movie.rating, movie.duration)) The type of the criteria is now a tuple type (Int, Int). Unfortunately, the compiler is unable to infer the corresponding ordering parameter. We need to define how simple orderings can be combined together to get an ordering for multiple criteria. We do so by defining the following implicit ordering: implicit def orderingPair[A, B](implicit orderingA: Ordering[A], orderingB: Ordering[B] ): Ordering[(A, B)] = ... This definition provides an ordering for pairs of type (A, B) given orderings for types A and B. The complete implementation is the following: implicit def orderingPair[A, B](implicit orderingA: Ordering[A], orderingB: Ordering[B] ): Ordering[(A, B)] = new Ordering[(A, B)] { def compare(pair1: (A, B), pair2: (A, B)): Int = { val firstCriteria = orderingA.compare(pair1._1, pair2._1) if (firstCriteria != 0) firstCriteria else orderingB.compare(pair1._2, pair2._2) } } We first compare the two values according to the first criteria, and if they are equal we compare them according to the second criteria. With this definition, the compiler is now able to infer the ordering for the following call: sort(movies)(movie => (movie.rating, movie.duration)) Here is the same call where the inferred parameter is explicitly written: sort(movies)(movie => (movie.rating, movie.duration))( orderingPair(Ordering.Int, Ordering.Int) ) Note that in the standard library the sort function that we have defined here is already available as a method sortBy on collections.","title":"Example: Sort by Multiple Criteria"},{"location":"scala/fpd-in-scala/#summary_2","text":"In this lesson, we have seen that: implicit definitions can also take implicit parameters, an arbitrary number of implicit definitions can be chained until a terminal definition is reached.","title":"Summary"},{"location":"scala/fpd-in-scala/#wee5-timely-effects","text":"","title":"Wee5: Timely Effects"},{"location":"scala/fpd-in-scala/#lecture-51-imperative-event-handling-the-observer-pattern","text":"The Observer Pattern\uc740 model\uc758 \ubcc0\ud654\uc5d0 \ub530\ub77c views\uac00 \ubcc0\uacbd\ub418\ub294 \uac83\uc774\ub2e4. publish / subscribe mode/view/controller(MVC) \ub77c\uace0\ub3c4 \ubd88\ub9b0\ub2e4. trait Publisher { private var subscribers: Set[Subscriber] = Set() def subscribe(subscriber: Subscriber): Unit = subscribers += subscriber def unsubscribe(subscriber: Subscriber): Unit = subscribers -= subscriber def publish(): Unit = subscribers.foreach(_.handler(this)) } trait Subscriber { def handler(pub: Publisher) } class BankAccount extends Publisher { private var balance = 0 def currentBalance: Int = balance def deposit(amount: Int): Unit = if (amount > 0) { balance = balance + amount publish() } def withdraw(amount: Int): Unit = if (0 < amount && amount <= balance) { balance = balance - amount publish() } else throw new Error(\"insufficient funds\") } class Consolidator(observed: List[BankAccount]) extends Subscriber { observed.foreach(_.subscribe(this)) private var total: Int = _ compute() private def compute() = total = observed.map(_.currentBalance).sum def handler(pub: Publisher) = compute() def totalBalance = total } val a = new BankAccount val b = new BankAccount val c = new Consolidator(List(a, b)) c.totalBalance a deposit 20 c.totalBalance b deposit 30 c.totalBalance \uc704 \ucf54\ub4dc\ub294 BankAccount\ub97c Observer Pattern\uc73c\ub85c \uad6c\ud604\ud55c \uac83\uc774\ub2e4. Observer Pattern\uc758 \uc7a5\uc810\uc740 - view\ub97c state\ub85c\ubd80\ud130 \ubd84\ub9ac\ud560 \uc218 \uc788\ub2e4. - \uc8fc\uc5b4\uc9c4 state\ub85c \uc5ec\ub7ec \uac1c\uc758 views\ub97c \ub9cc\ub4e4 \uc218 \uc788\ub2e4. - set up\uc774 \uac04\ub2e8\ud558\ub2e4. \ub2e8\uc810\uc740 - handler\uac00 Unit-typed\uc5ec\uc11c \uba85\ub839\ud615 \uc2a4\ud0c0\uc77c\uc774\ub2e4. - \ub9ce\uc740 moving parts\uac00 co-ordinated \ub418\uc5b4\uc57c \ud55c\ub2e4. - Concurrency\uac00 \ubb38\uc81c\ub97c \ubcf5\uc7a1\ud558\uac8c \ub9cc\ub4e0\ub2e4. - View\uac00 state\uc5d0 \uac15\ud558\uac8c bound\ub418\uc11c \uc989\uc2dc update\uac00 \uc77c\uc5b4\ub09c\ub2e4. \uac00\ub054 view\uc640 state\uac04\uc5d0 looser asynchronous relationship\uc744 \ub9cc\ub4e4\uace0 \uc2f6\uc744\ub54c \ub2e8\uc810\uc73c\ub85c \uc791\uc6a9\ud55c\ub2e4.","title":"Lecture 5.1 - Imperative Event Handling: The Observer Pattern"},{"location":"scala/fpd-in-scala/#lecture-52-functional-reactive-programming","text":"","title":"Lecture 5.2 - Functional Reactive Programming"},{"location":"scala/fpd-in-scala/#what-is-frp","text":"Reactive Programming\uc740 in time\uc5d0 \uc77c\uc5b4\ub09c \uc774\ubca4\ud2b8\ub4e4\uc758 sequence\uc5d0 reacting\ud558\ub294 \uac83\uc774\ub2e4. Functional view: event sequence\ub97c signal\ub85c \ud569\uce60 \uc218 \uc788\ub2e4. signal\uc740 \uacc4\uc18d\ud574\uc11c \ubcc0\ud558\ub294 value\uc774\ub2e4. mutable state\ub97c \uacc4\uc18d update \ud558\ub294 \ub300\uc2e0 \uc774\ubbf8 \uc788\ub294 signal\uc744 new\ub85c \uc815\uc758\ud560 \uc218 \uc788\ub2e4.","title":"What is FRP"},{"location":"scala/fpd-in-scala/#example-mouse-positions","text":"Event-based view: \ub9c8\uc6b0\uc2a4\uac00 \uc6c0\uc9c1\uc77c \ub54c\ub9c8\ub2e4 MouseMoved(toPos: Position) \uc774 fired \ub41c\ub2e4. FRP view: mousePosition: Signal[Position] \ud604\uc7ac \ub9c8\uc6b0\uc2a4 \uc704\uce58\ub97c \ud45c\ud604\ud558\ub294 signal\uc774 \uc788\ub2e4.","title":"Example: Mouse Positions"},{"location":"scala/fpd-in-scala/#fundamental-signal-operations","text":"2\uac1c\uc758 \uae30\ubcf8 operation\uc774 \uc788\ub2e4. 1. \ud604\uc7ac signal\uc758 value\ub97c \uc5bb\ub294 operation. \uc6b0\ub9ac\uac00 \uc815\uc758\ud560 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c\ub294 () \ub85c \ud45c\ud604\ud55c\ub2e4. mousePosition() 2. define a signal in terms of other signal. \uc6b0\ub9ac\uac00 \uc815\uc758\ud560 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c\ub294 Singal \uc0dd\uc131\uc790\ub85c \ud45c\ud604\ud55c\ub2e4. scala def inReactangle(LL: Position, UR: Position): Signal[Boolean] = Signal { val pos = mousePosition() LL <= pos && pos <= UR }","title":"Fundamental Signal Operations"},{"location":"scala/fpd-in-scala/#constant-signals","text":"\ud56d\uc0c1 same value\ub97c \uac16\ub294 signal\uc744 \uc815\uc758\ud560 \uc218 \uc788\ub2e4. val sig = Signal(3) // the signal that is always 3 \uc2dc\uac04\uc5d0 \ub530\ub77c \ubcc0\ud558\ub294 signal\uc744 \uc5b4\ub5bb\uac8c \uc815\uc758\ud560 \uac83\uc778\uac00? - mousePosition\uac19\uc774 \uc678\ubd80\uc5d0 \uc815\uc758\ub41c signal\uc744 map\uc73c\ub85c \uc21c\ud68c\ud558\ub294 \ubc29\ubc95\uc774 \uc788\ub2e4. - \ub610\ub294 Var \ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc774 \uc788\ub2e4. Signal\uc758 Value\ub294 immutable\ud558\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \ubcc0\uacbd\ub420 \uc218 \uc788\ub294 Signal\uc758 subclass\uc778 Var\ub97c \uad6c\ud604\ud560 \uac83\uc774\ub2e4. Var\ub294 value\ub97c \ud604\uc7ac\uc758 \uac12\uc73c\ub85c \ubcc0\uacbd\ud574\uc8fc\ub294 \"update\" operation\uc744 \uac16\ub294\ub2e4. val sig = Var(3) sig.update(5) scala\uc5d0\uc11c update\ub294 assignment\ub85c \uc4f8 \uc218 \uc788\ub2e4. \uc608\ub97c \ub4e4\uc5b4 arr\uc774\ub77c\ub294 \uc774\ub984\uc758 array\uac00 \uc788\ub2e4\uace0 \ud560 \ub54c arr(i) = 0 // \uc740 \uc544\ub798\uc640 \uac19\uc774 \ubcc0\ud658\ub41c\ub2e4. arr.update(i, 0) update method\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. class Array[T] { def update(idx: Int, value: T): Unit } \uc77c\ubc18\uc801\uc73c\ub85c f() = E\ub294 f.update(E)\ub85c \ucd95\uc57d\ud560 \uc218 \uc788\ub2e4. \uadf8\ub7ec\ubbc0\ub85c sig.update(5) \ub294 \uc544\ub798\uc640 \uac19\uc774 \ucd95\uc57d\ud560 \uc218 \uc788\ub2e4. sig() = 5 Var Signal\uc740 mutable variables\ucc98\ub7fc \ubcf4\uc778\ub2e4. sig() \ub294 \ud604\uc7ac \uac12\uc744 \uc5bb\ub294 \uac83\uc774\uba70 sig() = newValue \ub294 update\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc911\uc694\ud55c \ucc28\uc774\uc810\uc774 \uc788\ub2e4. Var\ub294 future points in time\uc5d0\uc11c \uc790\ub3d9\uc801\uc73c\ub85c \uacc4\uc0b0\ub418\ub294 \uac12\uc744 \uac00\uc9c8 \uc218 \uc788\ub2e4. \ub610\ud55c mutable variables\uc758 exists \ub9e4\ucee4\ub2c8\uc998\uc774 \uc5c6\uace0 \ubaa8\ub4e0 updates\ub97c \uc9c1\uc811 propagate\ud574\uc57c \ud55c\ub2e4. a = 2 b = 2 * a a = a + 1 b = 2 * a // a \uac12\uc774 \ubcc0\ud588\uc9c0\ub9cc b\uac12\uc774 \uc790\ub3d9\uc801\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8 \ub418\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 \ub2e4\uc2dc \ub123\uc5b4\uc918\uc57c \ud55c\ub2e4. a() = 2 b() = 2 * a() a() = 3 // a\uac00 3\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8 \ub410\uc73c\ubbc0\ub85c b\ub294 6\uc73c\ub85c \uc790\ub3d9 \uc5c5\ub370\uc774\ud2b8 \ub41c\ub2e4. \uc544\ub798\uc640 \uac19\uc740 \uacbd\uc6b0\ub294 \ucc98\ub9ac\ud560 \uc218 \uc5c6\ub2e4. s() = s() + 1 \uc774\ub294 s\uac00 \ud56d\uc0c1 \uc790\uae30 \uc790\uc2e0\ubcf4\ub2e4 1\ucee4\uc57c \ud55c\ub2e4\ub294 \uc758\ubbf8\uc774\ubbc0\ub85c \ubd88\uac00\ud558\ub2e4.","title":"Constant Signals"},{"location":"scala/fpd-in-scala/#lecture-53-a-simple-frp-implementation","text":"","title":"Lecture 5.3 - A Simple FRP Implementation"},{"location":"scala/fpd-in-scala/#thread-local-state","text":"global state\ub97c synchronization\ud558\ub824\uba74 concurrent access \ubb38\uc81c\uac00 \uc0dd\uae34\ub2e4. block\uc744 \uc0ac\uc6a9\ud558\uac8c \ub418\uba74 \ub290\ub824\uc9c0\uace0 dealock\uc758 \uc704\ud5d8\uc131\uc774 \uc788\ub2e4. \uc774\ub97c \ud574\uacb7\ud558\uae30 \uc704\ud55c \ubc29\ubc95\uc73c\ub85c global state\ub300\uc2e0 thread-local state\ub97c \ub458 \uc218 \uc788\ub2e4. thread-local state\ub780 \uac01 thread\uac00 variable\uc758 copy\ubcf8\uc5d0 \uc811\uadfc\ud55c\ub2e4\ub294 \uc758\ubbf8\uc774\ub2e4. \uadf8\ub798\uc11c global variable\uc744 \uc0ac\uc6a9\ud558\uc9c0\ub9cc thread\uc0ac\uc774\uc5d4 \uacf5\uc720\uac00 \uc548\ub41c\ub2e4. \uc774\ub97c scala\uc5d0\uc11c\ub294 scala.util.DynamicVariable\ub85c \uc9c0\uc6d0\ud55c\ub2e4.","title":"Thread-Local State"},{"location":"scala/fpp-in-scala/","text":"fpp-in-scala Week1 Lecture 1.1 - Programming Paradigms functional Programming\uc740 paradigm\uc774\ub2e4. classical imperative paradimg(Java or C)\uacfc \uc57d\uac04 \ub2e4\ub978. scala\uc5d0\uc11c\ub294 \uc774 2\uac1c\uc758 paradigm\uc744 \ud569\uce60 \uc218\ub3c4 \uc788\ub2e4. \uc774\ub294 \ub2e4\ub978 \uc5b8\uc5b4\uc5d0\uc11c\uc758 migration\uc744 \uc27d\uac8c \ud574\uc900\ub2e4. In science, a paradigm describes distinct concepts or thought patterns in some scientific discipline. Main Programming Paradigms: - imperative programming - functional programming - logic programming object-oriented programming\ub3c4 paradigm\uc774\ub77c\uace0 \ud558\ub294 \uc0ac\ub78c\ub4e4\ub3c4 \uc788\uc9c0\ub9cc \uc790\uc2e0\uc758 \uc0dd\uac01\uc73c\ub85c\ub294 \uc704 3\uac1c\uc758 \uad50\ucc28\uc810\uc5d0 \uc788\ub2e4\uace0 \uc0dd\uac01\ud55c\ub2e4. Imperative Programming modifying mutable variables using assignments and control structures such as if-then-else, loops, break, continue, return Von Neumann computer\uc758 sequence\ub97c \uc774\ud574\ud558\ub294 \uac83\uc740 imperative program\uc744 \uc774\ud574\ud558\ub294 most common informal way\uc774\ub2e4. Processor <------BUS ------> Memory Problem: Scaling up. How can we avoid conceptualizing programs word by word? high-level abstractions(collections, polynomials, geomtric shapes, strings, documents..)\ub97c \uc815\uc758\ud558\ub294 \ud14c\ud06c\ub2c9\uc774 \ud544\uc694\ud558\ub2e4. Ideally: Develop theories of collections, shapes, strings, ... What is a theory A theory consist of - one or more data types - operations on these types - laws that describe the relationships between values and operations \ubcf4\ud1b5 theory\ub294 mutations \ub97c describe\ud558\uc9c0 \uc54a\ub294\ub2e4! mutation: identity\ub294 \uc720\uc9c0\ud558\uba74\uc11c something\uc744 change\ud558\ub294 \uac83\uc774\ub2e4. Theories without mutations theory of polynomials (a x + b) + (c x + d) = (a+c)*x + (b+d) theory of strings (a ++ b) ++ c = a ++ (b ++ c) Consequences for Programming mathematical theroies\ub97c \ub530\ub974\uba74\uc11c high-level concepts \uad6c\ud604\uc744 \ud558\ub824\uba74 mutation\uc740 \uc5c6\uc5b4\uc57c \ud55c\ub2e4. - theroies do not admit it - mutation\uc740 theories\uc758 useful laws\ub97c destoy \ud560 \uc218 \uc788\ub2e4. \uadf8\ub7ec\ubbc0\ub85c - concentrate on defining theories for operators expressed as functions - avoid mutations - have powerful ways to abstract and compose functions start of function programming means avoid mutations Functional Programming In a restricted sense, FP means programming without mutable variables, assignments, loops, and other imperative control structures In a wider sense, FP meas focusing on the functions In particular, functions can be valuses that are produced, consumed, and composed All this becomes easier in a functional language Functional Programming Language In a restricted sense, a functional programming language is one which does not have mutable variables, assignments, or imperative control structures. In a wider sense, a functional programming language enables the construction of elegant programs that focus on functions. In particular, functions in a FP language are first-class citizens. This means they can be defined anywhere, including inside other functions like any other value, they can be passed as parameters to functions and returned as results as for other values, there exists a set operators to compose functions Some functional programming languages In the restricted sense: - Pure Lisp, XSLT, XPath, XQuery, FP - Haskell (without I/O Monad or UnsafePerformIO) In the wider sense: - Lisp, Scheme, Racket, Clojure \u25b6 SML, Ocaml, F# - Haskell (full language) - Scala - Smalltalk, Ruby (!) Why Functional Programming? Functional Programming is becoming increasingly popular because it offers the following benfits. - simpler reasoning principles - better modularity - good for exploiting parallelism for multicore and cloud computing. my summary \uc6b0\ub9ac\ub294 \uc218\ud559\uc744 \ubc30\uc6b0\uba74\uc11c mutable variables\ub97c \ubc30\uc6b4 \uc801\uc774 \uc5c6\ub2e4. 1 + 1 = 2\uc774\uace0 a + b = 3 \uc774\ub77c\uba74 \uadf8\ub0e5 3\uc778 \uac83\uc774\ub2e4. \uc624\ub298\uc740 a + b = 3 \uc774\uc5c8\ub294\ub370 \ub0b4\uc77c\uc740 a + b = 4\uc77c \uc21c \uc5c6\uc5c8\ub2e4. (ax^2 + bx + c \ub294 \uc5ec\ub7ec \uac12\uc774 \ub420 \uc218 \uc788\uaca0\uc9c0\ub9cc) \ud558\uc9c0\ub9cc imperative programming\uc5d0\uc11c\ub294 \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \uac1c\ub150\uc774\ub2e4. int a = 1; int b = 2; a + b = 3; a = 4; a + b = 6; \uc65c \uc218\ud559\uc801\uc778 \uc6d0\uce59\uc744 \uaebc\ub0b4 \ub4e4\uc5c8\ub0d0? module\ud654 \ub54c\ubb38\uc774\ub2e4. \ud504\ub85c\uadf8\ub7a8\uc774 \ubcf5\uc7a1\ud574\uc9c0\uba74\uc11c \ubaa8\ub4c8\ud654\ub294 \ud544\uc218\uc774\ub2e4. \uc798 \ub41c \ubaa8\ub4c8\ud654\ub780 \ubb34\uc5c7\uc77c\uae4c? \ud56d\uc0c1 \ub3d9\uc77c\ud55c \uacb0\uacfc\ub97c \ub9ac\ud134\ud558\ub294 \ubaa8\ub4c8\uc77c \uac83\uc774\ub2e4. map \ud568\uc218\ub97c \uc0dd\uac01\ud574\ubcf4\uba74 \uc5b4\ub290 \ud0c0\uc785\uc5d0 \uc0c1\uad00\uc5c6\uc774 List[U]\ub97c \ub9ac\ud134\ud55c\ub2e4. \uc774\ub7f0 \uc218\ud559\uc801 \uc6d0\uce59\ub4e4\uc740 mutable variables\ub97c \uc778\uc815\ud558\uc9c0 \uc54a\ub294\ub2e4. \uadf8\ub807\uae30\uc5d0 functional programming language\uc5d0 \uc798 \ub9de\ub294\ub2e4. fp\ub294 \uc774\ub7f0 \ubaa8\ud638\ud568\uc744 \uc81c\uac70\ud568\uc73c\ub85c\uc11c \uc6d0\uce59\uc744 \ubcf4\ub2e4 \uc798 \uad6c\ud604\ud558\uace0 \ubaa8\ub4c8\ud654 \ud558\uae30 \uc88b\uc73c\uba70 multicore\uc640 cloud computing \ud658\uacbd\uc5d0\uc11c \ubcd1\ub82c\ucc98\ub9ac\ub97c \uc798 \ud560 \uc218 \uc788\uac8c \ud574\uc900\ub2e4. substitution model \ud568\uc218\uc758 argument\ub97c \uc67c\ucabd\ubd80\ud130 \ubaa8\ub450 \ud3c9\uac00 \ud568\uc218\uc758 \uc624\ub978\ucabd\ubd80\ud130 \uad50\uccb4 -> \ubaa8\ub4e0 Expression\uc5d0 \uc0ac\uc6a9 \uac00\ub2a5 , No side effect foundation of functional programming\uc778 \ub78c\ub2e4 calculus\uc5d0 formalized \ub418\uc5b4\uc788\ub2e4. \ubaa8\ub4e0 Expr\uc774 reduce to a value? (X) \uc544\ub798\uc640 \uac19\uc740 \uc608\uac00 \uc788\ub2e4. def loop: Int = loop Evaluation Stratigies CBV(Call By Value), CBN(Call By Name) - CBV: \ubaa8\ub4e0 args\ub294 \ud55c \ubc88\ub9cc \ud3c9\uac00\ud55c\ub2e4\ub294 \uc7a5\uc810 - CBN: \ud638\ucd9c \ub420 \ub54c\uae4c\uc9c0 not evaluted\ub41c\ub2e4\ub294 \uc7a5\uc810 \ub9cc\uc57d CBV\uac00 \uc885\ub8cc\ub41c\ub2e4\uba74 CBN\ub3c4 \uc885\ub8cc\ub41c\ub2e4? (O) \ubc18\ub300\ub85c CBN\uc774 \uc885\ub8cc\ub41c\ub2e4\uba74 CBV\ub3c4 \uc885\ub8cc\ub41c\ub2e4? (X) scala\uc5d0\uc11c CBN\uc744 \uc4f0\ub294 \ubc29\ubc95\uc740 parameter\uc5d0 => \ub97c \ubd99\uc774\uba74 \ub41c\ub2e4. def myFunc(a:=> Int) = a Value Definitions val x = 2 val y = square(x) // \ubc14\ub85c \ud3c9\uac00\ub41c\ub2e4. def loop: Boolean = loop def x = loop // (O) def\ub294 \ud638\ucd9c\ub420\ub54c \ud3c9\uac00\ub41c\ub2e4. val x = loop // (X) Error def and(x: Boolean, y: Boolean) = if (x) y else false and(false, loop) // (X) Error def and2(x: Boolean, y:=> Boolean) = if (x) y else false and2(false, loop) // false Nested Functions small func\ub85c \ubd84\ub9ac\ud558\ub294 \uac83. good FP styles sqrtIter, imporve \uac19\uc740 \ud568\uc218\ub4e4\uc740 \uc678\ubd80\uc5d0 \uacf5\uac1c(direct \ud638\ucd9c) \ud558\uace0 \uc2f6\uc9c0 \uc54a\uc744 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c \ubcf4\uc870 \ud568\uc218\ub4e4\uc744 \ub0b4\ubd80 \ud568\uc218\ub85c \ub460\uc73c\ub85c\uc368 name-space pollution\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\ub2e4. def sqrt(x: Double) = { def improve def sqrtIter } Lexical Scoping outer block\uc5d0 \uc788\ub294 definitions\ub294 inside block\uc5d0\uc11c visible\ud558\ub2e4. \ubcf4\ud1b5 \ubb38\uc7a5 \ub77c\uc778 \ub05d ; \ub294 optional\uc774\ub2e4. \ub2e4\ub9cc \ud55c \ubb38\uc7a5\uc5d0 \uc5ec\ub7ec expr\uc744 \ud45c\ud604\ud560 \ub54c\ub294 \ud544\uc218 \uc774\ub2e4. val y = x + 1; y + y Tail Recursion calls itself as its last action. the function's stack frame can be reused (one stack frame\uc774 \ud544\uc694\ud558\uba70, tail calls \ub77c\uace0 \ud568) @tailrec annotation\uc744 \ud568\uc218 \uc704\uc5d0 \ucd94\uac00\ud558\uba74 \ud574\ub2f9 \ud568\uc218\uac00 tail recur \ud558\uc9c0 \uc54a\uc744 \uc2dc \uc624\ub958\uac00 \ubc1c\uc0dd\ud55c\ub2e4. \uc544\ub798\uc758 factorial \ud568\uc218\ub294 tailrc \ud568\uc218\uac00 \uc544\ub2c8\uba70 gcd\ub294 tailrec \ud568\uc218\uc774\ub2e4. def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b) def factorial(n: Int): Int = if (n == 0) 1 else n * factorial(n - 1) \uadf8 \ucc28\uc774\ub294 gcd\ub294 \uc2a4\ud15d\uc744 \uc9c4\ud589\uc744 \uacc4\uc18d \ud558\ub354\ub77c\ub3c4 \ubcf8\uc778 \ud638\ucd9c\ub9cc \uacc4\uc18d \ud558\uac8c \ub418\uc9c0\ub9cc factorial \uac19\uc740 \uacbd\uc6b0\uc5d0\ub294 \uc88c\uce21\uc774 \uacc4\uc18d \ub298\uc5b4\ub09c\ub2e4 4 * factorial(3) \uc774\ub97c tail recursive \ud558\uac8c \ubcc0\uacbd\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. def factorial(n: Int): Int = { @tailrec def loop(acc: Int, n: Int): Int = if (n == 0) acc else loop(acc * n, n -1) loop(1, n) } Donal Knuth said premature optimization is the source of the evil Week2 Higher order functions pass functions as arguments and retun them as results. functional languages treat functions as first-class values. = like any other value, a function can be passed as a parameter and returned as a result. provides a flexible way to compose program. Anonymous Function \ud568\uc218\ub97c parameter\ub85c \uc804\ub2ec\ud558\ub2e4\ubcf4\uba74 many small function\uc744 \ub9cc\ub4e4\uac8c \ub41c\ub2e4. \uadf8\ub807\uac8c \ub418\uba74 \uac01\uac01\uc758 naming\uc744 \uc815\ud558\ub294 \uac83\uc740 \uc5b4\ub835\uac8c \ub41c\ub2e4. => anonymous function\uc744 \uc0ac\uc6a9\ud55c\ub2e4. def str = \"abc\"; println(str) println(\"abc\") \uc704\uc5d0\ub294 str \ubcc0\uc218\ub97c \uc815\uc758\ud574\uc11c \ud638\ucd9c\ud588\uace0 \uc544\ub798\ub294 \uc815\uc758 \uc5c6\uc774 \uc0ac\uc6a9 \ud588\ub2e4. \uc774\uac83\uc774 \uac00\ub2a5\ud55c \uc774\uc720\ub294 \ubb58\uae4c? => str\uc740 literals \uc774\uae30 \ub54c\ubb38\uc774\ub2e4. \ub9c8\ucc2c\uac00\uc9c0\ub85c \uc774\ub984 \uc5c6\uc774 \ud568\uc218\ub97c \uc4f0\uba74 function literals\uac00 \ub41c\ub2e4. = anonymous functions // cube anonymous func (x: Int) => x * x * x (x: Int) \ub294 parameter x * x * x \ub294 body def sum(f: Int => Int, a: Int, b: Int): Int = { @tailrec def loop(a: Int, acc: Int): Int = { if (a > b) acc else loop(a + 1, f(a) + acc) } loop(a, 0) } def sumInts(a: Int, b: Int) = sum(x => x, a, b) def sumCubes(a: Int, b: Int) = sum(x => x * x * x, a, b) Currying \uc544\ub798 \ud568\uc218\ub97c \ub354 \uc9e7\uac8c \ud560 \uc218\ub294 \uc5c6\uc744\uae4c? def sumInts(a: Int, b: Int) = sum(x => x, a, b) def sum() def sum(f: Int => Int)(a: Int, b: Int): Int = if (a > b) 0 else f(a) + sum(f)(a + 1, b) def product(f: Int => Int)(a: Int, b: Int): Int = if (a > b) 1 else f(a) * product(f)(a + 1, b) \uc704\uc640 \uac19\uc740 \uc2a4\ud0c0\uc77c\uc758 definition\uacfc function\uc744 currying\uc774\ub77c\uace0 \ubd80\ub978\ub2e4. Haskell Brooks Curry\uc758 \uc774\ub984\uc744 \ub534 \ub124\uc774\ubc0d\uc774\ub2e4. Idea\ub294 \uadf8 \ubcf4\ub2e4 \uc804\uc778 Schonfinkel\uacfc Frege\uc5d0 \uc758\ud574\uc11c \ub098\uc654\uc9c0\ub9cc currying\uc774\ub780 \ub124\uc784\uc73c\ub85c \uad73\uc5b4\uc84c\ub2e4. Example: Finding Fixed Points A number x is called a fixed point(\uace0\uc815 \uc810) of a function f if f(x) = x \uc608\ub85c f: x => 1 + x/2 \ub77c \ud560 \ub54c fixed point\ub294 2\uc774\ub2e4. f(2) = 2\uc774\ubbc0\ub85c \uba87\uba87 \ud568\uc218\ub4e4\uc740 f\ub97c \ubc18\ubcf5\uc801\uc73c\ub85c \uc218\ud589\ud568\uc73c\ub85c\uc11c fixed point\ub97c \ucc3e\uc744 \uc218 \uc788\ub2e4. x, f(x), f(f(x)), f(f(f(x))), ... initial estimate\ub85c \uc2dc\uc791\ud574\uc11c f\ub97c \ubc18\ubcf5\uc801\uc73c\ub85c \uc218\ud589\ud558\ub2e4\ubcf4\uba74 \ub354 \uc774\uc0c1 \ubcc0\ud558\uc9c0 \uc54a\ub294 \uac12 \ud639\uc740 \ubcc0\uacbd\uc774 \ucda9\ubd84\ud788 \uc801\uc5b4\uc84c\uc744 \ub54c\uc758 \uac12\uc744 fixed point\ub77c \ubd80\ub97c \uc218 \uc788\ub2e4. import math.abs val tolerance = 0.0001 def isCloseEnough(x: Double, y: Double): Boolean = abs((x - y) / x) / x < tolerance def fixedPoint(f: Double => Double)(firstGuess: Double) = { @tailrec def iterate(guess: Double): Double = { val next = f(guess) if (isCloseEnough(guess, next)) next else iterate(next) } iterate(firstGuess) } fixedPoint(x => 1 + x/2)(1) // 1.9975 def sqrt(x: Double) = fixedPoint(y => x / y)(1) sqrt(2) // \ubb34\ud55c loop \uc704\uc758 \uc608\uc5d0\uc11c sqrt(2)\ub97c \uc218\ud589\ud558\uba74 \ubb34\ud55c loop\uac00 \ubc1c\uc0dd\ud55c\ub2e4. 1\uacfc 2 \ub97c \uacc4\uc18d \ubc18\ubcf5\ud55c\ub2e4 \uc774\ub97c \ud574\uacb0 \ud558\uae30 \uc704\ud574\uc11c\ub294 \uccab \ubc88\uc9f8 \uacc4\uc0b0 \uac12\uacfc \ub450 \ubc88\uc9f8 \uacc4\uc0b0 \uac12\uc758 \ud3c9\uade0\uc744 \uad6c\ud558\uba74 \ub41c\ub2e4. def sqrt(x: Double) = fixedPoint(y => (y + x / y) / 2)(1) functions as return values \uc704\uc758 \uc608\uc81c\uc5d0\uc11c \ud3c9\uade0\uc744 \ud1b5\ud574 \uc548\uc815\ud654\uc2dc\ud0a4\ub294 \uae30\uc220\uc740 \ucd94\uc0c1\ud654 \ub420 \uc218 \uc788\ub2e4. def averageDamp(f: Double => Double)(x: Double) = (x + f(x)) / 2 def sqrt3(x: Double) = fixedPoint(averageDamp(y => x / y))(1) Higher Order Function\uc774 \ud56d\uc0c1 \uc633\uc740 \uac83\uc740 \uc544\ub2c8\uba70 \uc801\uc808 \ud560 \ub54c \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4. Functions and Data Classes class Rational(x: Int, y: Int): def numer = x def denom = y \uc704 \uc815\uc758\ub294 two entities\ub97c \uc0dd\uc131\ud55c\ub2e4. - Rational \uc774\ub77c\ub294 \uc774\ub984\uc758 new type - \uc774 type\uc758 element\ub97c \ub9cc\ub4e4\uae30 \uc704\ud55c Rational constructo \uc2a4\uce7c\ub77c\ub294 types\uacfc value\uc758 names\ub97c different namespace \uc5d0 \ubcf4\uad00\ud558\uae30 \ub54c\ubb38\uc5d0 \ucda9\ub3cc\uc744 \uac71\uc815\ud560 \ud544\uc694 \uc5c6\ub2e4. Objects elements of a class type\uc744 objects\ub77c\uace0 \ubd80\ub978\ub2e4. class \uc758 \uc0dd\uc131\uc790\ub97c calling \ud568\uc73c\ub85c\uc11c object\ub97c \ub9cc\ub4e4 \uc218 \uc788\ub2e4. Rational(1, 2) \uc544\ub798\uc640 \uac19\uc774 class\ub0b4 member\uc5d0 \uc811\uadfc \uac00\ub2a5\ud558\ub2e4 val x = Rational(1 ,2) x.numer x.denom object rationals { val x = new Rational(1, 3) val y = new Rational(5, 7) val z = new Rational(3, 2) x.add(y).mul(z) } class Rational(x: Int, y: Int) { def numer = x def denom = y def add(r: Rational) = new Rational(numer * r.denom + r.numer * denom, denom * r.denom) def mul(r: Rational) = new Rational(numer * r.numer, denom * r.denom) def neg = new Rational(-numer, denom) def sub(r: Rational) = add(r.neg) override def toString = s\"$numer/$denom\" } More Fun With Rationals Client's view\uc5d0\uc11c\ub294 \ub0b4\ubd80\uac00 \uc5b4\ub5bb\uac8c \ub3d9\uc791\ud558\ub358\uc9c0 \ub3d9\uc77c\ud558\uac8c \ubcf4\uc778\ub2e4. without affecting client\ub97c \ud558\uba74\uc11c \ub2e4\ub978 \uad6c\ud604\uc744 \uc120\ud0dd\ud558\ub294 \uac83\uc744 data abstraction \uc774\ub77c\uace0 \ud55c\ub2e4. S/E\uc5d0\uc11c\uc758 cornerstone\uc774\ub2e4. Self Reference inside of a class, this \ub294 \ud604\uc7ac \uc2e4\ud589 \uc911\uc778 method\ub0b4\uc5d0\uc11c\uc758 object\ub97c \uc758\ubbf8\ud55c\ub2e4 Preconditions require \ub85c class\uc5d0 \uc870\uac74\uc744 \ucd94\uac00\ud560 \uc218 \uc788\ub2e4. \uc870\uac74\uc5d0 \ub9de\uc9c0 \uc54a\uc73c\uba74 IllegalArgumentException\uc774 \ubc1c\uc0dd\ud558\uba70 \ucd94\uac00\ud55c \uc5d0\ub7ec \uba54\uc138\uc9c0\uac00 \ucd9c\ub825\ub41c\ub2e4. Assertions require\uc640 \ube44\uc2b7\ud55c \uc758\ubbf8\uc774\ub2e4. require\uc640 \ub3d9\uc77c\ud558\uac8c condtion\uacfc optional message string\uc744 \ubc1b\ub294\ub2e4. val x = sqrt(y) assert(x >= 0) fail\uc77c \uacbd\uc6b0 assert\ub294 require\uc640 \ub2ec\ub9ac AssertionError\ub97c \ubc1c\uc0dd\ud55c\ub2e4. require\ub294 \ud568\uc218 \ud638\ucd9c\uc790\uc5d0\uac8c precondition\uc744 \uac15\uc694\ud560 \ub54c \uc4f0\uc778\ub2e4 assert\ub294 \ud568\uc218 \uc790\uc2e0\uc774 \uccb4\ud06c \ud560 \ub54c \uc0ac\uc6a9\ud55c\ub2e4. Constructors \ubaa8\ub4e0 class\ub294 primary constructor(\uae30\ubcf8 \uc0dd\uc131\uc790)\uac00 \uc554\uc2dc\uc801\uc73c\ub85c \uc788\ub2e4. - class\uc758 \ubaa8\ub4e0 paramters\ub97c \ubc1b\uace0 - class body\uc758 \ubaa8\ub4e0 statement\ub97c \uc2e4\ud589\ud55c\ub2e4. Java \uac19\uc774 \uc5ec\ub7ec \uc0dd\uc131\uc790\ub97c \uac16\ub294 \uac83\ub3c4 \uac00\ub2a5\ud558\ub2e4. object rationals { val x = new Rational(1, 3) val y = new Rational(5, 7) val z = new Rational(3, 2) x.add(y).mul(z) y.add(y) x.less(y) x.max(y) new Rational(2) } class Rational(x: Int, y: Int) { require(y != 0, \"denominator must be nonezero\") def this(x: Int) = this(x, 1) // \uc5ec\uae30\uc5d0\uc11c\uc758 this\ub294 constructor \uc758\ubbf8\ub85c \uc4f0\uc778\ub2e4. private def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b) private val g = gcd(x, y) // val\ub85c \uc120\uc5b8\ud588\uae30\uc5d0 \ubc14\ub85c \ud3c9\uac00\ub418\uc11c \ub2e4\uc74c \ubd80\ud134 \uacc4\uc0b0\uc744 \uc548\ud558\uace0 \uc7ac\uc0ac\uc6a9\ud55c\ub2e4. def numer = x / g // def numer = x / gcd(x,y) // \ub9cc\uc57d \uc774\uc640 \uac19\uc774 \uc120\uc5b8 \ud558\uba74 \ub9e4\ubc88 gcd\ub97c \uacc4\uc0b0\ud574\uc57c \ud55c\ub2e4. \uacc4\uc0b0 \ub9ac\uc18c\uc2a4\uac00 \ud06c\uace0 \uac00\ub054 \ud638\ucd9c\ub420 \ub54c \uc0ac\uc6a9\ud558\uba74 \uc88b\ub2e4. def denom = y / g def less(that: Rational) = numer * that.denom < that.numer * denom def max(that: Rational) = if (this.less(that)) that else this def add(r: Rational) = new Rational(numer * r.denom + r.numer * denom, denom * r.denom) def mul(r: Rational) = new Rational(numer * r.numer, denom * r.denom) def neg = new Rational(-numer, denom) def sub(r: Rational) = add(r.neg) override def toString = s\"$numer/$denom\" } Evaluation and Operators Operators Infix Notation parameter\ub97c \uac16\ub294 \ubaa8\ub4e0 \uba54\uc18c\ub4dc\ub294 infix operaotr\ucc98\ub7fc \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. r add s r.add(s) r less s r.less(s) r max s r.max(s) Relaxed Identifiers operaotr\ub294 identifier\ub85c \uc0ac\uc6a9\ub420 \uc218 \uc788\ub2e4. - \uc601\ubb38\uc790: \ubb38\uc790\ub85c \uc2dc\uc791\ud558\uace0, \ub4a4\uc5d0\ub294 \ubb38\uc790 \ud639\uc740 \uc22b\uc790\uac00 \uc62c \uc218 \uc788\ub2e4. - Symbolic: operator symbol\ub85c \uc2dc\uc791\ud574\uc11c, \ub2e4\ub978 \uc2ec\ubcfc\uc774 \ub4a4\uc5d0 \uc62c \uc218 \uc788\ub2e4. - _ \ubb38\uc790\ub294 \ubb38\uc790\ub85c \uce74\uc6b4\ud2b8 \ub41c\ub2e4 - \uc601\ubb38\uc790 identifiers\ub294 underscore\ub85c \ub05d\ub0a0 \uc218 \uc788\uace0 \ub4a4\uc5d0 \ub2e4\ub978 operator symbols\uac00 \ubd99\uc744 \uc218 \uc788\ub2e4. * \ub9cc\uc57d \ub05d\uc774 symbol\ub4e4\ub85c \ub05d\ub098\uba74 \ub4a4\uc5d0 \ud0c0\uc785\uc744 \uc704\ud55c : \uacfc \ud55c \uce78 \ub744\uc6cc\uc57c \ud55c\ub2e4. examples - x1 - * - +?%& - vector_++ - counter_= -a \ucc98\ub7fc \ube7c\uae30\uac00 \uc544\ub2c8\ub77c \ub9c8\uc774\ub108\uc2a4 operator\ub97c \ucd94\uac00\ud558\uace0 \uc2f6\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud574\uc57c \ud55c\ub2e4. ( unary_ \uac00 \uc55e\uc5d0 \ubd99\uc5b4\uc57c \ud558\uace0 : \uacfc \ud55c\uce78 \ub744\uc6cc \uc368\uc57c \ud55c\ub2e4.) def unary_- : Rational = new Rational(-numer, denom) Precedence Rules \uc5f0\uc0b0\uc790 \uc6b0\uc120\uc21c\uc704. \uccab \ubc88\uc9f8 \ubb38\uc790\uc5d0 \ub530\ub77c \uacb0\uc815\ub41c\ub2e4. Java\ud639\uc740 C\uc640 \ucc28\uc774 \uc5c6\ub2e4. 1\ubc88\uc774 \uac00\uc7a5 \ub0ae\uc740 \uc21c\uc704\uc774\ub2e4. (all letters) | ^ & < > = ! : - / % (all other special values) class Rational(x: Int, y: Int) { require(y != 0, \"denominator must be nonezero\") def this(x: Int) = this(x, 1) // \uc5ec\uae30\uc5d0\uc11c\uc758 this\ub294 constructor \uc758\ubbf8\ub85c \uc4f0\uc778\ub2e4. private def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b) private val g = gcd(x, y) // val\ub85c \uc120\uc5b8\ud588\uae30\uc5d0 \ubc14\ub85c \ud3c9\uac00\ub418\uc11c \ub2e4\uc74c \ubd80\ud134 \uacc4\uc0b0\uc744 \uc548\ud558\uace0 \uc7ac\uc0ac\uc6a9\ud55c\ub2e4. def numer = x / g // def numer = x / gcd(x,y) // \ub9cc\uc57d \uc774\uc640 \uac19\uc774 \uc120\uc5b8 \ud558\uba74 \ub9e4\ubc88 gcd\ub97c \uacc4\uc0b0\ud574\uc57c \ud55c\ub2e4. \uacc4\uc0b0 \ub9ac\uc18c\uc2a4\uac00 \ud06c\uace0 \uac00\ub054 \ud638\ucd9c\ub420 \ub54c \uc0ac\uc6a9\ud558\uba74 \uc88b\ub2e4. def denom = y / g // def less(that: Rational) = numer * that.denom < that.numer * denom def < (that: Rational) = numer * that.denom < that.numer * denom def max(that: Rational) = if (this.<(that)) that else this def +(r: Rational) = new Rational(numer * r.denom + r.numer * denom, denom * r.denom) def mul(r: Rational) = new Rational(numer * r.numer, denom * r.denom) def unary_- : Rational = new Rational(-numer, denom) def -(that: Rational) = this + -that override def toString = s\"$numer/$denom\" } Week3 Class Hierarchies \uc2e4\uc81c \uba54\uc18c\ub4dc\ub294 runtime type\uc5d0 \uc758\uc874\ud55c\ub2e4. \uc774\ub97c dynamic binding\uc774\ub77c\uace0 \ud55c\ub2e4. \uc774\ub294 OOP\uc5d0 \uae30\ubcf8 \uc694\uc18c\uc774\ub2e4. \uc544\ub798 \ud568\uc218\ub294 abstract class\uc774\ub2e4. abstract class IntSet { def incl(x: Int): IntSet def contains(x: Int): Boolean } \ucd94\uc0c1 \ud074\ub798\uc2a4\ub294 - \uad6c\ud604\uccb4\uac00 \uc5c6\ub294 \uba64\ubc84\ub97c \ud3ec\ud568\ud560 \uc218 \uc788\ub2e4. - new operator\ub97c \uc0ac\uc6a9\ud55c \uc778\uc2a4\ud134\uc2a4 \uc0dd\uc131\uc744 \ud560 \uc218 \uc5c6\ub2e4. abstract class IntSet { def incl(x: Int): IntSet def contains(x: Int): Boolean } class NonEmpty(elem: Int, left: IntSet, right: IntSet) extends IntSet { override def incl(x: Int): IntSet = if (x < elem) new NonEmpty(elem, left incl x, right) else if (x > elem) new NonEmpty(elem, left, right incl x) else this override def contains(x: Int): Boolean = if (x < elem) left contains x else if (x > elem) right contains x else true override def toString = \"{\" + left + elem + right + \"}\" } class Empty extends IntSet { override def incl(x: Int) = new NonEmpty(x, new Empty, new Empty) override def contains(x: Int) = false override def toString = \".\" } val t1 = new NonEmpty(3, new Empty, new Empty) val t2 = t1 incl 4 \uc704\uc758 \uc608\uc5d0\uc11c IntSet\uc740 Empty\uc640 NonEmpty\uc758 superclass\uc774\ub2e4. Empty\uc640 NonEmpty\ub294 IntSet\uc758 subclasses\uc774\ub2e4. \uc2a4\uce7c\ub77c\uc5d0\uc11c superclass\uac00 \uc5c6\uc73c\uba74 java.lang\uc5d0 \uc788\ub294 Java standard class Object \ub97c \uc0c1\uc18d \ubc1b\ub294\ub2e4. \ud074\ub798\uc2a4\uc758 direct or indirect superclass\ub97c base classes\ub77c\uace0 \ud55c\ub2e4. NonEmpty\uc640 IntSet\uc758 base classes\ub294 Object\uc774\ub2e4. non-abstract definition\uc744 redfine\ud560 \ub54c\ub294 override keyword\ub97c \uc368\uc918\uc57c \ud55c\ub2e4. abstract class Base { def foo = 1 def bar: Int } class Sub extends Base { override def foo = 2 def bar = 3 } Object Definitions \uc704\uc758 \uc608\uc5d0\uc11c \uc720\uc800\uac00 \ub9ce\uc740 EmptySet\uc744 \ub9cc\ub4e4\uac8c \ub418\uba74 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud55c\ub2e4. \uadf8\ub798\uc11c \uc774\ub97c object\ub85c \uc120\uc5b8\ud558\ub294 \uac83\uc774 \ub0ab\ub2e4. object Empty extends IntSet { def incl(x: Int) = new NonEmpty(x, new Empty, new Empty) def contains(x: Int) = false } \uc774\ub807\uac8c \ud558\uba74 Empty\ub77c\ub294 \uc774\ub984\uc758 singleton object\uac00 \ub9cc\ub4e4\uc5b4 \uc9c4\ub2e4. \uc774\ub85c\uc368 \ub2e4\ub978 Empty \uc778\uc2a4\ud134\uc2a4\ub294 \ub9cc\ub4e4\uc5b4\uc9c8 \uc218 \uc5c6\ub2e4. Singleton Object\ub294 values \uc774\ubbc0\ub85c, Empty\ub294 \ubc14\ub85c \ud3c9\uac00\ub41c\ub2e4. Programs Scala\uc5d0\uc11c standalone application\uc744 \ub9cc\ub4dc\ub294 \uac83\uc740 \uac00\ub2a5\ud558\ub2e4. main method\ub97c \ud3ec\ud568\ud558\ub294 object\ub97c \ub9cc\ub4e4\uba74 \ub41c\ub2e4. object Hello { def main(args: Array[String]) = println(\"hello world!\") } \ud504\ub85c\uadf8\ub7a8\uc774 \ucef4\ud30c\uc77c \ub418\uace0 \ub098\uba74 \uc544\ub798\uc758 \ucee4\ub9e8\ub4dc\ub85c \uc2e4\ud589\ud560 \uc218 \uc788\ub2e4. > scala Hello Dynamic Binding code invoked by a method call depends on the runtime of the object that contains the method ex) Empty contains 1 -> false Lecture 3.2 - How Classes Are Organized Packages Classes\uc640 objects\ub294 package\uc548\uc5d0 \uad6c\uc131\ub41c\ub2e4\ub41c package\uc5d0 \uc18d\ud558\ub294 class, object\ub294 \uc18c\uc2a4 \ud30c\uc77c\uc758 \ucd5c\uc0c1\ub2e8\uc5d0 package\ub97c \uc368\uc57c \ud55c\ub2e4. package progfun.examples object Hello { ... } \uc544\ub798\uc640 \uac19\uc774 \ud504\ub85c\uadf8\ub7a8\uc744 \uc2e4\ud589\ud560 \uc218 \uc788\ub2e4. > scala progfun.examples.Hello Forms of Imports import week3.Rational // named imports import week3.{Rational, Hello} // named imports import week3._ // wildcard import Automatic Imports All members of package scala All members of package java.lang All members of the singleton object scala.Predef Int: scala.Int Boolean: scala.Boolean Object : java.lang.Object require: scala.Predef.require assert: scala.Predef.assert Traits Java \ucc98\ub7fc Scala\ub294 \uc624\uc9c1 \ud558\ub098\uc758 superclass\ub97c \uac00\uc9c8 \uc218 \uc788\ub2e4.(Single Inheritance) \ud558\uc9c0\ub9cc \uc5ec\ub7ec\uac1c\uc758 supertypes\ub97c \uac16\uace0 \uc2f6\ub2e4\uba74 \uc5b4\ub5bb\uac8c \ud560\uae4c? traits\ub97c \uc0ac\uc6a9\ud558\uba74 \ub41c\ub2e4. trait\uc740 abstract class\ucc98\ub7fc \uc815\uc758 \ud558\uba74\uc11c trait \ud0a4\uc6cc\ub4dc\ub97c \uc4f0\uba74 \ub41c\ub2e4\ub97c trait Planar { def height: Int def width: Int def surface = height * width } class Square extends Shape with Planar with Movable ... trait\uc740 Java\uc758 interface\uc640 \ube44\uc2b7\ud558\uc9c0\ub9cc \ub354 \uac15\ub825\ud558\ub2e4. fields\uc640 concrete methods(\uc815\uc758\ub41c \uba54\uc18c\ub4dc)\ub97c \uac00\uc9c8 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \ud558\uc9c0\ub9cc trait\uc740 (value) parameters\ub97c \uac00\uc9c8 \uc218 \uc5c6\ub2e4. \uc774\ub294 \ud074\ub798\uc2a4\ub9cc \uac00\ub2a5\ud558\ub2e4. Top Types Any: The base type of all types. Methods: '==', '!=', 'equals', 'hashCode', 'toString AnyRef: The base type of all reference types; Alias of 'java.lang.Object' AnyVal: The base type of all primitive types The Nothing Type Nothing\uc740 Scala type hierarchy\uc5d0\uc11c \ucd5c \ud558\ub2e8\uc5d0 \uc788\ub2e4. type Nothing\uc5d0\ub294 value\uac00 \uc5c6\ub2e4. \uc65c \uc4f0\uc77c\uae4c? - To signal abnormal termination - As an element type of empty collection Exceptions \uc790\ubc14\uc640 \uc720\uc0ac\ud558\ub2e4 throw Exc \uc774 expr\uc758 type\uc740 Nothing\uc774\ub2e4. example def error(msg: String) = throw new Error(msg) error(\"test\") The Null Type every reference class type\uc740 null \uac12\uc744 \uac16\ub294\ub2e4. null\uc758 \ud0c0\uc785\uc740 Null \uc774\ub2e4. Null\uc740 Object\ub97c \uc0c1\uc18d\ubc1b\ub294 \ubaa8\ub4e0 \ud074\ub798\uc2a4\uc758 subtype\uc774\ub2e4. \ud558\uc9c0\ub9cc AnyVal\uc758 subtypes\uacfc\ub294 incompativle \ud558\ub2e4. val x = null // x: Null val y: String = null // y: String val z: Int = null // error: type mismatch Lecture 3.3 - Polymorphism Type Parameter \uc5ec\ub7ec \ud0c0\uc785\uc5d0 \ub300\uc751\ud560 \uc218 \uc788\ub294 \ud0c0\uc785\uc774\ub2e4. trait List[T] class Cons[T](val head: T, val tail: List[T]) extends List[T] class Nil[T] extends List[T] \ud0c0\uc785 \ud30c\ub77c\ubbf8\ud130\ub294 square brackets \uc548\uc5d0 \uc4f0\uc778\ub2e4. Generic Functions classes\ucc98\ub7fc function\uc5d0\ub3c4 type parameter\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. def singleton[T](elem: T) = new Cons[T](elem, new Nil[T]) singleton[Int](1) singleton[Boolean](true) Type Inference \uc2a4\uce7c\ub77c\ub294 function call\uc758 arguments\ub85c \ubd80\ud130 parameter\uc758 \uc633\uc740 \ud0c0\uc785\uc744 \ucd94\uce21\ud560 \uc218 \uc788\ub2e4. \uadf8\ub807\uae30\uc5d0 \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0\uc5d0\uc11c type parameters\ub294 \uc548\uc368\ub3c4 \ub41c\ub2e4. singleton(1) singleton(true) Type Evaluation \uc2a4\uce7c\ub77c\uc5d0\uc11c Type parameter\ub294 evaluation\uc5d0 \uc601\ud5a5\uc744 \ub07c\uce58\uc9c0 \uc54a\ub294\ub2e4. \ubaa8\ub4e0 type parameters\uc640 type arguments\ub294 \ud504\ub85c\uadf8\ub7a8\uc744 \ud3c9\uac00\ud558\uae30 \uc804\uc5d0 \uc81c\uac70\ub41c\ub2e4. \uc774\ub97c type erasure \ub77c\uace0 \ubd80\ub978\ub2e4. Java, Scala, Haskell, ML, OCaml\uc5d0\uc11c\ub294 type erasure\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \ud558\uc9c0\ub9cc C++, C#, F# \uac19\uc740 \uc5b8\uc5b4\ub294 run time\uc2dc\uc5d0\ub3c4 type parameter\ub97c \uc720\uc9c0\ud55c\ub2e4. Polymorphism function type comes \"in many forms\" - function\uc774 \uc5ec\ub7ec \ud0c0\uc785\uc758 argument\uc5d0 \uc801\uc6a9\ub420 \uc218 \uc788\ub2e4 - \ud0c0\uc785\uc774 \uc5ec\ub7ec \ud0c0\uc785\uc758 \uc778\uc2a4\ud134\uc2a4\ub97c \uac00\uc9c8 \uc218 \uc788\ub2e4. \ud3f4\ub9ac\ubab0\ud53c\uc998\uc758 \ub450 \uac00\uc9c0 \ud615\ud0dc - subtyping: instances of subclass \ub294 base class\ub85c \uc804\ub2ec \ub420 \uc218 \uc788\ub2e4. - generic: function \ud639\uc740 \ud074\ub798\uc2a4\uc758 instance\ub294 type paramterization\uc73c\ub85c \ub9cc\ub4e4 \uc218 \uc788\ub2e4. Week4 Lecture 4.1 - Objects Everywhere Pure Object Orientation A pure object-oriented language is one in which every value is an object \uc2a4\uce7c\ub77c\ub294 pure object-oriented language\uc778\uac00? Standard Classes \uc2a4\uce7c\ub77c\uc5d0\uc11c Int \ud639\uc740 Boolean \uac19\uc740 \ud0c0\uc785\ub4e4\uc740 \ud2b9\ubcc4\ucde8\uae09\uc744 \ubc1b\uc9c0 \uc54a\ub294\ub2e4. \ub2e4\ub978 \ud074\ub798\uc2a4\ub4e4\uacfc \ub9c8\ucc2c\uac00\uc9c0\ub85c scala \ud328\ud0a4\uc9c0 \uc548\uc5d0 \uc815\uc758\ub41c \uac83\uc774\ub2e4. Pure Booleans Boolean type\uc740 JVM\uc758 primitive type boolean\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub97c \ud074\ub798\uc2a4\ub85c\ub3c4 \uc815\uc758\ud560 \uc218 \uc788\ub2e4. package idealized.scala abstract class Boolean { def ifThenElse[T](t: => T, e: => T): T def && (x: => Boolean): Boolean = ifThenElse(x, false) def || (x: => Boolean): Boolean = ifThenElse(true, x) def unary_!: Boolean = ifThenElse(x, false) def == (x: Boolean): Boolean = ifThenElse(x, x.unary_!) def !& (x: Boolean): Boolean = ifThenElse(x.unary_!, x) ... } object true extends Boolean { def ifThenElse[T](t: => T, e: => T) = t } object false extends Boolean { def ifThenElse[T](t: => T, e: => T) = e } Lecture 4.2 - Functions as Objects function values\ub294 object\ub85c \ucde8\uae09\ub41c\ub2e4. function type A => B\ub294 class scala.Function1[A,B]\uc758 \ucd95\uc57d\uc774\ub2e4. package scala trait Function1[A, B] { def apply(x: A): B } \uadf8\ub7ec\ubbc0\ub85c functions\ub294 objects\uc774\ub2e4 apply methods\ub97c \uac16\ub294. (x: Int) => x * x \ub294 \uc544\ub798\uc640 \uac19\uc774 \ud655\uc7a5\ub41c\ub2e4. { class AnonFun extends Function1[Int, Int] { def apply(x: Int) = x * x } new AnnonFun } \ub610\ub294 new Function1[Int, Int] { def apply(x: Int) = x * x } Expansion of Function Calls f(a, b)\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud655\uc7a5\ub41c\ub2e4. f.apply(a, b) val f = (x: Int) => x * x f(7) \uc740 \uc544\ub798\uc640 \uac19\uc774 \ud655\uc7a5\ub41c\ub2e4. val f = new Function1[Int, Int] { def apply(x: Int) => x * x } f.apply(7) Functions and Methods \uc544\ub798\uc640 \uac19\uc740 \uba54\uc18c\ub4dc\ub294 \uadf8 \uc790\uccb4\ub85c\ub294 function value\uac00 \uc544\ub2c8\ub2e4. def f(x: Int): Boolean = ... \ud558\uc9c0\ub9cc f\ub294 Function type\uc774 \uae30\ub300\ub418\ub294 \uacf3\uc5d0\uc11c \uc0ac\uc6a9\ub418\uae30 \ub54c\ubb38\uc5d0 \uc790\ub3d9\uc801\uc73c\ub85c function value\ub85c \ubcc0\ud658\ub41c\ub2e4. (x: Int) => f(x) Lecture 4.3 - Subtyping and Generics Type Bounds \ub9cc\uc57d assertAllPos \ub77c\ub294 method\uac00 \uc788\ub2e4\uace0 \ud558\uc790. IntSet\uc744 \ubc1b\uace0 \ub9cc\uc57d \ubaa8\ub4e0 \uc5d8\ub9ac\uba3c\ud2b8\uac00 positive\ub77c\uba74 IntSet\uc744 \ub9ac\ud134\ud55c\ub2e4. \uadf8 \uc678\uc5d0\ub294 exception\uc744 \ub358\uc9c4\ub2e4. def assertAllPos(s: IntSet): IntSet \uc774 \ucf54\ub4dc\ub294 \ubaa8\ub4e0 \uacbd\uc6b0\ub97c \ud3ec\ud568\ud558\uc9c0 \uc54a\ub294\ub2e4. nonEmpty\ub97c \ud30c\ub77c\ubbf8\ud130\ub85c \uc900\ub2e4\uba74 NonEmpty \ud639\uc740 Error\uac00 \ubc18\ud658\ub418\uace0 Empty\ub97c \ud30c\ub77c\ubbf8\ud130\ub85c \ubcf4\ub0b4\uba74 Empty\uac00 \ubc18\ud658\ub418\uae30 \ub584\ubb38\uc774\ub2e4. \uc815\ud655\ud55c IntSet\uc774 \uc544\ub2cc \uc0c1\uc18d Empty\uc640 NonEmpty\ub294 \ud558\uc704 \ud074\ub798\uc2a4\uc774\ub2e4. def assertAllPos[S <: IntSet](r: S): S \uc5ec\uae30\uc5d0\uc11c <: IntSet \uc740 type parameter S\uc758 upper bound \uc774\ub2e4. S <: T means: S\ub294 T\uc758 subtype\uc774\ub2e4 S >: T means: S\ub294 T\uc758 supertype\uc774\ub2e4. \ub610\ub294 T\ub294 S\uc758 subtype\uc774\ub2e4. [S >: NonEmpty] \uc77c\ub54c S\ub294 NonEmpty, IntSet, AnyRef, Any\uac00 \ub420 \uc218 \uc788\ub2e4. \uc774 \ub450\uac1c\ub97c \uc11e\ub294 \uac83\ub3c4 \uac00\ub2a5\ud558\ub2e4 [S >: NonEmpty <: IntSet] Covariance subtyping\uacfc type parameter\ub97c \uc0ac\uc6a9\ud560 \ub54c \uace0\ub824\ud574\uc57c\ud560 \uc0ac\ud56d\uc774 \uc788\ub2e4. NonEmpty <: IntSet \uc77c\ub54c List[NonEmpty] <: List[IntSet] \uc778\uac00? \uc6b0\ub9ac\ub294 \uc774\ub7f0 \uac83\uc744 \uacf5\ubcc0(covariant)\ub77c\uace0 \ubd80\ub978\ub2e4. Java\uc5d0\uc120 \ub418\uc9c0\ub9cc \uc2a4\uce7c\ub77c\uc5d0\uc120 \uc548\ub41c\ub2e4. \uc544\ub798\uc640 \uac19\uc740 Java Code\ub97c \ubcf4\uc790 NonEmpty[] a = new NonEmpty[]{new NonEmpty(1, Empty, Empty)} IntSet[] b = a b[0] = Empty NonEmpty s = a[0] 3\ubc88 \uc9f8 \ub77c\uc778\uc5d0\uc11c runtime error(ArrayStoreException)\uac00 \ubc1c\uc0dd\ud55c\ub2e4. NonEmpty\uc5d0 Empty\ub97c \ub123\uc73c\ub824\uace0 \uc2dc\ub3c4\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 Scala\uc5d0\uc120 List Convariance\uac00 \uc548\ub41c\ub2e4. val a: Array[NonEmpty] = Array(new NonEmpty(1, Empty, Empty)) val b: Array[IntSet] = a b(0) = Empty val s: NonEmpty = a(0) \uc704\uc5d0\uc11c 2\ubc88\uca30 \ub77c\uc778\uc5d0\uc11c\uac00 error\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Scala\uc5d0\uc120 List Convariance\uac00 \uc548\ub418\uae30 \ub54c\ubb38\uc774\ub2e4. Lecture 4.5 - Decomposition Decomposition \uc608\ub97c\ub4e4\uc5b4 \uc218\ud559 \uc5f0\uc0b0\uc744 \uc704\ud55c \uc791\uc740 \uc778\ud130\ud504\ub9ac\ud130\ub97c \ub9cc\ub4e0\ub2e4\uace0 \ud558\uc790. \uac04\ub2e8\ud558\uac8c \ud558\uae30 \uc704\ud574 numbers\uc640 additions\ub9cc \ud55c\ub2e4. Expressions\ub294 class hierarchy\ub85c \ud45c\ud604\ub420 \uc218 \uc788\uace0 \uae30\ubcf8 trait\uc778 Expr\uacfc two subclasses\uc778 Number\uc640 Sum\uc73c\ub85c \uad6c\uc131\ud560 \uc218 \uc788\ub2e4. trait Expr { def isNumber: Boolean def isSum: Boolean def numValue: Int def leftOp: Expr def rightOp: Expr } class Number(n: Int) extends Expr { def isNumber: Boolean = true def isSum: Boolean = false def numValue: Int = n def leftOp: Expr = throw new Error(\"Number.leftOp\") def rightOp: Expr = throw new Error(\"Number.rightOp\") } class Sum(e1: Expr, e2: Expr) extends Expr { def isNumber: Boolean = false def isSum: Boolean = true def numValue: Int = throw new Error(\"Sum.numValue\") def leftOp: Expr = e1 def rightOp: Expr = e2 } def eval(e: Expr): Int = { if (e.isNumber) e.numValue else if (e. isSum) eval(e.leftOp) + eval(e.rightOp) else throw new Error(\"Unknown expression\" + e) } \ub9cc\uc57d Expr\uc5d0 \uc0c8\ub85c\uc6b4 \uba54\uc11c\ub4dc\ub97c \ucd94\uac00\ud55c\ub2e4\uace0 \uac00\uc815\ud558\uba74 \uc774\ub97c \ud655\uc7a5\ud558\ub294 class\ub4e4\uc5d0\ub3c4 \ubaa8\ub450 \uad6c\ud604\uc744 \ucd94\uac00\ud574\uc57c \ud55c\ub2e4. \uc0c8\ub85c\uc6b4 \ud074\ub798\uc2a4\ub97c \ucd94\uac00\ud560\uc218\ub85d \uacc4\uc18d\ud574\uc11c \ub9ce\uc740 \uba54\uc11c\ub4dc\ub97c \ucd94\uac00\ud574\uc57c \ud55c\ub2e4. Non-Solution: Type Tests and Type Casts def isInstanceOf[T]: Boolean // check whether this object's type conforms to 'T' def asInstanceOf[T]: T // treat this object as an instance of type 'T' // throws 'ClassCastException' if it isn't def eval(e: Expr): Int = if (e.isInstanceOf[Number]) e.asInstanceOf[Number].numValue else if (e.isInstanceOf[Sum]) eval(e.asInstanceOf[Sum].leftOp) + eval(e.asInstanceOf[Sum].rightOp) else throw new Error(\"Unknown expression \" + e) no need for classification methods, access \uba54\uc18c\ub4dc\ub97c \ud544\uc694\ud560 \ub54c\ub9cc \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4. low-level, potentially unsafe\ud558\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. type cast\ub97c \uc218\ud589\ud588\uc744 \ub54c \uc624\ub958\uac00 \ubc1c\uc0dd\ud558\ub294 \uac83\uc744 runtime\uc5d0\uc11c \uc54c \uc218 \uc788\ub2e4. Solution1: Object-Oriented Decomposition trait Expr { def eval: Int } class Number(n: Int) extends Expr { def eval: Int = n } class Sum(e1: Expr, e2: Expr) extends Expr { def eval: Int = e1.eval + e2.eval \ub9cc\uc57d \uc0c8\ub85c\uc6b4 \uba54\uc18c\ub4dc\uac00 \ucd94\uac00\ub41c\ub2e4\uba74 \ubaa8\ub4e0 \ud074\ub798\uc2a4 hierarchy\uc5d0 \uc788\ub294 \ud074\ub798\uc2a4\uc5d0 \uba54\uc18c\ub4dc\ub97c \ucd94\uac00\ud574\uc57c \ud55c\ub2e4. Lecture 4.6 - Pattern Matching good fit for the problem of decomposition Case Classes case classes \uc758 \uc815\uc758\ub294 \uc77c\ubc18 class \uc815\uc758\uc640 \ube44\uc2b7\ud558\ub2e4. \uc55e\uc5d0 case\ub97c \ubd99\uc774\ub294 \uac83 \ube7c\uace0 trait Expr case class Number(n: Int) extends Expr case class Sum(e1: Expr, e2: Expr) extends Expr \uc704\uc640 \uac19\uc774 \ud558\uba74 \uc790\ub3d9\uc801\uc73c\ub85c object\uc640 apply methods\uac00 \uc0dd\uc131\ub41c\ub2e4. object Number { def apply(n: Int) = new Number(n) } object Sum { def apply(e1: Expr, e2: Expr) = new Sum(e1, e2) } object\ub85c \ub9cc\ub4e4\uc5b4\uc84c\uae30 \ub54c\ubb38\uc5d0 new Number(1) \ub300\uc2e0 Number(1)\ub85c\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. Pattern Matching \ud074\ub798\uc2a4 \uad6c\uc870\uc5d0\uc11c C\uc640 Java\uc758 switch \uc758 generalization\uc774\ub2e4. def eval(e: Expr): Int = e match { case Number(n) => n case Sum(e1, e2) => eval(e1) + eval(e2) } \ub9cc\uc57d \uc77c\uce58\ud558\ub294 \ud328\ud134\uc774 \uc5c6\ub2e4\uba74 MatchError\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Pattern\uc774 \ub420 \uc218 \uc788\ub294 \uac83\uc740 \uc544\ub798\uc640 \uac19\ub2e4. - constructors, eg. Number, Sum - variables, e.g. n, e1, e2 - wildcard patterns _, - constants, e.g. 1, true. \ucc38\uace0\ub85c \uc544\ub798\uc640 \uac19\uc740 \uc81c\uc57d \uc0ac\ud56d\uc774 \uc788\ub2e4. variables\ub294 \ubc18\ub4dc\uc2dc \uc18c\ubb38\uc790\ub85c \uc2dc\uc791\ud574\uc57c \ud558\uba70 pattern\uc548\uc5d0 \ubcc0\uc218\uba85\uc740 \ud55c \ubc88\ub9cc \uc4f0\uc5ec\uc57c \ud55c\ub2e4. \uc608\ub85c Sum(x, x)\ub294 \uc548\ub41c\ub2e4. constants\uc758 names\uc740 \ub300\ubb38\uc790\ub85c \uc2dc\uc791\ud574\uc57c \ud55c\ub2e4. Lecture 4.7 - Lists val fruit = List(\"apples\", \"oranges\", \"pears\") val nums = List(1,2,3,4) val diag3 = List(List(1, 0, 0)) val empty = List() List\uc640 Array\ub294 \uc911\uc694\ud55c \ucc28\uc774\uac00 \uc788\ub2e4. - List\ub294 immutable \ud558\ub2e4. -> element of a list cannot be changed - List\ub294 recursive \ud558\ub2e4. \ubc18\uba74 Array\ub294 flat\ud558\ub2e4. The List Type array\uac19\uc774 lists\ub294 homogeneous\ud558\ub2e4. (list\uc758 \ubaa8\ub4e0 element\uac00 \ub3d9\uc77c\ud55c \ud0c0\uc785\uc774\uc5b4\uc57c \ud55c\ub2e4) val fruit: List[String] = List(\"apples\", \"oranges\", \"pears\") val nums: List[Int] = List(1,2,3,4) val diag3: List[List[Int]] = List(List(1, 0, 0)) val empty: List[Nothing] = List() Constructors of Lists empty list Nil operation :: (pronounced cons) x :: xs \ub294 \uccab \ubc88\uc9f8 element\uac00 x\uc774\uace0 \uadf8 \ub4a4\ub85c xs\uac00 \uc624\ub294 \uc0c8\ub85c\uc6b4 list\ub97c \uc0dd\uc131\ud55c\ub2e4. fruit = \"apples\" :: (\"oranged\" :: (\"pears\" :: Nil)) nums = 1 :: (2 :: (3 :: (4 :: Nil))) empty = Nil A :: B :: C\ub294 A :: (B :: C)\ub85c \uc778\ud130\ud504\ub9ac\ud2b8 \ub41c\ub2e4. val nums = 1 :: 2 :: 3 :: 4 :: Nil // \uc704 \ud45c\ud604\uacfc \uac19\uc740 \uc758\ubbf8\ub85c \ub2e4\ub974\uac8c \uc4f8 \uc218 \uc788\ub2e4. Nil.::(4).::(3).::(2).::(1) Operations on Lists head tail isEmpty List Patterns Nil -> The Nil constant p :: ps -> head\uac00 p\uc774\uace0 \ub098\uba38\uc9c0\uac00 ps\uc5d0 \ub9e4\uce6d List(p1, ... , pn) -> same as p1 :: ... :: pn :: Nil examples 1 :: 2 :: xs 1\uacfc 2\ub85c \uc2dc\uc791\ud558\ub294 list x :: Nil length\uac00 1\uc778 List List (x) x :: Nil\uacfc \ub3d9\uc77c List() empty list, Nil\uacfc \ub3d9\uc77c List(2 :: xs) 2\ub85c \uc2dc\uc791\ud558\ub294 \ub9ac\uc2a4\ud2b8\ub97c \ud3ec\ud568\ud558\ub294 \ub9ac\uc2a4\ud2b8 Sorting Lists Insertion Sort List(7,3,9,2)\uac00 \uc788\uc744 \ub54c tail List\uc778 List(3,9,2)\ub97c sort\ud574\uc11c (2, 3, 9)\ub97c \uc5bb\uace0 7\uc744 \uc801\uc808\ud55c \uc704\uce58\uc5d0 \ubc30\uce58\ud574 (2, 3, 7, 9)\ub97c \ub9cc\ub4e0\ub2e4. def isort(xs: List[Int]): List[Int] = xs match { case List() => List() case y :: ys => insert(y, isort(ys)) } Week5 Lecture 5.1 - More Functions on Lists List Methods(1) xs.length xs.last -> xs\uac00 empty\ub77c\uba74 Error xs.init -> last\ub97c \uc81c\uc678\ud55c \ub098\uba38\uc9c0 \ub9ac\uc2a4\ud2b8. xs\uac00 empty\ub77c\uba74 Error xs take n -> xs \ub9ac\uc2a4\ud2b8 \ub0b4 \ub9e8 \uc55e\ubd80\ud130 n\uac1c\ub97c \ucde8\ud55c \ub9ac\uc2a4\ud2b8. \ub610\ub294 n\uc774 xs \uae38\uc774 \ubcf4\ub2e4 \ud06c\ub2e4\uba74 xs \uc790\uccb4\ub97c \ub9ac\ud134 xs drop n -> xs \ub9ac\uc2a4\ud2b8 \ub0b4 \ub9e8 \uc55e\ubd80\ud130 n \uac1c\ub97c \ubc84\ub9b0 \ub4a4\uc758 \ub9ac\uc2a4\ud2b8 xs(n) List Methods(1) Creating new Lists: xs ++ ys -> xs \ub9ac\uc2a4\ud2b8 \ub4a4\uc5d0 ys\ub97c \ud569\uce5c \ub9ac\uc2a4\ud2b8 xs.reverse xs updated (n, x) -> n\ubc88\uc9f8 index\uc5d0\ub294 x\ub97c \ub123\uace0 \ub098\uba38\uc9c0\ub294 xs\uc778 \ub9ac\uc2a4\ud2b8 Finding elements: xs indxOf x -> xs \ub9ac\uc2a4\ud2b8\ub0b4 x\uc640 \uac19\uc740 \uccab \ubc88\uc9f8 element\uc758 index. \uc5c6\ub2e4\uba74 -1 \ub9ac\ud134 xs contains x -> same as xs indexOf x >= 0 Implementations def last[T](xs: List[T]): T = xs match { case List() => throw new Error(\"last of empty list\") case List(x) => x case y :: ys => last(ys) } def init[T](xs: List[T]): List[T] = xs match { case List() => throw new Error(\"init of empty list\") case List(x) => List() case y :: ys => y :: init(ys) } def concat[T](xs: List[T], ys: List[T]): = xs match { case List() => ys case z :: zs => z :: concat(zs, ys) } def reverse[T](xs: List[T]): List[T] = xs match { case List() => List() case y :: ys => reverse(ys) ++ List(y) // ++ \ub294 concatenation\uc774\ub2e4. } \uc704\uc758 reverse\ud568\uc218\ub294 concat\ud560\ub54c n, reverse\ud560 \ub54c n n * n \uc758 \ubcf5\uc7a1\ub3c4\ub97c \uac16\ub294\ub2e4. def removeAt(n: Int, xs: List[Int]) = (xs take n) ::: (xs drop n + 1) Lecture 5.2 - Pairs and Tuples insertion sort\ubcf4\ub2e4 \uc88b\uc740 \uc54c\uace0\ub9ac\uc998\uc778 merge sort\ub97c \uad6c\ud604\ud574\ubcf4\uc790. \ub9cc\uc57d \ub9ac\uc2a4\ud2b8\uac00 \ube44\uc5b4\uc788\uac70\ub098 \ud55c \uac1c\uc758 \uc5d8\ub9ac\uba3c\ud2b8\uac00 \uc788\ub2e4\uba74 \uc774\ubbf8 \uc18c\ud305\ub41c \uac83\uc774\ub2e4. \uadf8\ub807\uc9c0 \uc54a\ub2e4\uba74 - \ub9ac\uc2a4\ud2b8\ub97c \uc808\ubc18\uc73c\ub85c \ub098\ub204\uc5b4 \ub450 \uac1c\uc758 \uc11c\ube0c \ub9ac\uc2a4\ud2b8\ub97c \ub9cc\ub4e0\ub2e4. - \uc11c\ube0c \ub9ac\uc2a4\ud2b8\ub97c sort \ud55c\ub2e4 - \ub450 \uac1c\uc758 sort\ub41c sub lists\ub97c \ud569\uce5c\ub2e4. SplitAt Function returns two sublists \uc8fc\uc5b4\uc9c4 index\ub85c \ubd80\ud130\uc758 \uc55e \uc694\uc18c\ub4e4 \uc8fc\uc5b4\uc9c4 index\ub85c \ubd80\ud130\uc758 \ub4a4 \uc694\uc18c\ub4e4 lists\uac00 pair \ub85c \ub9ac\ud134\ub41c\ub2e4. Detour: Pair and Tuples x,y pair\ub294 (x, y) \ub85c \ud45c\ud604\ud55c\ub2e4. val pair = (\"answer\", 42) val (label, value) = pair The Tuple class \ubaa8\ub4e0 Tuple n \ud074\ub798\uc2a4\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ud328\ud134\uc73c\ub85c \ubaa8\ub378 \ub41c\ub2e4. case class Tuple2[T1, T2](_1: +T1, _2: +T2) { override def toString = \"(\" + _1 + \",\" + _2 + \")\" } \uc544\ub798\ub294 pattern binding val (label, value) = pair \ub300\uc2e0 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. val label = pair._1 val value = pair._2 \ud558\uc9c0\ub9cc \ud328\ud134 \ub9e4\uce6d \ud3fc\uc774 \uc77c\ubc18\uc801\uc73c\ub85c \uc120\ud638\ub41c\ub2e4. def msort(xs: List[Int]): List[Int] = { val n = xs.length / 2 if (n == 0) xs else { def merge(xs: List[Int], ys: List[Int]): List[Int] = (xs, ys) match { case (Nil, ys) => ys case (xs, Nil) => xs case (x :: xs1, y :: ys1) => if (x < y) x :: merge (xs1, ys) else y :: merge(xs, ys1) } val (fst, snd) = xs splitAt n merge(msort(fst), msort(snd)) } } val nums = List(2, -4, 5, 7, 1) msort(nums) Lecture 5.3 - Implicit Parameters Making Sort more General msort\ub97c \ubaa8\ub4e0 \ud0c0\uc785\uc5d0 \ub300\ud574\uc11c \uad6c\ud604\ud574\ubcf4\uc790 def msort[T](xs: List[T]): List[T] = ??? \uc774\uc804\uc5d0 \uad6c\ud604\ud588\ub358 \ucf54\ub4dc\ub97c \uadf8\ub300\ub85c \ubc14\uafbc\ub2e4\uace0 \ud558\uba74 \ub3d9\uc791\ud558\uc9c0 \uc54a\ub294\ub2e4. < \uac00 \ubaa8\ub4e0 \ud0c0\uc785 T\uc5d0\uc11c \uc815\uc758\ub418\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc774\ub2e4. => \ube44\uad50 \ud568\uc218\uac00 \ud544\uc694\ud558\ub2e4. Parametrization with Ordered Ordering\uc744 \uc704\ud55c standard library\uac00 \uc788\ub2e4. scala.math.Ordering[T] \uc774\ub97c \uc774\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uac00\ub2a5\ud558\ub2e4. import math.Ordering def msort[T](xs: List[T])(ord: Ordering[T]): List[T] = { val n = xs.length / 2 if (n == 0) xs else { def merge(xs: List[T], ys: List[T]): List[T] = (xs, ys) match { case (Nil, ys) => ys case (xs, Nil) => xs case (x :: xs1, y :: ys1) => if (ord.lt(x, y)) x :: merge (xs1, ys) else y :: merge(xs, ys1) } val (fst, snd) = xs splitAt n merge(msort(fst)(ord), msort(snd)(ord)) } } val nums = List(2, -4, 5, 7, 1) // x\uc640 y\uc5d0 \ud0c0\uc785\uc744 \uc9c0\uc815 \uc548\ud574\ub3c4 scala compiler\uac00 Int\ub97c \uc778\uc9c0 \ud55c\ub2e4. msort(nums)(Ordering.Int) val fruits = List(\"apple\", \"pineapple\", \"orange\", \"banana\") // compareTo\ub294 JAVA String \uba54\uc11c\ub4dc\uc774\ub2e4. // first string \uc774 second string \ubcf4\ub2e4 \uc791\uc73c\uba74 -1 \uac19\uc73c\uba74 0 \ud06c\uba74 1\uc744 \ub9ac\ud134\ud55c\ub2e4. msort(fruits)(Ordering.String) Aside: Implicit Parameters passing lt or ord values\ub294 \uac70\ucd94\uc7a5\uc2a4\ub7fd\ub2e4. implicit paramter\ub85c \uc774\ub97c \ud53c\ud560 \uc218 \uc788\ub2e4. import math.Ordering def msort[T](xs: List[T])(implicit ord: Ordering[T]): List[T] = { val n = xs.length / 2 if (n == 0) xs else { def merge(xs: List[T], ys: List[T]): List[T] = (xs, ys) match { case (Nil, ys) => ys case (xs, Nil) => xs case (x :: xs1, y :: ys1) => if (ord.lt(x, y)) x :: merge (xs1, ys) else y :: merge(xs, ys1) } val (fst, snd) = xs splitAt n merge(msort(fst), msort(snd)) } } val nums = List(2, -4, 5, 7, 1) // x\uc640 y\uc5d0 \ud0c0\uc785\uc744 \uc9c0\uc815 \uc548\ud574\ub3c4 scala compiler\uac00 Int\ub97c \uc778\uc9c0 \ud55c\ub2e4. msort(nums) val fruits = List(\"apple\", \"pineapple\", \"orange\", \"banana\") // compareTo\ub294 JAVA String \uba54\uc11c\ub4dc\uc774\ub2e4. // first string \uc774 second string \ubcf4\ub2e4 \uc791\uc73c\uba74 -1 \uac19\uc73c\uba74 0 \ud06c\uba74 1\uc744 \ub9ac\ud134\ud55c\ub2e4. msort(fruits) \uc704\uc640 \uac19\uc774 implicit\uc744 \uc120\uc5b8\ud574\uc8fc\uba74 \ud0c0\uc785\uc744 \uc548\ub118\uaca8\uc918\ub3c4 \ucef4\ud30c\uc77c\ub7ec\uac00 \ud574\uacb0\ud574\uc900\ub2e4. Rules for Implicit Parameters function\uc774 type T\uc5d0 \ub300\ud55c implicit arguments\ub97c \ubc1b\ub294 \ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 implicit definition\uc744 \ucc3e\ub294\ub2e4. - is marked implicit - has a type compatible with T - is visible at the point of the function call, or is defined in a companion object associated with T \ub9cc\uc57d \uc774 \uc911\uc5d0 \uac00\uc7a5 \uad6c\uccb4\uc801\uc778 \ud558\ub098\uc758 \uc815\uc758\uac00 \uc788\ub2e4\uba74 \uc774\ub97c \uc801\uc6a9\ud55c\ub2e4. \uadf8 \uc678\uc758 \ubaa8\ub4e0 \uacbd\uc6b0\ub294 \uc5d0\ub7ec\ub97c \ubc1c\uc0dd\uc2dc\ud0a8\ub2e4. Lecture 5.4 - Higher-Order List Functions lists\uc758 \ubaa8\ub4e0 \ud568\uc218\ub4e4\uc740 first order\uc774\ub2e4. \uadf8 \ub9d0\uc740 functions\ub4e4\uc774 primitive type\uc758 lists\ub97c arguments\ub85c \ubc1b\uace0 \uadf8\ub4e4\uc744 \uacb0\uacfc\ub85c\uc11c \ub9ac\ud134\ud55c\ub2e4\ub294 \uac83\uc774\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 \ub2e4\ub978 \ud568\uc218\ub97c arguments\ub85c \ubc1b\ub294 higher-order list functions\ub97c \uad6c\ud604\ud574\ubcf8\ub2e4. Map map\uc758 \uc2e4\uc81c \uad6c\ud604\uc740 \uc544\ub798 \ubcf4\ub2e4 \ubcf5\uc7a1\ud558\ub2e4. \uc2e4\uc81c \ucf54\ub4dc\ub294 tail-recusrive \ud558\uace0, \ubaa8\ub4e0 collection\uc5d0 \ub300\ud574 \uc801\uc6a9 \uac00\ub2a5\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. abstract class List[T] { def map[U](f: T => U): List[U] = this match { case Nil => this case x :: xs => f(x) :: xs.map(f) } } \uc774\ub97c \uc774\uc6a9\ud55c \ud568\uc218\ub294 \uc544\ub798\uc640 \uac19\ub2e4. def scalaList(xs: List[Double], factor: Double) = xs map (x => x * factor) Filtering \ub610 \ub2e4\ub978 common operation on lists\uc5d0\ub294 \uc8fc\uc5b4\uc9c4 \uc870\uac74\uc5d0 \ub9de\ub294 elements\ub9cc select\ud558\ub294 filtering\uc774 \uc788\ub2e4. def filter(p: T => Boolean): List[T] = this match { case Nil => this case x :: xs => if(p(x)) x :: xs.filter(p) else xs.filter(p) } // filter \uc0ac\uc6a9 \uc548 \ud55c \ubc84\uc804 def posElems(xs: List[Int]): List[Int] = xs match { case Nil => xs case y :: ys => if (y > 0) y :: posElems(ys) else posElems(ys) } // filter \uc0ac\uc6a9 \ubc84\uc804 def posElems(xs: List[Int]): List[Int] = xs filter (x => x > 0) Variations of Filter xs filterNot p // same as filter(x => !p(x)) xs partition p // same as (xs filter p, xs filterNot p) xs takeWhile p // p\ub97c \ub9cc\uc871\ud558\ub294 \ubaa8\ub4e0 elements\ub4e4\uc758 longest prefix of list xs xs dropWhile p // \uccab \uc694\uc18c \ubd80\ud130 p\ub97c \ub9cc\uc871\ud558\ub294 elements\ub97c \ubaa8\ub450 \uc81c\uc678\ud558\uace0 \ubd88\ub9cc\uc871 \ub41c element \ubd80\ud130 \ub9c8\uc9c0\ub9c9\uae4c\uc9c0\uc758 list xs span p // same as (xs takeWhile p, xs dropWhile p) val nums = List(2, -4, 5, 7, 1) val fruits = List(\"apple\", \"pineapple\", \"orange\", \"banana\") nums filter (x => x > 0) nums filterNot (x => x > 0) nums partition (x => x > 0) nums takeWhile (x => x > 0) // 2 nums dropWhile (x => x > 0) // -4, 5, 7, 1 nums span (x => x > 0) // (List(2), List(-4, 5, 7, 1)) val data = List(\"a\", \"a\", \"a\" , \"b\", \"c\" , \"c\", \"a\") def pack[T](xs: List[T]): List[List[T]] = xs match { case Nil => Nil case x :: xs1 => val (first, rest) = xs span(y => y == x) first :: pack(rest) } pack(data) // List(List(a,a,a), List(b)... def encode[T](xs: List[T]): List[(T, Int)] = pack(xs) map (ys => (ys.head, ys.length)) encode(data) // List((a, 3), (b, 1), (c, 2), (a, 1)) Lecture 5.5 - Reduction of Lists \uc774\ubc88\uc5d0\ub294 \uc800\ubc88\uc5d0 \uc774\uc5b4 hof\ub97c \ubc30\uc6b0\ub294\ub370 fold or reduce combiators\ub77c \ubd88\ub9ac\ub294 \ud568\uc218\ub4e4\uc744 \ubc30\uc6b8 \uac83\uc774\ub2e4. ReduceLeft def sum(xs: List[Int]): Int = xs match { case Nil => 0 case y :: ys => y + sum(ys) } \uc704 \uc640 \uac19\uc740 \uba54\uc11c\ub4dc\ub294 redceLeft\ub85c abstract \uac00\ub2a5\ud558\ub2e4. reduceLeft\ub294 given binary operator\ub97c list\uc758 adjacent elements \uc0ac\uc774\uc5d0 insert\ud558\ub294 \uac83\uc774\ub2e4. List(x1, ..., xn) reduceLeft op = (... (x1 op x2) op ... ) op xn reduceLeft\ub97c \uc0ac\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uac00\ub2a5\ud558\ub2e4 def sum(xs: List[Int]) = (0 :: xs) reduceLeft ((x, y) => x + y) def product(xs: List[Int]) = (1 :: xs) reduceLeft ((x, y) => x * y) A Shorter Way To Write Functions ((x, y) => x * y) \ub300\uc2e0 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. (_ * _) \ubaa8\ub4e0 _ \ub294 new parameter\ub97c \ub098\ud0c0\ub0b8\ub2e4. \uc67c\ucabd\ubd80\ud130 \uc624\ub978\ucabd\uc73c\ub85c \uc774 parameter\ub294 next outer pair of parentheses\uae4c\uc9c0 \uc720\ud6a8\ud558\ub2e4. def sum(xs: List[Int]) = (0 :: xs) reduceLeft (_ + _) def product(xs: List[Int]) = (1 :: xs) reduceLeft (_ * _) FoldLeft reduceLeft\uc758 \ub354 general \ud55c function\uc774 \uc788\ub294\ub370 \uc774\ub97c foldLeft\ub77c\uace0 \ud55c\ub2e4. reduceLeft\uc640 \ube44\uc2b7\ud558\uc9c0\ub9cc additional parameter\ub85c\uc11c accumulator z \ub97c \ubc1b\ub294\ub2e4. empty list\uc77c \uacbd\uc6b0 z\ub97c \ub9ac\ud134\ud55c\ub2e4. def sum(xs: List[Int]) = (xs foldLeft 0) (_ + _) def product(xs: List[Int]) = (xs foldLeft 1) (_ * _) Implementations of ReduceLeft and FoldLeft abstract class List[T] { ... def reduceLeft(op: (T, T) => T): T = this match { case Nil => throw new Error(\"Nil.reduceLeft\") case x :: xs => (xs foldLeft x)(op) } def foldLeft[U](z: U)(op: (U, T) => U): U = this match { case Nil => z case x :: xs => (xs foldLeft op(z, x))(op) } } FoldRight and ReduceRight foldLeft\uc640 reduceLeft\ub294 unfold on tress that lean to the left \ub97c \ud55c\ub2e4. \ubc18\ub300\ub85c FoldRight\uc640 ReduceRight\ub294 produce trees which lean to the right \ud55c\ub2e4. List(x1, ..., x{n-1}, xn) reduceRight op = x1 op ( ... (x{n-1} op xn) ... ) (List(x1, ..., xn) foldRight acc)(op) = x1 op ( ... (xn op acc) ... ) abstract class List[T] { ... def reduceRight(op: (T, T) => T): T = this match { case Nil => throw new Error(\"Nil.reduceLeft\") case x :: xs => op(x, xs.reduceRight(op)) } def foldRight[U](z: U)(op: (U, T) => U): U = this match { case Nil => z case x :: xs => op(x, (xs foldRight z)(op)) } } Difference between FoldLeft and FoldRight produce different tree Left leaning trees, Right leaning trees \ub458 \uc774 \ub3d9\uc77c\ud55c \uacb0\uacfc\ub97c \ub9cc\ub4e4\uc5b4 \ub0bc \uc218\ub3c4 \uc788\uc9c0\ub9cc(\ud6a8\uc728\uc131\uc5d0\uc120 \ucc28\uc774\uac00 \uc788\uc744 \uc218 \uc788\ub2e4) \ub54c\ub54c\ub85c\ub294 \ud55c \uac00\uc9c0\ub9cc \uc801\uc6a9 \uac00\ub2a5\ud558\ub2e4. def concat[T](xs: List[T], ys: List[T]): List[T] = (xs foldRight ys) (_ :: _) \uc704\uc758 \ud568\uc218\uc5d0\uc11c foldLeft\ub294 \uc548\ub41c\ub2e4. \uc65c\ub0d0?! :: \uc5f0\uc0b0\uc790\uc5d0 paramter \ud0c0\uc785 T\uac00 \uc62c \uc218 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4. List\ub9cc \uc804\ub2ec\ud574\uc57c \ud55c\ub2e4. (\uadf8\ub9bc\uc744 \uadf8\ub824\ubcf4\uba74 foldLeft\uc758 \uc624\ub978\ucabd\uc5d4 T\uac12\ub4e4\uc774 \uc628\ub2e4.) Lecture 5.6 - Reasoning About Concat Lecture 5.7 - A Larger Equational Proof on Lists Week6 Lecture 6.1 - Other Collections \uc774\uc804\uc5d0 \ubd24\ub4ef\uc774 list\ub294 linear\uc774\ub2e4. \uccab \ubc88\uc9f8 element\uc5d0 \uc811\uadfc\ud558\ub294 \uac83\uc774 \ub9c8\uc9c0\ub9c9 element\ub97c \uc811\uadfc\ud558\ub294 \uac83\ubcf4\ub2e4 \ube60\ub974\ub2e4. \uc2a4\uce7c\ub77c\uc5d0\ub294 sequence\ub97c \uad6c\ud604\ud55c \ub610 \ub2e4\ub978 Vector \uac00 \uc788\ub2e4. \uc774\ub294 list\ubcf4\ub2e4 \ubcf4\ub2e4 \ubc38\ub7f0\uc2a4\ub41c access pattern\uc744 \uac16\ub294\ub2e4. \uc5ec\ub7ec depth\uc758 Vector\uc758 \uacf5\uc2dd\uc740 log 32 n (n\uc740 vector size\uc774\ub2e4.) \uc774 \uc758\ubbf8\ub294 vector\uc758 \ud06c\uae30\uac00 \ucee4\uc838\ub3c4 access \uc18d\ub3c4\uac00 \ub290\ub9ac\uac8c \uc99d\uac00\ud55c\ub2e4\ub294 \uac83\uc774\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 decent random access\uc5d0\uc11c list \ubcf4\ub2e4 vector\uac00 \ube60\ub974\ub2e4. \ub610\ud55c vector\ub294 bulk operation\uc5d0 \uc88b\ub2e4. - sequence\ub97c traverse\ud558\ub294 \uac83 - map \ud568\uc218 \uc801\uc6a9 - fold \ud568\uc218 \uc801\uc6a9 Vector\uac00 \ub354 \uc88b\ub2e4\uba74 list\ub294 \uc65c \uc368\uc57c \ud558\ub098? head\ub9cc \uc5bb\uc744 \uacbd\uc6b0 \ud639\uc740 tail\ub9cc list\uac00 \uc0c1\uc218 \uc2dc\uac04\uc73c\ub85c\uc11c \ub354 \uc720\ub9ac\ud558\ub2e4. \uc774\ucc98\ub7fc access pattern\uc774 recursive structures\uc77c \ub550 list\uac00 \ub354 \uc720\ub9ac\ud558\ub2e4. \ub9cc\uc57d access pattern\uc774 bulk operation\uc774\ub77c\uba74 map, fold, filter \ucc98\ub7fc, vector\uac00 \ub354 \uc720\ub9ac\ud558\ub2e4. \ub2e4\ud589\ud788\ub3c4 Vector\uc640 List\uac04\uc758 \uc804\ud658\uc740 \uc27d\ub2e4 val nums = Vector(1, 2, 3, -88) val people = Vector(\"Vector\", \"James\", \"Peter\") \ub610\ud55c list\uc640 \ub3d9\uc77c\ud55c operations\uc744 \uc9c0\uc6d0\ud55c\ub2e4. :: \uc744 \uc81c\uc678\ud558\uace0 \ub300\uc2e0 \uc544\ub798\uc640 \uac19\uc740 \uac83\ub4e4\uc774 \uc788\ub2e4. x +: xs // Create a new vector with leading element x, followed by all elements of xs xs :+ x // Create a new vector with trailing element x, preceded by all elements of xs ( : \uc740 \ud56d\uc0c1 sequence\ub97c \uac00\ub9ac\ud0a8\ub2e4.) vector\uc5d0 add\ub294 \uc5b4\ub5bb\uac8c \ud560\uae4c? \uc544\ub798 \uadf8\ub9bc\uacfc \uac19\uc774 \ub9e8 \uc544\ub798\ucabd\uc5d0 \uc704\uce58\ud55c \ube68\uac04\uc0c9\uc758 Vector\ub97c \ucd94\uac00\ud55c\ub2e4\uace0 \ud588\uc744 \ub54c \uc704\ucabd \ub808\ubca8\ub85c \uc62c\ub77c\uac00\uba74\uc11c \uc774\uc804\uacfc \ub3d9\uc77c\ud55c element\ub4e4\uc744 \uac00\ub9ac\ud0a4\uba74\uc11c \ud558\ub098\ub294 \uc0c8\ub85c \ub9cc\ub4e0 element\ub97c \uac00\ub9ac\ud0a4\ub294 \uc0c8\ub85c\uc6b4 \ubca1\ud130\ub97c \ub9cc\ub4e4\uba74\uc11c root \ub808\ubca8\uae4c\uc9c0 \uc62c\ub77c\uac04\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \ubcf5\uc7a1\ub3c4\ub294 log 32 n \uc774\ub2e4. (Object\ub97c \uc0dd\uc131\ud558\ub294\ub370 \uac78\ub9ac\ub294 \uc2dc\uac04) list\uc640 vector\uc758 base class\ub294 Seq\uc774\ub2e4. \uadf8\ub9ac\uace0 Seq\uc740 Iterable\uc758 subclass\uc774\ub2e4. Arrays\uc640 Strings\ub294 Seq\uc640 \ub3d9\uc77c\ud55c operations\ub97c \uc81c\uacf5\ud55c\ub2e4. \uadf8\ub807\uae30\uc5d0 \ud544\uc694\ud560\ub54c \uba85\uc2dc\uc801\uc73c\ub85c \ubcc0\uacbd\ud560 \uc218 \uc788\ub2e4. val xs: Array[Int] = Array(1,2,3) xs map (x => 2 * x) val ys: String = \"Hello world!\" ys filter (_.isUpper) \ub610\ub2e4\ub978 sequence type\uc73c\ub85c range \uac00 \uc788\ub2e4. sequence of evenly spaced integers\ub97c \ud45c\ud604\ud55c\ub2e4. 3\uac1c\uc758 operators\uac00 \uc788\ub2e4. - to (inclusive) - until (exclusive) - by (to determine step value) val r: Range = 1 until 5 // 1,2,3,4 val s: Range = 1 to 5 // 1,2,3,4,5 1 to 10 by 3 // 1,4,7,10 6 to 1 by -2 // 6,4,2 Ranges\ub294 lower bound, upper bound, step value 3\uac1c\uc758 \ud544\ub4dc\ub97c \uac16\ub294 single Object\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. Sequence\ub294 \uc544\ub798\uc640 \uac19\uc740 operations\ub3c4 \uc81c\uacf5\ud55c\ub2e4. xs exists p xs forall p // xs\uc758 \ubaa8\ub4e0 \uc694\uc18c\uc5d0 p(x)\uac00 true\ub77c\uba74 true \uadf8 \uc678\ub294 false xs zip ys // xs\uc640 ys\uc758 \uc694\uc18c\ub4e4\uc744 pair\ub85c \ub9cc\ub4e0 \uc0c8\ub85c\uc6b4 sequence\ub97c \ub9cc\ub4e0\ub2e4. \uc608\ub85c List(1,2) List('a', 'b')\uac00 \uc788\ub2e4\uba74 List((1, 'a'), (2, 'b'))\uac00 \ub41c\ub2e4. xs.unzip xs.flatMap f // f\ub97c xs\uc758 \ubaa8\ub4e0 element\uc5d0 \uc801\uc6a9\ud558\uace0 results\ub97c concatenate\ud55c\ub2e4. xs.sum xs.product xs.max xs.min val s = \"Hello World\" s filter (c => c.isUpper) // HW s exists (c => c.isUpper) // true s forall (c => c.isUpper) // false val pairs = List(1,2,3) zip s // List((1, H), (2, e), (3, l)) pairs.unzip // (List(1,2,3), List(H,e,l)) s flatMap (c => List('.', c)) // .H.e.l.l.o. .W.o.r.l.d xs.sum // 50 xs.max // 44 def combinations(m: Int, n: Int) = (1 to m) flatMap (x => (1 to n) map (y => (x, y))) combinations(4,4) def scalaProduct(xs: Vector[Double], ys: Vector[Double]): Double = (xs zip ys).map(xy => xy._1 * xy._2).sum // \uc704 \uc2dd\uc740 pattern matching function value\ub85c \uc544\ub798\ucc98\ub7fc \uc4f8 \uc218 \uc788\ub2e4. def scalaProductUsingPattern(xs: Vector[Double], ys: Vector[Double]): Double = (xs zip ys).map{ case (x, y) => x * y}.sum def isPrime(n: Int): Boolean = (2 until n) forall (d => n % d != 0) Lecture 6.2 - Combinatorial Search and For-Expressions \uc885\uc885 Higher order functions \uc640 colledctions\ub294 imperative language\uc5d0\uc11c\uc758 loops\ub97c replace \ud55c\ub2e4. \uc911\ucca9\ub41c loops\ub294 \uc774\ub7ec\ud55c hof\uc758 \uc870\ud569\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \uc608\ub97c \ub4e4\uc5b4 integer i,j \uc5d0 \ub300\ud574 1 <= j < i < n\uc5d0 \ub300\ud574 i + j\uac00 prime\uc778 i, j\ub97c \ucc3e\ub294\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \ub9cc\uc57d n\uc774 7\uc774\ub77c\uba74 i,j\ub294 (2, 1), (3, 2), (4, 1) \ub4f1 \uc77c \uac83\uc774\ub2e4. \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc73c\ub85c\ub294 i\uc640 j\ub97c nested loops\ub85c \uad6c\uc131\ud558\uace0 buffer\ub97c \uc0dd\uc131\ud55c \ub4a4 prime\uc774\ub77c\uba74 buffer\uc5d0 \ucd94\uac00\ud560 \uac83\uc774\ub2e4. (i, j) pair\ub97c \ucc3e\ub294 \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc740 \uc544\ub798\uc640 \uac19\ub2e4. (1 until n) map (i => (1 until i) map (j => (i, j))) // Vector(Vector(), Vector((2,1)), Vector((3,1), (3,2)...) \ud558\uc9c0\ub9cc \uc704 \uacb0\uacfc\ub97c \ubcf4\uba74 Vectors of vectors\uac00 \ub9cc\ub4e4\uc5b4\uc9c4\ub2e4. \uc774\ub97c \ud558\ub098\uc758 Vectors\ub85c \ubc14\uafb8\ub824\uba74 \uc5b4\ub5bb\uac8c \ud574\uc57c \ud560\uae4c \uc704 \ucf54\ub4dc\uc758 \uacb0\uacfc\ub97c xss\ub77c\uace0 \ud560\ub54c \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218 \uc788\ub2e4. // 1. foldRight\uc744 \uc774\uc6a9\ud55c \ubc29\ubc95 (xss foldRight Seq[Int]())(_ ++ _) // 2. buil-in method \uc778 flatten\uc744 \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95 xss.flatten ((1 until n) map (i => (1 until i) map (j => (i, j)))).flatten \uc720\uc6a9\ud55c Law\uac00 \uc788\ub2e4. xs flatMap f = (xs map f).flatten \uadf8\ub807\uae30\uc5d0 \uc544\ub798\uc640 \uac19\uc774 \uac00\ub2a5\ud558\ub2e4 (1 until n) flatMap(i => (1 until i) map (j => (i, j))) \ud558\uc9c0\ub9cc \uc5ec\uc804\ud788 \uc644\uc131\uc740 \uc548\ub410\ub2e4. sum\uc758 \uacb0\uacfc\uac00 prime\uc778 pair\ub9cc \ud544\uc694\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. def isPrime(n: Int) = (2 until n) forall (n % _ != 0) (1 until n) flatMap(i => (1 until i) map (j => (i, j))) filter (pair => isPrime(pair._1 + pair._2)) HOF\uc778 map, flatMap, filter\ub294 manipulating lists\ub97c \uc81c\uacf5\ud558\ub294 \uac15\ub825\ud55c \ubc29\ubc95\uc774\ub2e4. \ud558\uc9c0\ub9cc \ub54c\ub54c\ub85c \uc774\ub294 \uc774\ud574\ud558\uae30 \uc5b4\ub835\uac8c \ud55c\ub2e4. Scala\uc758 for expreesion notaion\uc774 \ud574\uacb0\ud574 \uc904 \uc218 \uc788\ub2e4. // \uc544\ub798\uc640 \uac19\uc740 class\uac00 \uc788\ub2e4\uace0 \ud558\uc790 case class Person(name: String, age: Int) // 20\uc0b4\uc774 \ub118\ub294 Person\uc758 names\ub97c \uc5bb\uace0 \uc2f6\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218 \uc788\ub2e4. for (p <- persons if p.age > 20) yield p.name // \uc704 \ud45c\ud604\uc740 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. persons filter (p => p.age > 20) map (p => p.name) \uc704 \ucf54\ub4dc\ub97c \ubd24\uc744 \ub54c for\ub97c \uc774\uc6a9\ud55c \ubc29\uc2dd\uc774 \ub354 \uc774\ud574\ud558\uae30 \uc27d\ub2e4. imperative languages\uc758 loop\uc640 \ub2e4\ub978\uc810\uc740 \uacb0\uacfc\ub85c \uc0c8\ub85c\uc6b4 list\ub97c \ub9cc\ub4e4\uc5b4 \ub0b8\ub2e4\ub294 \uc810\uc774\ub2e4. Syntax of For for ( s ) yield e s\ub294 sequence of generators and filters \uc774\uace0, e\ub294 iteration\uc5d0\uc11c return \ub418\ub294 value\uc758 expression\uc774\ub2e4. generator \uc758 form\uc740 p <- e \uc774\ub2e4. p\ub294 pattern\uc774\uace0 e \ub294 collection\uc758 value\uac00 \ub418\ub294 expression\uc774\ub2e4. filter \uc758 form\uc740 if f \uc774\ub2e4. f\ub294 boolean expression\uc774\ub2e4. sequence\ub294 \ubc18\ub4dc\uc2dc generator\ub85c \uc2dc\uc791\ud574\uc57c \ud55c\ub2e4. sequence\ub0b4 \ub9cc\uc57d \uc5ec\ub7ec \uac1c\uc758 generator\uac00 \uc788\ub2e4\uba74, \ub9c8\uc9c0\ub9c9 generator\ub294 \uccab \ubc88\uc9f8\ubcf4\ub2e4 \ube60\ub974\ub2e4. ( s ) \ub300\uc2e0 { s } \ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. \uadf8\ub9ac\uace0 sequence of generators and filters\ub294 semicolon \uc5c6\uc774 \uc5ec\ub7ec \uc904\ub85c \uc4f0\uc77c \uc218 \uc788\ub2e4. \uc804\uc5d0 \ud480\uc5c8\ub358 prime pair\ub97c \uad6c\ud558\ub294 \ubc95\uc744 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. for { i <- 1 until n j <- 1 until i if isPrime(i + j) } yield (i, j) def scalarProduct(xs: List[Double], ys: List[Double]): Double = { (for { x <- xs y <- ys } yield x * y).sum } Lecture 6.3 - Combinatorial Search Example Set\uc740 Scala collections\uc758 \ub610 \ub2e4\ub978 basic abstraction\uc774\ub2e4. val fruit = Set(\"apple\", \"banana\", \"pear\") val s = (1 to 6).toSet \ub300\ubd80\ubd84\uc758 sequence operations\ub294 sets\uc5d0\uc11c\ub3c4 \uac00\ub2a5\ud558\ub2e4. s map (_ + 2) // Set(3,4,5,6,7,8) fruit filter (_.startsWith == \"app\") s.nonEmpty Sets vs Sequences \ucc28\uc774\uc810\uc774 3\uac00\uc9c0 \uc788\ub2e4. 1. Sets\ub294 unordered\uc774\ub2e4. 2. Sets\ub294 \uc911\ubcf5\ub41c elements\ub97c \uac16\uc9c0 \ubabb\ud55c\ub2e4. 3. fundamental operation\uc774 sets\uc5d0\ub294 \ud3ec\ud568\ub418\uc5b4 \uc788\ub2e4. s contains 5 Example: N-Queens recursive alogrithm\uc73c\ub85c \ud574\uacb0 \uac00\ub2a5\ud558\ub2e4. - \uc0ac\uc774\uc988\uac00 n\uc778 \uccb4\uc2a4 \ubcf4\ub4dc\uc5d0\uc11c k-1 \uac1c\uc758 queen\uc744 \ub193\ub294 \ubaa8\ub4e0 solutions\ub97c \uc774\ubbf8 \uc0dd\uc131\ud588\ub2e4\uace0 \uac00\uc815\ud558\uc790 - \uac01 solution\uc740 0~n-1 \uae4c\uc9c0\uc758 columns\ub97c \uac16\ub294 k-1\uc758 length \ub9ac\uc2a4\ud2b8\ub85c \ud45c\ud604 \uac00\ub2a5\ud558\ub2e4. - k-1 \ubc88\uc9f8\uc758 row\uc5d0\uc11c column number of the queen \ub9ac\uc2a4\ud2b8\uc758 \uccab \ubc88\uc9f8\ub85c \uc624\uace0 \uadf8 \ub2e4\uc74c\uc5d0\ub294 k-2 \ubc88\uc9f8\uc758 row\uc5d0 \uc788\ub294 column number of queen\uc774 \uc628\ub2e4. - solution set\uc740 set of lists\ub85c \ud45c\ud604\ub418\uba70, \uac01 element\ub294 solution\uc774 \ub41c\ub2e4. - \uc774\uc81c k\ubc88\uc9f8\uc5d0 queen\uc744 \ubc30\uce58\ud558\uae30 \uc704\ud574, \uc774\uc804\uc5d0 \ub193\uc600\ub358 \uac83\uc744 \uace0\ub824\ud558\uc5ec \uac00\ub2a5\ud55c \ubaa8\ub4e0 \ubc30\uce58 \uacbd\uc6b0\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud55c\ub2e4. def queens(n: Int): Set[List[Int]] = { def placeQueens(k: Int): Set[List[Int]] = { if (k == 0) Set(List()) else // { // placeQueens(k-1) flatMap ((a) => { // 0 until 2 map (b => { // if isSafe(b, a) // b :: a // }) // }) // } for { queens <- placeQueens(k - 1) col <- 0 until n if isSafe(col, queens) } yield col :: queens } def isSafe(col: Int, queens: List[Int]): Boolean = { val row = queens.length val queensWithRow = (row -1 to 0 by -1) zip queens queensWithRow forall { case (r, c) => col != c && math.abs(col - c) != row - r } } placeQueens(n) } def show(queens: List[Int]): String = { val lines = for (col <- queens.reverse) yield Vector.fill(queens.length)(\"* \").updated(col, \"X \").mkString \"\\n\" + (lines mkString \"\\n\") } def main(args: Array[String]): Unit = { val s = (queens(4) map show) mkString \"\\n\" println(s) println((queens(5) take 3 map show) mkString \"\\n\") } Lecture 6.4 - Maps \ub610 \ub2e4\ub978 fundamental collection type\uc73c\ub85c map\uc774 \uc788\ub2e4. val romanNumerals: Map[String, Int] = Map(\"I\" -> 1, \"V\" -> 5, \"X\" -> 10) val capitalOfCountry = Map(\"US\" -> \"Washington\") Class Map[Key, Value]\ub294 colletion type\uc778 Iterable[(Key, Value)]\ub85c \ud655\uc7a5\ud560 \uc218 \uc788\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 iterables\uac00 \ud560 \uc218 \uc788\ub294 collection operations\uac00 \uac00\ub2a5\ud558\ub2e4. val countryOfCaptials = capitalOfCountry map { case (x, y) => (y, x) } map\uc740 key/value \uc30d\uc758 iterable\uc744 \ud655\uc7a5\ud55c \uac83\uc774\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 key -> value \ub294 (key, value) \ub85c \uc4f8 \uc218\ub3c4 \uc788\ub2e4. Class Map[Key, Value]\ub294 \ub610\ud55c Key => Value\uc758 type\uc744 \uac16\ub294 function\uc73c\ub85c \ud655\uc7a5\ud560 \uc218 \uc788\ub2e4. \ub530\ub77c\uc11c \ud568\uc218\uac00 \ud560 \uc218 \uc788\ub294 \uac83\uc740 \ud560 \uc218 \uc788\ub2e4. capitalOfCountry(\"US\") map\uc5d0 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294 key\ub97c \uc8fc\uba74 error\uac00 \ubc1c\uc0dd\ud55c\ub2e4. capitalOfCountry(\"Andorra\") // java.util.NoSuchElementException \uc8fc\uc5b4\uc9c4 key\uac00 map\uc5d0 \uc874\uc7ac\ud558\ub294\uc9c0 \uc54c \uc218 \uc5c6\uc744 \ub54c query\ub97c \ud558\uba74 \ub41c\ub2e4. capitalOfCountry get \"US\" // Some(\"Washington\") capitalOfCountry get \"Andorra\" // None get operation\uc758 \uacb0\uacfc\ub294 Option value\uc774\ub2e4. The Option Type standard library\uc5d0 \uc18d\ud55c\ub2e4. trait Option[+A] case class Some[+A](value: A) extends Option[A] object None extends Option[Nothing] map get key\ub294 \ub2e4\uc74c\uc744 \ub9ac\ud134\ud55c\ub2e4. - None : \ub9f5\uc5d0 \ud574\ub2f9 key\uc5d0 \ud574\ub2f9\ud558\ub294 \uac12\uc774 \uc5c6\uc744 \uacbd\uc6b0 - Some(x): map\uc5d0 \ud574\ub2f9 key\uc5d0 \ud574\ub2f9\ud558\ub294 \uac12\uc774 \uc788\uc744 \uacbd\uc6b0 value x options\uac00 case classes\uc774\ubbc0\ub85c pattern matching\uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. def showCapital(country: String) = capitalOfCountry.get(country) match { case Some(capital) => capital case None => \"missing data\" } showCapital(\"US\") showCapital(\"Andorra\") Options\ub294 \ub2e4\ub978 collections\uc5d0\uc11c\ub3c4 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. Sorted and GroupBy SQL queries\uc5d0\uc11c \uc720\uc6a9\ud55c groupBy\uc640 orderBy\ub97c \uc4f8 \uc218 \uc788\ub2e4. val fruit = List(\"apple\", \"pear\", \"orange\", \"pineapple\") fruit sortWith (_.length < _.length) // List(\"pear\", \"apple\", \"orange\", \"pineapple\") fruit.sorted // List(\"apple\", \"orange\", \"pear\", \"pineapple\") fruit groupBy (_.head) // Map(p -> List(pear, pineapple), a -> List(apple), o -> List(orange)) map\uc740 partial functions \uc774\ub2e4. map(key) \ub97c \uc218\ud589\ud588\uc744 \ub54c \ud574\ub2f9 \ud558\ub294 \uac12\uc774 \uc5c6\uc73c\uba74 exception\uc744 \ubc1c\uc0dd\uc2dc\ud0a4\uae30 \ub54c\ubb38\uc774\ub2e4. withDefaultValue operation\uc744 \uc0ac\uc6a9\ud558\uba74 map\uc744 total function\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc900\ub2e4. val cap1 = capitalOfCountry withDefaultValue \"unknown\" cap1(\"Andorra\") // \"Unknown\" Poly\ub97c \uc5ec\ub7ec \uae30\uc220\uc744 \uc801\uc6a9\ud574\uc11c \ubc14\uafd4\ubcf4\uc790. class Poly(val terms0: Map[Int, Double]) { // p1\uacfc p2 \uc120\uc5b8\uc5d0\uc11c \ubcf4\uba74 \ud56d\uc0c1 Map\uc73c\ub85c \uac10\uc2f8\uc918\uc57c \ud588\ub2e4. // \uc544\ub798\uc640 \uac19\uc774 constructor\ub97c \uc0ac\uc6a9\ud558\uba74 \uc774\ub7f0 \ubc18\ubcf5\uc801\uc778 \ud589\ub3d9\uc744 \uc9c0\uc6b8 \uc218 \uc788\ub2e4. // *\uc758 \uc758\ubbf8\ub294 \ud574\ub2f9 \ud328\ud134\uc774 \ubc18\ubcf5\ub41c\ub2e4\ub294 \uac83\uc774\ub2e4. def this(bindings: (Int, Double)*) = this(bindings.toMap) // defaultValue\ub97c \uc120\uc5b8\ud568\uc73c\ub85c\uc11c addTerm\uc5d0\uc11c Option\ubd80\ubd84\uc744 \uc81c\uac70\ud588\ub2e4. val terms = terms0 withDefaultValue 0.0 // map\uc758 concatenation\uc778 ++\ub97c foldLeft\ub97c \ubcc0\uacbd\ud574\uc11c \uac04\ub2e8\ud558\uac8c \ubcc0\uacbd\ud588\ub2e4. def + (other: Poly) = new Poly((other.terms foldLeft terms)(addTerm)) def addTerm(terms: Map[Int, Double], term: (Int, Double)): Map[Int, Double] = { val (exp, coeff) = term terms + (exp -> (coeff + terms(exp))) } override def toString = (for ((exp, coeff) <- terms.toList.sorted.reverse) yield coeff+\"x^\"+exp) mkString \" + \" } val p1 = new Poly(1 -> 2.0, 3 -> 4.0, 5 -> 6.2) val p2 = new Poly(Map(0 -> 3.0, 3 -> 7.0)) p1 + p2 \uadf8\ub807\ub2e4\uba74 addTerm\uc744 foldLeft\ub85c \uad6c\ud604\ud558\ub294 \uac83\uacfc ++\ub85c \uad6c\ud604\ud558\ub294 \uac83 \uc911\uc5d0 \uc5b4\ub5a4 \uac83\uc774 \ub354 \ud6a8\uacfc\uc801\uc77c\uae4c? foldLeft\ub85c \uad6c\ud604\ud558\uba74 \uae30\uc874 data structure\uc5d0 \uacc4\uc18d \ub354\ud558\ub294 \ubc18\uba74 ++\ub85c \uad6c\ud604\ud558\uba74 \uc0c8\ub85c\uc6b4 List\uac00 \uacc4\uc18d \uc0dd\uc131\ub418\uae30 \ub584\ubb38\uc5d0 foldLeft \ubc84\uc804\uc774 \ub354\uc6b1 \ud6a8\uacfc\uc801\uc774\ub2e4. Lecture 6.5 - Putting the Pieces Together val mnemonics = Map( '2' -> \"ABC\", '3' -> \"DEF\", '4' -> \"GHI\", '5' -> \"JKL\", '6' -> \"MNO\", '7' -> \"PQRS\", '8' -> \"TUV\", '9' -> \"WXYZ\") \uc704\uc640 \uac19\uc740 dictionary words\uac00 \uc788\uc744 \ub54c method translate\uc744 \uad6c\ud604\ud55c\ub2e4\uace0 \ud574\ubcf4\uc790. translate(phoneNumber) \uc608\ub97c \ub4e4\uc5b4 \"7225247386\"\uc744 \uc704\uc758 mnemonics\ub97c \ucc38\uc870\ud574\uc11c \ubcc0\uacbd\ud558\ub294\ub370 \"Scala is fun\" solution phrases\uc911 \ud558\ub098\uc5ec\uc57c \ud55c\ub2e4. \uc704 \uc608\uc81c\ub294 Lutz Prechelt: An Empirical Comparison of Seven Programming Languages. (2000) Tested with Tcl, Python, Perl, Rexx, Java, C++, C. \ub17c\ubb38\uc5d0\uc11c \uac00\uc838\uc628 \uac83\uc774\ub2e4. scripting language\ub294 100 \ub77c\uc778\uc73c\ub85c \ud574\uacb0 \uac00\ub2a5\ud588\uace0 \ub2e4\ub978 \uac83\uc740 200-300 \ub77c\uc778\uc73c\ub85c \ud574\uacb0 \uac00\ub2a5\ud588\ub2e4 Scala Immutable collections are - easy to use: few steps to do the job - concise: one word replaces a whole loop - safe: type checker is really good at catching errors. - fast: collection ops are tuned, can be parallelized. - universal: one vocabulary to work on all kinds of collections. \uc774\uac83\ub4e4\uc774 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\uc5d0 \uc788\uc5b4 \ub9e4\uc6b0 \ub9e4\ub825\uc801\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc900\ub2e4.","title":"Coursera/fpp-in-scala"},{"location":"scala/fpp-in-scala/#fpp-in-scala","text":"","title":"fpp-in-scala"},{"location":"scala/fpp-in-scala/#week1","text":"","title":"Week1"},{"location":"scala/fpp-in-scala/#lecture-11-programming-paradigms","text":"functional Programming\uc740 paradigm\uc774\ub2e4. classical imperative paradimg(Java or C)\uacfc \uc57d\uac04 \ub2e4\ub978. scala\uc5d0\uc11c\ub294 \uc774 2\uac1c\uc758 paradigm\uc744 \ud569\uce60 \uc218\ub3c4 \uc788\ub2e4. \uc774\ub294 \ub2e4\ub978 \uc5b8\uc5b4\uc5d0\uc11c\uc758 migration\uc744 \uc27d\uac8c \ud574\uc900\ub2e4. In science, a paradigm describes distinct concepts or thought patterns in some scientific discipline. Main Programming Paradigms: - imperative programming - functional programming - logic programming object-oriented programming\ub3c4 paradigm\uc774\ub77c\uace0 \ud558\ub294 \uc0ac\ub78c\ub4e4\ub3c4 \uc788\uc9c0\ub9cc \uc790\uc2e0\uc758 \uc0dd\uac01\uc73c\ub85c\ub294 \uc704 3\uac1c\uc758 \uad50\ucc28\uc810\uc5d0 \uc788\ub2e4\uace0 \uc0dd\uac01\ud55c\ub2e4.","title":"Lecture 1.1 - Programming Paradigms"},{"location":"scala/fpp-in-scala/#imperative-programming","text":"modifying mutable variables using assignments and control structures such as if-then-else, loops, break, continue, return Von Neumann computer\uc758 sequence\ub97c \uc774\ud574\ud558\ub294 \uac83\uc740 imperative program\uc744 \uc774\ud574\ud558\ub294 most common informal way\uc774\ub2e4. Processor <------BUS ------> Memory Problem: Scaling up. How can we avoid conceptualizing programs word by word? high-level abstractions(collections, polynomials, geomtric shapes, strings, documents..)\ub97c \uc815\uc758\ud558\ub294 \ud14c\ud06c\ub2c9\uc774 \ud544\uc694\ud558\ub2e4. Ideally: Develop theories of collections, shapes, strings, ...","title":"Imperative Programming"},{"location":"scala/fpp-in-scala/#what-is-a-theory","text":"A theory consist of - one or more data types - operations on these types - laws that describe the relationships between values and operations \ubcf4\ud1b5 theory\ub294 mutations \ub97c describe\ud558\uc9c0 \uc54a\ub294\ub2e4! mutation: identity\ub294 \uc720\uc9c0\ud558\uba74\uc11c something\uc744 change\ud558\ub294 \uac83\uc774\ub2e4.","title":"What is a theory"},{"location":"scala/fpp-in-scala/#theories-without-mutations","text":"theory of polynomials (a x + b) + (c x + d) = (a+c)*x + (b+d) theory of strings (a ++ b) ++ c = a ++ (b ++ c)","title":"Theories without mutations"},{"location":"scala/fpp-in-scala/#consequences-for-programming","text":"mathematical theroies\ub97c \ub530\ub974\uba74\uc11c high-level concepts \uad6c\ud604\uc744 \ud558\ub824\uba74 mutation\uc740 \uc5c6\uc5b4\uc57c \ud55c\ub2e4. - theroies do not admit it - mutation\uc740 theories\uc758 useful laws\ub97c destoy \ud560 \uc218 \uc788\ub2e4. \uadf8\ub7ec\ubbc0\ub85c - concentrate on defining theories for operators expressed as functions - avoid mutations - have powerful ways to abstract and compose functions start of function programming means avoid mutations","title":"Consequences for Programming"},{"location":"scala/fpp-in-scala/#functional-programming","text":"In a restricted sense, FP means programming without mutable variables, assignments, loops, and other imperative control structures In a wider sense, FP meas focusing on the functions In particular, functions can be valuses that are produced, consumed, and composed All this becomes easier in a functional language","title":"Functional Programming"},{"location":"scala/fpp-in-scala/#functional-programming-language","text":"In a restricted sense, a functional programming language is one which does not have mutable variables, assignments, or imperative control structures. In a wider sense, a functional programming language enables the construction of elegant programs that focus on functions. In particular, functions in a FP language are first-class citizens. This means they can be defined anywhere, including inside other functions like any other value, they can be passed as parameters to functions and returned as results as for other values, there exists a set operators to compose functions","title":"Functional Programming Language"},{"location":"scala/fpp-in-scala/#some-functional-programming-languages","text":"In the restricted sense: - Pure Lisp, XSLT, XPath, XQuery, FP - Haskell (without I/O Monad or UnsafePerformIO) In the wider sense: - Lisp, Scheme, Racket, Clojure \u25b6 SML, Ocaml, F# - Haskell (full language) - Scala - Smalltalk, Ruby (!)","title":"Some functional programming languages"},{"location":"scala/fpp-in-scala/#why-functional-programming","text":"Functional Programming is becoming increasingly popular because it offers the following benfits. - simpler reasoning principles - better modularity - good for exploiting parallelism for multicore and cloud computing.","title":"Why Functional Programming?"},{"location":"scala/fpp-in-scala/#my-summary","text":"\uc6b0\ub9ac\ub294 \uc218\ud559\uc744 \ubc30\uc6b0\uba74\uc11c mutable variables\ub97c \ubc30\uc6b4 \uc801\uc774 \uc5c6\ub2e4. 1 + 1 = 2\uc774\uace0 a + b = 3 \uc774\ub77c\uba74 \uadf8\ub0e5 3\uc778 \uac83\uc774\ub2e4. \uc624\ub298\uc740 a + b = 3 \uc774\uc5c8\ub294\ub370 \ub0b4\uc77c\uc740 a + b = 4\uc77c \uc21c \uc5c6\uc5c8\ub2e4. (ax^2 + bx + c \ub294 \uc5ec\ub7ec \uac12\uc774 \ub420 \uc218 \uc788\uaca0\uc9c0\ub9cc) \ud558\uc9c0\ub9cc imperative programming\uc5d0\uc11c\ub294 \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \uac1c\ub150\uc774\ub2e4. int a = 1; int b = 2; a + b = 3; a = 4; a + b = 6; \uc65c \uc218\ud559\uc801\uc778 \uc6d0\uce59\uc744 \uaebc\ub0b4 \ub4e4\uc5c8\ub0d0? module\ud654 \ub54c\ubb38\uc774\ub2e4. \ud504\ub85c\uadf8\ub7a8\uc774 \ubcf5\uc7a1\ud574\uc9c0\uba74\uc11c \ubaa8\ub4c8\ud654\ub294 \ud544\uc218\uc774\ub2e4. \uc798 \ub41c \ubaa8\ub4c8\ud654\ub780 \ubb34\uc5c7\uc77c\uae4c? \ud56d\uc0c1 \ub3d9\uc77c\ud55c \uacb0\uacfc\ub97c \ub9ac\ud134\ud558\ub294 \ubaa8\ub4c8\uc77c \uac83\uc774\ub2e4. map \ud568\uc218\ub97c \uc0dd\uac01\ud574\ubcf4\uba74 \uc5b4\ub290 \ud0c0\uc785\uc5d0 \uc0c1\uad00\uc5c6\uc774 List[U]\ub97c \ub9ac\ud134\ud55c\ub2e4. \uc774\ub7f0 \uc218\ud559\uc801 \uc6d0\uce59\ub4e4\uc740 mutable variables\ub97c \uc778\uc815\ud558\uc9c0 \uc54a\ub294\ub2e4. \uadf8\ub807\uae30\uc5d0 functional programming language\uc5d0 \uc798 \ub9de\ub294\ub2e4. fp\ub294 \uc774\ub7f0 \ubaa8\ud638\ud568\uc744 \uc81c\uac70\ud568\uc73c\ub85c\uc11c \uc6d0\uce59\uc744 \ubcf4\ub2e4 \uc798 \uad6c\ud604\ud558\uace0 \ubaa8\ub4c8\ud654 \ud558\uae30 \uc88b\uc73c\uba70 multicore\uc640 cloud computing \ud658\uacbd\uc5d0\uc11c \ubcd1\ub82c\ucc98\ub9ac\ub97c \uc798 \ud560 \uc218 \uc788\uac8c \ud574\uc900\ub2e4.","title":"my summary"},{"location":"scala/fpp-in-scala/#substitution-model","text":"\ud568\uc218\uc758 argument\ub97c \uc67c\ucabd\ubd80\ud130 \ubaa8\ub450 \ud3c9\uac00 \ud568\uc218\uc758 \uc624\ub978\ucabd\ubd80\ud130 \uad50\uccb4 -> \ubaa8\ub4e0 Expression\uc5d0 \uc0ac\uc6a9 \uac00\ub2a5 , No side effect foundation of functional programming\uc778 \ub78c\ub2e4 calculus\uc5d0 formalized \ub418\uc5b4\uc788\ub2e4. \ubaa8\ub4e0 Expr\uc774 reduce to a value? (X) \uc544\ub798\uc640 \uac19\uc740 \uc608\uac00 \uc788\ub2e4. def loop: Int = loop","title":"substitution model"},{"location":"scala/fpp-in-scala/#evaluation-stratigies","text":"CBV(Call By Value), CBN(Call By Name) - CBV: \ubaa8\ub4e0 args\ub294 \ud55c \ubc88\ub9cc \ud3c9\uac00\ud55c\ub2e4\ub294 \uc7a5\uc810 - CBN: \ud638\ucd9c \ub420 \ub54c\uae4c\uc9c0 not evaluted\ub41c\ub2e4\ub294 \uc7a5\uc810 \ub9cc\uc57d CBV\uac00 \uc885\ub8cc\ub41c\ub2e4\uba74 CBN\ub3c4 \uc885\ub8cc\ub41c\ub2e4? (O) \ubc18\ub300\ub85c CBN\uc774 \uc885\ub8cc\ub41c\ub2e4\uba74 CBV\ub3c4 \uc885\ub8cc\ub41c\ub2e4? (X) scala\uc5d0\uc11c CBN\uc744 \uc4f0\ub294 \ubc29\ubc95\uc740 parameter\uc5d0 => \ub97c \ubd99\uc774\uba74 \ub41c\ub2e4. def myFunc(a:=> Int) = a","title":"Evaluation Stratigies"},{"location":"scala/fpp-in-scala/#value-definitions","text":"val x = 2 val y = square(x) // \ubc14\ub85c \ud3c9\uac00\ub41c\ub2e4. def loop: Boolean = loop def x = loop // (O) def\ub294 \ud638\ucd9c\ub420\ub54c \ud3c9\uac00\ub41c\ub2e4. val x = loop // (X) Error def and(x: Boolean, y: Boolean) = if (x) y else false and(false, loop) // (X) Error def and2(x: Boolean, y:=> Boolean) = if (x) y else false and2(false, loop) // false","title":"Value Definitions"},{"location":"scala/fpp-in-scala/#nested-functions","text":"small func\ub85c \ubd84\ub9ac\ud558\ub294 \uac83. good FP styles sqrtIter, imporve \uac19\uc740 \ud568\uc218\ub4e4\uc740 \uc678\ubd80\uc5d0 \uacf5\uac1c(direct \ud638\ucd9c) \ud558\uace0 \uc2f6\uc9c0 \uc54a\uc744 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c \ubcf4\uc870 \ud568\uc218\ub4e4\uc744 \ub0b4\ubd80 \ud568\uc218\ub85c \ub460\uc73c\ub85c\uc368 name-space pollution\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\ub2e4. def sqrt(x: Double) = { def improve def sqrtIter }","title":"Nested Functions"},{"location":"scala/fpp-in-scala/#lexical-scoping","text":"outer block\uc5d0 \uc788\ub294 definitions\ub294 inside block\uc5d0\uc11c visible\ud558\ub2e4. \ubcf4\ud1b5 \ubb38\uc7a5 \ub77c\uc778 \ub05d ; \ub294 optional\uc774\ub2e4. \ub2e4\ub9cc \ud55c \ubb38\uc7a5\uc5d0 \uc5ec\ub7ec expr\uc744 \ud45c\ud604\ud560 \ub54c\ub294 \ud544\uc218 \uc774\ub2e4. val y = x + 1; y + y","title":"Lexical Scoping"},{"location":"scala/fpp-in-scala/#tail-recursion","text":"calls itself as its last action. the function's stack frame can be reused (one stack frame\uc774 \ud544\uc694\ud558\uba70, tail calls \ub77c\uace0 \ud568) @tailrec annotation\uc744 \ud568\uc218 \uc704\uc5d0 \ucd94\uac00\ud558\uba74 \ud574\ub2f9 \ud568\uc218\uac00 tail recur \ud558\uc9c0 \uc54a\uc744 \uc2dc \uc624\ub958\uac00 \ubc1c\uc0dd\ud55c\ub2e4. \uc544\ub798\uc758 factorial \ud568\uc218\ub294 tailrc \ud568\uc218\uac00 \uc544\ub2c8\uba70 gcd\ub294 tailrec \ud568\uc218\uc774\ub2e4. def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b) def factorial(n: Int): Int = if (n == 0) 1 else n * factorial(n - 1) \uadf8 \ucc28\uc774\ub294 gcd\ub294 \uc2a4\ud15d\uc744 \uc9c4\ud589\uc744 \uacc4\uc18d \ud558\ub354\ub77c\ub3c4 \ubcf8\uc778 \ud638\ucd9c\ub9cc \uacc4\uc18d \ud558\uac8c \ub418\uc9c0\ub9cc factorial \uac19\uc740 \uacbd\uc6b0\uc5d0\ub294 \uc88c\uce21\uc774 \uacc4\uc18d \ub298\uc5b4\ub09c\ub2e4 4 * factorial(3) \uc774\ub97c tail recursive \ud558\uac8c \ubcc0\uacbd\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. def factorial(n: Int): Int = { @tailrec def loop(acc: Int, n: Int): Int = if (n == 0) acc else loop(acc * n, n -1) loop(1, n) } Donal Knuth said premature optimization is the source of the evil","title":"Tail Recursion"},{"location":"scala/fpp-in-scala/#week2","text":"","title":"Week2"},{"location":"scala/fpp-in-scala/#higher-order-functions","text":"pass functions as arguments and retun them as results. functional languages treat functions as first-class values. = like any other value, a function can be passed as a parameter and returned as a result. provides a flexible way to compose program.","title":"Higher order functions"},{"location":"scala/fpp-in-scala/#anonymous-function","text":"\ud568\uc218\ub97c parameter\ub85c \uc804\ub2ec\ud558\ub2e4\ubcf4\uba74 many small function\uc744 \ub9cc\ub4e4\uac8c \ub41c\ub2e4. \uadf8\ub807\uac8c \ub418\uba74 \uac01\uac01\uc758 naming\uc744 \uc815\ud558\ub294 \uac83\uc740 \uc5b4\ub835\uac8c \ub41c\ub2e4. => anonymous function\uc744 \uc0ac\uc6a9\ud55c\ub2e4. def str = \"abc\"; println(str) println(\"abc\") \uc704\uc5d0\ub294 str \ubcc0\uc218\ub97c \uc815\uc758\ud574\uc11c \ud638\ucd9c\ud588\uace0 \uc544\ub798\ub294 \uc815\uc758 \uc5c6\uc774 \uc0ac\uc6a9 \ud588\ub2e4. \uc774\uac83\uc774 \uac00\ub2a5\ud55c \uc774\uc720\ub294 \ubb58\uae4c? => str\uc740 literals \uc774\uae30 \ub54c\ubb38\uc774\ub2e4. \ub9c8\ucc2c\uac00\uc9c0\ub85c \uc774\ub984 \uc5c6\uc774 \ud568\uc218\ub97c \uc4f0\uba74 function literals\uac00 \ub41c\ub2e4. = anonymous functions // cube anonymous func (x: Int) => x * x * x (x: Int) \ub294 parameter x * x * x \ub294 body def sum(f: Int => Int, a: Int, b: Int): Int = { @tailrec def loop(a: Int, acc: Int): Int = { if (a > b) acc else loop(a + 1, f(a) + acc) } loop(a, 0) } def sumInts(a: Int, b: Int) = sum(x => x, a, b) def sumCubes(a: Int, b: Int) = sum(x => x * x * x, a, b)","title":"Anonymous Function"},{"location":"scala/fpp-in-scala/#currying","text":"\uc544\ub798 \ud568\uc218\ub97c \ub354 \uc9e7\uac8c \ud560 \uc218\ub294 \uc5c6\uc744\uae4c? def sumInts(a: Int, b: Int) = sum(x => x, a, b) def sum() def sum(f: Int => Int)(a: Int, b: Int): Int = if (a > b) 0 else f(a) + sum(f)(a + 1, b) def product(f: Int => Int)(a: Int, b: Int): Int = if (a > b) 1 else f(a) * product(f)(a + 1, b) \uc704\uc640 \uac19\uc740 \uc2a4\ud0c0\uc77c\uc758 definition\uacfc function\uc744 currying\uc774\ub77c\uace0 \ubd80\ub978\ub2e4. Haskell Brooks Curry\uc758 \uc774\ub984\uc744 \ub534 \ub124\uc774\ubc0d\uc774\ub2e4. Idea\ub294 \uadf8 \ubcf4\ub2e4 \uc804\uc778 Schonfinkel\uacfc Frege\uc5d0 \uc758\ud574\uc11c \ub098\uc654\uc9c0\ub9cc currying\uc774\ub780 \ub124\uc784\uc73c\ub85c \uad73\uc5b4\uc84c\ub2e4.","title":"Currying"},{"location":"scala/fpp-in-scala/#example-finding-fixed-points","text":"A number x is called a fixed point(\uace0\uc815 \uc810) of a function f if f(x) = x \uc608\ub85c f: x => 1 + x/2 \ub77c \ud560 \ub54c fixed point\ub294 2\uc774\ub2e4. f(2) = 2\uc774\ubbc0\ub85c \uba87\uba87 \ud568\uc218\ub4e4\uc740 f\ub97c \ubc18\ubcf5\uc801\uc73c\ub85c \uc218\ud589\ud568\uc73c\ub85c\uc11c fixed point\ub97c \ucc3e\uc744 \uc218 \uc788\ub2e4. x, f(x), f(f(x)), f(f(f(x))), ... initial estimate\ub85c \uc2dc\uc791\ud574\uc11c f\ub97c \ubc18\ubcf5\uc801\uc73c\ub85c \uc218\ud589\ud558\ub2e4\ubcf4\uba74 \ub354 \uc774\uc0c1 \ubcc0\ud558\uc9c0 \uc54a\ub294 \uac12 \ud639\uc740 \ubcc0\uacbd\uc774 \ucda9\ubd84\ud788 \uc801\uc5b4\uc84c\uc744 \ub54c\uc758 \uac12\uc744 fixed point\ub77c \ubd80\ub97c \uc218 \uc788\ub2e4. import math.abs val tolerance = 0.0001 def isCloseEnough(x: Double, y: Double): Boolean = abs((x - y) / x) / x < tolerance def fixedPoint(f: Double => Double)(firstGuess: Double) = { @tailrec def iterate(guess: Double): Double = { val next = f(guess) if (isCloseEnough(guess, next)) next else iterate(next) } iterate(firstGuess) } fixedPoint(x => 1 + x/2)(1) // 1.9975 def sqrt(x: Double) = fixedPoint(y => x / y)(1) sqrt(2) // \ubb34\ud55c loop \uc704\uc758 \uc608\uc5d0\uc11c sqrt(2)\ub97c \uc218\ud589\ud558\uba74 \ubb34\ud55c loop\uac00 \ubc1c\uc0dd\ud55c\ub2e4. 1\uacfc 2 \ub97c \uacc4\uc18d \ubc18\ubcf5\ud55c\ub2e4 \uc774\ub97c \ud574\uacb0 \ud558\uae30 \uc704\ud574\uc11c\ub294 \uccab \ubc88\uc9f8 \uacc4\uc0b0 \uac12\uacfc \ub450 \ubc88\uc9f8 \uacc4\uc0b0 \uac12\uc758 \ud3c9\uade0\uc744 \uad6c\ud558\uba74 \ub41c\ub2e4. def sqrt(x: Double) = fixedPoint(y => (y + x / y) / 2)(1)","title":"Example: Finding Fixed Points"},{"location":"scala/fpp-in-scala/#functions-as-return-values","text":"\uc704\uc758 \uc608\uc81c\uc5d0\uc11c \ud3c9\uade0\uc744 \ud1b5\ud574 \uc548\uc815\ud654\uc2dc\ud0a4\ub294 \uae30\uc220\uc740 \ucd94\uc0c1\ud654 \ub420 \uc218 \uc788\ub2e4. def averageDamp(f: Double => Double)(x: Double) = (x + f(x)) / 2 def sqrt3(x: Double) = fixedPoint(averageDamp(y => x / y))(1) Higher Order Function\uc774 \ud56d\uc0c1 \uc633\uc740 \uac83\uc740 \uc544\ub2c8\uba70 \uc801\uc808 \ud560 \ub54c \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4.","title":"functions as return values"},{"location":"scala/fpp-in-scala/#functions-and-data","text":"","title":"Functions and Data"},{"location":"scala/fpp-in-scala/#classes","text":"class Rational(x: Int, y: Int): def numer = x def denom = y \uc704 \uc815\uc758\ub294 two entities\ub97c \uc0dd\uc131\ud55c\ub2e4. - Rational \uc774\ub77c\ub294 \uc774\ub984\uc758 new type - \uc774 type\uc758 element\ub97c \ub9cc\ub4e4\uae30 \uc704\ud55c Rational constructo \uc2a4\uce7c\ub77c\ub294 types\uacfc value\uc758 names\ub97c different namespace \uc5d0 \ubcf4\uad00\ud558\uae30 \ub54c\ubb38\uc5d0 \ucda9\ub3cc\uc744 \uac71\uc815\ud560 \ud544\uc694 \uc5c6\ub2e4.","title":"Classes"},{"location":"scala/fpp-in-scala/#objects","text":"elements of a class type\uc744 objects\ub77c\uace0 \ubd80\ub978\ub2e4. class \uc758 \uc0dd\uc131\uc790\ub97c calling \ud568\uc73c\ub85c\uc11c object\ub97c \ub9cc\ub4e4 \uc218 \uc788\ub2e4. Rational(1, 2) \uc544\ub798\uc640 \uac19\uc774 class\ub0b4 member\uc5d0 \uc811\uadfc \uac00\ub2a5\ud558\ub2e4 val x = Rational(1 ,2) x.numer x.denom object rationals { val x = new Rational(1, 3) val y = new Rational(5, 7) val z = new Rational(3, 2) x.add(y).mul(z) } class Rational(x: Int, y: Int) { def numer = x def denom = y def add(r: Rational) = new Rational(numer * r.denom + r.numer * denom, denom * r.denom) def mul(r: Rational) = new Rational(numer * r.numer, denom * r.denom) def neg = new Rational(-numer, denom) def sub(r: Rational) = add(r.neg) override def toString = s\"$numer/$denom\" }","title":"Objects"},{"location":"scala/fpp-in-scala/#more-fun-with-rationals","text":"Client's view\uc5d0\uc11c\ub294 \ub0b4\ubd80\uac00 \uc5b4\ub5bb\uac8c \ub3d9\uc791\ud558\ub358\uc9c0 \ub3d9\uc77c\ud558\uac8c \ubcf4\uc778\ub2e4. without affecting client\ub97c \ud558\uba74\uc11c \ub2e4\ub978 \uad6c\ud604\uc744 \uc120\ud0dd\ud558\ub294 \uac83\uc744 data abstraction \uc774\ub77c\uace0 \ud55c\ub2e4. S/E\uc5d0\uc11c\uc758 cornerstone\uc774\ub2e4.","title":"More Fun With Rationals"},{"location":"scala/fpp-in-scala/#self-reference","text":"inside of a class, this \ub294 \ud604\uc7ac \uc2e4\ud589 \uc911\uc778 method\ub0b4\uc5d0\uc11c\uc758 object\ub97c \uc758\ubbf8\ud55c\ub2e4","title":"Self Reference"},{"location":"scala/fpp-in-scala/#preconditions","text":"require \ub85c class\uc5d0 \uc870\uac74\uc744 \ucd94\uac00\ud560 \uc218 \uc788\ub2e4. \uc870\uac74\uc5d0 \ub9de\uc9c0 \uc54a\uc73c\uba74 IllegalArgumentException\uc774 \ubc1c\uc0dd\ud558\uba70 \ucd94\uac00\ud55c \uc5d0\ub7ec \uba54\uc138\uc9c0\uac00 \ucd9c\ub825\ub41c\ub2e4.","title":"Preconditions"},{"location":"scala/fpp-in-scala/#assertions","text":"require\uc640 \ube44\uc2b7\ud55c \uc758\ubbf8\uc774\ub2e4. require\uc640 \ub3d9\uc77c\ud558\uac8c condtion\uacfc optional message string\uc744 \ubc1b\ub294\ub2e4. val x = sqrt(y) assert(x >= 0) fail\uc77c \uacbd\uc6b0 assert\ub294 require\uc640 \ub2ec\ub9ac AssertionError\ub97c \ubc1c\uc0dd\ud55c\ub2e4. require\ub294 \ud568\uc218 \ud638\ucd9c\uc790\uc5d0\uac8c precondition\uc744 \uac15\uc694\ud560 \ub54c \uc4f0\uc778\ub2e4 assert\ub294 \ud568\uc218 \uc790\uc2e0\uc774 \uccb4\ud06c \ud560 \ub54c \uc0ac\uc6a9\ud55c\ub2e4.","title":"Assertions"},{"location":"scala/fpp-in-scala/#constructors","text":"\ubaa8\ub4e0 class\ub294 primary constructor(\uae30\ubcf8 \uc0dd\uc131\uc790)\uac00 \uc554\uc2dc\uc801\uc73c\ub85c \uc788\ub2e4. - class\uc758 \ubaa8\ub4e0 paramters\ub97c \ubc1b\uace0 - class body\uc758 \ubaa8\ub4e0 statement\ub97c \uc2e4\ud589\ud55c\ub2e4. Java \uac19\uc774 \uc5ec\ub7ec \uc0dd\uc131\uc790\ub97c \uac16\ub294 \uac83\ub3c4 \uac00\ub2a5\ud558\ub2e4. object rationals { val x = new Rational(1, 3) val y = new Rational(5, 7) val z = new Rational(3, 2) x.add(y).mul(z) y.add(y) x.less(y) x.max(y) new Rational(2) } class Rational(x: Int, y: Int) { require(y != 0, \"denominator must be nonezero\") def this(x: Int) = this(x, 1) // \uc5ec\uae30\uc5d0\uc11c\uc758 this\ub294 constructor \uc758\ubbf8\ub85c \uc4f0\uc778\ub2e4. private def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b) private val g = gcd(x, y) // val\ub85c \uc120\uc5b8\ud588\uae30\uc5d0 \ubc14\ub85c \ud3c9\uac00\ub418\uc11c \ub2e4\uc74c \ubd80\ud134 \uacc4\uc0b0\uc744 \uc548\ud558\uace0 \uc7ac\uc0ac\uc6a9\ud55c\ub2e4. def numer = x / g // def numer = x / gcd(x,y) // \ub9cc\uc57d \uc774\uc640 \uac19\uc774 \uc120\uc5b8 \ud558\uba74 \ub9e4\ubc88 gcd\ub97c \uacc4\uc0b0\ud574\uc57c \ud55c\ub2e4. \uacc4\uc0b0 \ub9ac\uc18c\uc2a4\uac00 \ud06c\uace0 \uac00\ub054 \ud638\ucd9c\ub420 \ub54c \uc0ac\uc6a9\ud558\uba74 \uc88b\ub2e4. def denom = y / g def less(that: Rational) = numer * that.denom < that.numer * denom def max(that: Rational) = if (this.less(that)) that else this def add(r: Rational) = new Rational(numer * r.denom + r.numer * denom, denom * r.denom) def mul(r: Rational) = new Rational(numer * r.numer, denom * r.denom) def neg = new Rational(-numer, denom) def sub(r: Rational) = add(r.neg) override def toString = s\"$numer/$denom\" }","title":"Constructors"},{"location":"scala/fpp-in-scala/#evaluation-and-operators","text":"","title":"Evaluation and Operators"},{"location":"scala/fpp-in-scala/#operators","text":"","title":"Operators"},{"location":"scala/fpp-in-scala/#infix-notation","text":"parameter\ub97c \uac16\ub294 \ubaa8\ub4e0 \uba54\uc18c\ub4dc\ub294 infix operaotr\ucc98\ub7fc \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. r add s r.add(s) r less s r.less(s) r max s r.max(s)","title":"Infix Notation"},{"location":"scala/fpp-in-scala/#relaxed-identifiers","text":"operaotr\ub294 identifier\ub85c \uc0ac\uc6a9\ub420 \uc218 \uc788\ub2e4. - \uc601\ubb38\uc790: \ubb38\uc790\ub85c \uc2dc\uc791\ud558\uace0, \ub4a4\uc5d0\ub294 \ubb38\uc790 \ud639\uc740 \uc22b\uc790\uac00 \uc62c \uc218 \uc788\ub2e4. - Symbolic: operator symbol\ub85c \uc2dc\uc791\ud574\uc11c, \ub2e4\ub978 \uc2ec\ubcfc\uc774 \ub4a4\uc5d0 \uc62c \uc218 \uc788\ub2e4. - _ \ubb38\uc790\ub294 \ubb38\uc790\ub85c \uce74\uc6b4\ud2b8 \ub41c\ub2e4 - \uc601\ubb38\uc790 identifiers\ub294 underscore\ub85c \ub05d\ub0a0 \uc218 \uc788\uace0 \ub4a4\uc5d0 \ub2e4\ub978 operator symbols\uac00 \ubd99\uc744 \uc218 \uc788\ub2e4. * \ub9cc\uc57d \ub05d\uc774 symbol\ub4e4\ub85c \ub05d\ub098\uba74 \ub4a4\uc5d0 \ud0c0\uc785\uc744 \uc704\ud55c : \uacfc \ud55c \uce78 \ub744\uc6cc\uc57c \ud55c\ub2e4. examples - x1 - * - +?%& - vector_++ - counter_= -a \ucc98\ub7fc \ube7c\uae30\uac00 \uc544\ub2c8\ub77c \ub9c8\uc774\ub108\uc2a4 operator\ub97c \ucd94\uac00\ud558\uace0 \uc2f6\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud574\uc57c \ud55c\ub2e4. ( unary_ \uac00 \uc55e\uc5d0 \ubd99\uc5b4\uc57c \ud558\uace0 : \uacfc \ud55c\uce78 \ub744\uc6cc \uc368\uc57c \ud55c\ub2e4.) def unary_- : Rational = new Rational(-numer, denom)","title":"Relaxed Identifiers"},{"location":"scala/fpp-in-scala/#precedence-rules","text":"\uc5f0\uc0b0\uc790 \uc6b0\uc120\uc21c\uc704. \uccab \ubc88\uc9f8 \ubb38\uc790\uc5d0 \ub530\ub77c \uacb0\uc815\ub41c\ub2e4. Java\ud639\uc740 C\uc640 \ucc28\uc774 \uc5c6\ub2e4. 1\ubc88\uc774 \uac00\uc7a5 \ub0ae\uc740 \uc21c\uc704\uc774\ub2e4. (all letters) | ^ & < > = ! : - / % (all other special values) class Rational(x: Int, y: Int) { require(y != 0, \"denominator must be nonezero\") def this(x: Int) = this(x, 1) // \uc5ec\uae30\uc5d0\uc11c\uc758 this\ub294 constructor \uc758\ubbf8\ub85c \uc4f0\uc778\ub2e4. private def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b) private val g = gcd(x, y) // val\ub85c \uc120\uc5b8\ud588\uae30\uc5d0 \ubc14\ub85c \ud3c9\uac00\ub418\uc11c \ub2e4\uc74c \ubd80\ud134 \uacc4\uc0b0\uc744 \uc548\ud558\uace0 \uc7ac\uc0ac\uc6a9\ud55c\ub2e4. def numer = x / g // def numer = x / gcd(x,y) // \ub9cc\uc57d \uc774\uc640 \uac19\uc774 \uc120\uc5b8 \ud558\uba74 \ub9e4\ubc88 gcd\ub97c \uacc4\uc0b0\ud574\uc57c \ud55c\ub2e4. \uacc4\uc0b0 \ub9ac\uc18c\uc2a4\uac00 \ud06c\uace0 \uac00\ub054 \ud638\ucd9c\ub420 \ub54c \uc0ac\uc6a9\ud558\uba74 \uc88b\ub2e4. def denom = y / g // def less(that: Rational) = numer * that.denom < that.numer * denom def < (that: Rational) = numer * that.denom < that.numer * denom def max(that: Rational) = if (this.<(that)) that else this def +(r: Rational) = new Rational(numer * r.denom + r.numer * denom, denom * r.denom) def mul(r: Rational) = new Rational(numer * r.numer, denom * r.denom) def unary_- : Rational = new Rational(-numer, denom) def -(that: Rational) = this + -that override def toString = s\"$numer/$denom\" }","title":"Precedence Rules"},{"location":"scala/fpp-in-scala/#week3","text":"","title":"Week3"},{"location":"scala/fpp-in-scala/#class-hierarchies","text":"\uc2e4\uc81c \uba54\uc18c\ub4dc\ub294 runtime type\uc5d0 \uc758\uc874\ud55c\ub2e4. \uc774\ub97c dynamic binding\uc774\ub77c\uace0 \ud55c\ub2e4. \uc774\ub294 OOP\uc5d0 \uae30\ubcf8 \uc694\uc18c\uc774\ub2e4. \uc544\ub798 \ud568\uc218\ub294 abstract class\uc774\ub2e4. abstract class IntSet { def incl(x: Int): IntSet def contains(x: Int): Boolean } \ucd94\uc0c1 \ud074\ub798\uc2a4\ub294 - \uad6c\ud604\uccb4\uac00 \uc5c6\ub294 \uba64\ubc84\ub97c \ud3ec\ud568\ud560 \uc218 \uc788\ub2e4. - new operator\ub97c \uc0ac\uc6a9\ud55c \uc778\uc2a4\ud134\uc2a4 \uc0dd\uc131\uc744 \ud560 \uc218 \uc5c6\ub2e4. abstract class IntSet { def incl(x: Int): IntSet def contains(x: Int): Boolean } class NonEmpty(elem: Int, left: IntSet, right: IntSet) extends IntSet { override def incl(x: Int): IntSet = if (x < elem) new NonEmpty(elem, left incl x, right) else if (x > elem) new NonEmpty(elem, left, right incl x) else this override def contains(x: Int): Boolean = if (x < elem) left contains x else if (x > elem) right contains x else true override def toString = \"{\" + left + elem + right + \"}\" } class Empty extends IntSet { override def incl(x: Int) = new NonEmpty(x, new Empty, new Empty) override def contains(x: Int) = false override def toString = \".\" } val t1 = new NonEmpty(3, new Empty, new Empty) val t2 = t1 incl 4 \uc704\uc758 \uc608\uc5d0\uc11c IntSet\uc740 Empty\uc640 NonEmpty\uc758 superclass\uc774\ub2e4. Empty\uc640 NonEmpty\ub294 IntSet\uc758 subclasses\uc774\ub2e4. \uc2a4\uce7c\ub77c\uc5d0\uc11c superclass\uac00 \uc5c6\uc73c\uba74 java.lang\uc5d0 \uc788\ub294 Java standard class Object \ub97c \uc0c1\uc18d \ubc1b\ub294\ub2e4. \ud074\ub798\uc2a4\uc758 direct or indirect superclass\ub97c base classes\ub77c\uace0 \ud55c\ub2e4. NonEmpty\uc640 IntSet\uc758 base classes\ub294 Object\uc774\ub2e4. non-abstract definition\uc744 redfine\ud560 \ub54c\ub294 override keyword\ub97c \uc368\uc918\uc57c \ud55c\ub2e4. abstract class Base { def foo = 1 def bar: Int } class Sub extends Base { override def foo = 2 def bar = 3 }","title":"Class Hierarchies"},{"location":"scala/fpp-in-scala/#object-definitions","text":"\uc704\uc758 \uc608\uc5d0\uc11c \uc720\uc800\uac00 \ub9ce\uc740 EmptySet\uc744 \ub9cc\ub4e4\uac8c \ub418\uba74 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud55c\ub2e4. \uadf8\ub798\uc11c \uc774\ub97c object\ub85c \uc120\uc5b8\ud558\ub294 \uac83\uc774 \ub0ab\ub2e4. object Empty extends IntSet { def incl(x: Int) = new NonEmpty(x, new Empty, new Empty) def contains(x: Int) = false } \uc774\ub807\uac8c \ud558\uba74 Empty\ub77c\ub294 \uc774\ub984\uc758 singleton object\uac00 \ub9cc\ub4e4\uc5b4 \uc9c4\ub2e4. \uc774\ub85c\uc368 \ub2e4\ub978 Empty \uc778\uc2a4\ud134\uc2a4\ub294 \ub9cc\ub4e4\uc5b4\uc9c8 \uc218 \uc5c6\ub2e4. Singleton Object\ub294 values \uc774\ubbc0\ub85c, Empty\ub294 \ubc14\ub85c \ud3c9\uac00\ub41c\ub2e4.","title":"Object Definitions"},{"location":"scala/fpp-in-scala/#programs","text":"Scala\uc5d0\uc11c standalone application\uc744 \ub9cc\ub4dc\ub294 \uac83\uc740 \uac00\ub2a5\ud558\ub2e4. main method\ub97c \ud3ec\ud568\ud558\ub294 object\ub97c \ub9cc\ub4e4\uba74 \ub41c\ub2e4. object Hello { def main(args: Array[String]) = println(\"hello world!\") } \ud504\ub85c\uadf8\ub7a8\uc774 \ucef4\ud30c\uc77c \ub418\uace0 \ub098\uba74 \uc544\ub798\uc758 \ucee4\ub9e8\ub4dc\ub85c \uc2e4\ud589\ud560 \uc218 \uc788\ub2e4. > scala Hello","title":"Programs"},{"location":"scala/fpp-in-scala/#dynamic-binding","text":"code invoked by a method call depends on the runtime of the object that contains the method ex) Empty contains 1 -> false","title":"Dynamic Binding"},{"location":"scala/fpp-in-scala/#lecture-32-how-classes-are-organized","text":"","title":"Lecture 3.2 - How Classes Are Organized"},{"location":"scala/fpp-in-scala/#packages","text":"Classes\uc640 objects\ub294 package\uc548\uc5d0 \uad6c\uc131\ub41c\ub2e4\ub41c package\uc5d0 \uc18d\ud558\ub294 class, object\ub294 \uc18c\uc2a4 \ud30c\uc77c\uc758 \ucd5c\uc0c1\ub2e8\uc5d0 package\ub97c \uc368\uc57c \ud55c\ub2e4. package progfun.examples object Hello { ... } \uc544\ub798\uc640 \uac19\uc774 \ud504\ub85c\uadf8\ub7a8\uc744 \uc2e4\ud589\ud560 \uc218 \uc788\ub2e4. > scala progfun.examples.Hello","title":"Packages"},{"location":"scala/fpp-in-scala/#forms-of-imports","text":"import week3.Rational // named imports import week3.{Rational, Hello} // named imports import week3._ // wildcard import","title":"Forms of Imports"},{"location":"scala/fpp-in-scala/#automatic-imports","text":"All members of package scala All members of package java.lang All members of the singleton object scala.Predef Int: scala.Int Boolean: scala.Boolean Object : java.lang.Object require: scala.Predef.require assert: scala.Predef.assert","title":"Automatic Imports"},{"location":"scala/fpp-in-scala/#traits","text":"Java \ucc98\ub7fc Scala\ub294 \uc624\uc9c1 \ud558\ub098\uc758 superclass\ub97c \uac00\uc9c8 \uc218 \uc788\ub2e4.(Single Inheritance) \ud558\uc9c0\ub9cc \uc5ec\ub7ec\uac1c\uc758 supertypes\ub97c \uac16\uace0 \uc2f6\ub2e4\uba74 \uc5b4\ub5bb\uac8c \ud560\uae4c? traits\ub97c \uc0ac\uc6a9\ud558\uba74 \ub41c\ub2e4. trait\uc740 abstract class\ucc98\ub7fc \uc815\uc758 \ud558\uba74\uc11c trait \ud0a4\uc6cc\ub4dc\ub97c \uc4f0\uba74 \ub41c\ub2e4\ub97c trait Planar { def height: Int def width: Int def surface = height * width } class Square extends Shape with Planar with Movable ... trait\uc740 Java\uc758 interface\uc640 \ube44\uc2b7\ud558\uc9c0\ub9cc \ub354 \uac15\ub825\ud558\ub2e4. fields\uc640 concrete methods(\uc815\uc758\ub41c \uba54\uc18c\ub4dc)\ub97c \uac00\uc9c8 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \ud558\uc9c0\ub9cc trait\uc740 (value) parameters\ub97c \uac00\uc9c8 \uc218 \uc5c6\ub2e4. \uc774\ub294 \ud074\ub798\uc2a4\ub9cc \uac00\ub2a5\ud558\ub2e4.","title":"Traits"},{"location":"scala/fpp-in-scala/#top-types","text":"Any: The base type of all types. Methods: '==', '!=', 'equals', 'hashCode', 'toString AnyRef: The base type of all reference types; Alias of 'java.lang.Object' AnyVal: The base type of all primitive types","title":"Top Types"},{"location":"scala/fpp-in-scala/#the-nothing-type","text":"Nothing\uc740 Scala type hierarchy\uc5d0\uc11c \ucd5c \ud558\ub2e8\uc5d0 \uc788\ub2e4. type Nothing\uc5d0\ub294 value\uac00 \uc5c6\ub2e4. \uc65c \uc4f0\uc77c\uae4c? - To signal abnormal termination - As an element type of empty collection","title":"The Nothing Type"},{"location":"scala/fpp-in-scala/#exceptions","text":"\uc790\ubc14\uc640 \uc720\uc0ac\ud558\ub2e4 throw Exc \uc774 expr\uc758 type\uc740 Nothing\uc774\ub2e4. example def error(msg: String) = throw new Error(msg) error(\"test\")","title":"Exceptions"},{"location":"scala/fpp-in-scala/#the-null-type","text":"every reference class type\uc740 null \uac12\uc744 \uac16\ub294\ub2e4. null\uc758 \ud0c0\uc785\uc740 Null \uc774\ub2e4. Null\uc740 Object\ub97c \uc0c1\uc18d\ubc1b\ub294 \ubaa8\ub4e0 \ud074\ub798\uc2a4\uc758 subtype\uc774\ub2e4. \ud558\uc9c0\ub9cc AnyVal\uc758 subtypes\uacfc\ub294 incompativle \ud558\ub2e4. val x = null // x: Null val y: String = null // y: String val z: Int = null // error: type mismatch","title":"The Null Type"},{"location":"scala/fpp-in-scala/#lecture-33-polymorphism","text":"","title":"Lecture 3.3 - Polymorphism"},{"location":"scala/fpp-in-scala/#type-parameter","text":"\uc5ec\ub7ec \ud0c0\uc785\uc5d0 \ub300\uc751\ud560 \uc218 \uc788\ub294 \ud0c0\uc785\uc774\ub2e4. trait List[T] class Cons[T](val head: T, val tail: List[T]) extends List[T] class Nil[T] extends List[T] \ud0c0\uc785 \ud30c\ub77c\ubbf8\ud130\ub294 square brackets \uc548\uc5d0 \uc4f0\uc778\ub2e4.","title":"Type Parameter"},{"location":"scala/fpp-in-scala/#generic-functions","text":"classes\ucc98\ub7fc function\uc5d0\ub3c4 type parameter\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. def singleton[T](elem: T) = new Cons[T](elem, new Nil[T]) singleton[Int](1) singleton[Boolean](true)","title":"Generic Functions"},{"location":"scala/fpp-in-scala/#type-inference","text":"\uc2a4\uce7c\ub77c\ub294 function call\uc758 arguments\ub85c \ubd80\ud130 parameter\uc758 \uc633\uc740 \ud0c0\uc785\uc744 \ucd94\uce21\ud560 \uc218 \uc788\ub2e4. \uadf8\ub807\uae30\uc5d0 \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0\uc5d0\uc11c type parameters\ub294 \uc548\uc368\ub3c4 \ub41c\ub2e4. singleton(1) singleton(true)","title":"Type Inference"},{"location":"scala/fpp-in-scala/#type-evaluation","text":"\uc2a4\uce7c\ub77c\uc5d0\uc11c Type parameter\ub294 evaluation\uc5d0 \uc601\ud5a5\uc744 \ub07c\uce58\uc9c0 \uc54a\ub294\ub2e4. \ubaa8\ub4e0 type parameters\uc640 type arguments\ub294 \ud504\ub85c\uadf8\ub7a8\uc744 \ud3c9\uac00\ud558\uae30 \uc804\uc5d0 \uc81c\uac70\ub41c\ub2e4. \uc774\ub97c type erasure \ub77c\uace0 \ubd80\ub978\ub2e4. Java, Scala, Haskell, ML, OCaml\uc5d0\uc11c\ub294 type erasure\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \ud558\uc9c0\ub9cc C++, C#, F# \uac19\uc740 \uc5b8\uc5b4\ub294 run time\uc2dc\uc5d0\ub3c4 type parameter\ub97c \uc720\uc9c0\ud55c\ub2e4.","title":"Type Evaluation"},{"location":"scala/fpp-in-scala/#polymorphism","text":"function type comes \"in many forms\" - function\uc774 \uc5ec\ub7ec \ud0c0\uc785\uc758 argument\uc5d0 \uc801\uc6a9\ub420 \uc218 \uc788\ub2e4 - \ud0c0\uc785\uc774 \uc5ec\ub7ec \ud0c0\uc785\uc758 \uc778\uc2a4\ud134\uc2a4\ub97c \uac00\uc9c8 \uc218 \uc788\ub2e4. \ud3f4\ub9ac\ubab0\ud53c\uc998\uc758 \ub450 \uac00\uc9c0 \ud615\ud0dc - subtyping: instances of subclass \ub294 base class\ub85c \uc804\ub2ec \ub420 \uc218 \uc788\ub2e4. - generic: function \ud639\uc740 \ud074\ub798\uc2a4\uc758 instance\ub294 type paramterization\uc73c\ub85c \ub9cc\ub4e4 \uc218 \uc788\ub2e4.","title":"Polymorphism"},{"location":"scala/fpp-in-scala/#week4","text":"","title":"Week4"},{"location":"scala/fpp-in-scala/#lecture-41-objects-everywhere","text":"","title":"Lecture 4.1 - Objects Everywhere"},{"location":"scala/fpp-in-scala/#pure-object-orientation","text":"A pure object-oriented language is one in which every value is an object \uc2a4\uce7c\ub77c\ub294 pure object-oriented language\uc778\uac00?","title":"Pure Object Orientation"},{"location":"scala/fpp-in-scala/#standard-classes","text":"\uc2a4\uce7c\ub77c\uc5d0\uc11c Int \ud639\uc740 Boolean \uac19\uc740 \ud0c0\uc785\ub4e4\uc740 \ud2b9\ubcc4\ucde8\uae09\uc744 \ubc1b\uc9c0 \uc54a\ub294\ub2e4. \ub2e4\ub978 \ud074\ub798\uc2a4\ub4e4\uacfc \ub9c8\ucc2c\uac00\uc9c0\ub85c scala \ud328\ud0a4\uc9c0 \uc548\uc5d0 \uc815\uc758\ub41c \uac83\uc774\ub2e4.","title":"Standard Classes"},{"location":"scala/fpp-in-scala/#pure-booleans","text":"Boolean type\uc740 JVM\uc758 primitive type boolean\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub97c \ud074\ub798\uc2a4\ub85c\ub3c4 \uc815\uc758\ud560 \uc218 \uc788\ub2e4. package idealized.scala abstract class Boolean { def ifThenElse[T](t: => T, e: => T): T def && (x: => Boolean): Boolean = ifThenElse(x, false) def || (x: => Boolean): Boolean = ifThenElse(true, x) def unary_!: Boolean = ifThenElse(x, false) def == (x: Boolean): Boolean = ifThenElse(x, x.unary_!) def !& (x: Boolean): Boolean = ifThenElse(x.unary_!, x) ... } object true extends Boolean { def ifThenElse[T](t: => T, e: => T) = t } object false extends Boolean { def ifThenElse[T](t: => T, e: => T) = e }","title":"Pure Booleans"},{"location":"scala/fpp-in-scala/#lecture-42-functions-as-objects","text":"function values\ub294 object\ub85c \ucde8\uae09\ub41c\ub2e4. function type A => B\ub294 class scala.Function1[A,B]\uc758 \ucd95\uc57d\uc774\ub2e4. package scala trait Function1[A, B] { def apply(x: A): B } \uadf8\ub7ec\ubbc0\ub85c functions\ub294 objects\uc774\ub2e4 apply methods\ub97c \uac16\ub294. (x: Int) => x * x \ub294 \uc544\ub798\uc640 \uac19\uc774 \ud655\uc7a5\ub41c\ub2e4. { class AnonFun extends Function1[Int, Int] { def apply(x: Int) = x * x } new AnnonFun } \ub610\ub294 new Function1[Int, Int] { def apply(x: Int) = x * x }","title":"Lecture 4.2 - Functions as Objects"},{"location":"scala/fpp-in-scala/#expansion-of-function-calls","text":"f(a, b)\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud655\uc7a5\ub41c\ub2e4. f.apply(a, b) val f = (x: Int) => x * x f(7) \uc740 \uc544\ub798\uc640 \uac19\uc774 \ud655\uc7a5\ub41c\ub2e4. val f = new Function1[Int, Int] { def apply(x: Int) => x * x } f.apply(7)","title":"Expansion of Function Calls"},{"location":"scala/fpp-in-scala/#functions-and-methods","text":"\uc544\ub798\uc640 \uac19\uc740 \uba54\uc18c\ub4dc\ub294 \uadf8 \uc790\uccb4\ub85c\ub294 function value\uac00 \uc544\ub2c8\ub2e4. def f(x: Int): Boolean = ... \ud558\uc9c0\ub9cc f\ub294 Function type\uc774 \uae30\ub300\ub418\ub294 \uacf3\uc5d0\uc11c \uc0ac\uc6a9\ub418\uae30 \ub54c\ubb38\uc5d0 \uc790\ub3d9\uc801\uc73c\ub85c function value\ub85c \ubcc0\ud658\ub41c\ub2e4. (x: Int) => f(x)","title":"Functions and Methods"},{"location":"scala/fpp-in-scala/#lecture-43-subtyping-and-generics","text":"Type Bounds \ub9cc\uc57d assertAllPos \ub77c\ub294 method\uac00 \uc788\ub2e4\uace0 \ud558\uc790. IntSet\uc744 \ubc1b\uace0 \ub9cc\uc57d \ubaa8\ub4e0 \uc5d8\ub9ac\uba3c\ud2b8\uac00 positive\ub77c\uba74 IntSet\uc744 \ub9ac\ud134\ud55c\ub2e4. \uadf8 \uc678\uc5d0\ub294 exception\uc744 \ub358\uc9c4\ub2e4. def assertAllPos(s: IntSet): IntSet \uc774 \ucf54\ub4dc\ub294 \ubaa8\ub4e0 \uacbd\uc6b0\ub97c \ud3ec\ud568\ud558\uc9c0 \uc54a\ub294\ub2e4. nonEmpty\ub97c \ud30c\ub77c\ubbf8\ud130\ub85c \uc900\ub2e4\uba74 NonEmpty \ud639\uc740 Error\uac00 \ubc18\ud658\ub418\uace0 Empty\ub97c \ud30c\ub77c\ubbf8\ud130\ub85c \ubcf4\ub0b4\uba74 Empty\uac00 \ubc18\ud658\ub418\uae30 \ub584\ubb38\uc774\ub2e4. \uc815\ud655\ud55c IntSet\uc774 \uc544\ub2cc \uc0c1\uc18d Empty\uc640 NonEmpty\ub294 \ud558\uc704 \ud074\ub798\uc2a4\uc774\ub2e4. def assertAllPos[S <: IntSet](r: S): S \uc5ec\uae30\uc5d0\uc11c <: IntSet \uc740 type parameter S\uc758 upper bound \uc774\ub2e4. S <: T means: S\ub294 T\uc758 subtype\uc774\ub2e4 S >: T means: S\ub294 T\uc758 supertype\uc774\ub2e4. \ub610\ub294 T\ub294 S\uc758 subtype\uc774\ub2e4. [S >: NonEmpty] \uc77c\ub54c S\ub294 NonEmpty, IntSet, AnyRef, Any\uac00 \ub420 \uc218 \uc788\ub2e4. \uc774 \ub450\uac1c\ub97c \uc11e\ub294 \uac83\ub3c4 \uac00\ub2a5\ud558\ub2e4 [S >: NonEmpty <: IntSet]","title":"Lecture 4.3 - Subtyping and Generics"},{"location":"scala/fpp-in-scala/#covariance","text":"subtyping\uacfc type parameter\ub97c \uc0ac\uc6a9\ud560 \ub54c \uace0\ub824\ud574\uc57c\ud560 \uc0ac\ud56d\uc774 \uc788\ub2e4. NonEmpty <: IntSet \uc77c\ub54c List[NonEmpty] <: List[IntSet] \uc778\uac00? \uc6b0\ub9ac\ub294 \uc774\ub7f0 \uac83\uc744 \uacf5\ubcc0(covariant)\ub77c\uace0 \ubd80\ub978\ub2e4. Java\uc5d0\uc120 \ub418\uc9c0\ub9cc \uc2a4\uce7c\ub77c\uc5d0\uc120 \uc548\ub41c\ub2e4. \uc544\ub798\uc640 \uac19\uc740 Java Code\ub97c \ubcf4\uc790 NonEmpty[] a = new NonEmpty[]{new NonEmpty(1, Empty, Empty)} IntSet[] b = a b[0] = Empty NonEmpty s = a[0] 3\ubc88 \uc9f8 \ub77c\uc778\uc5d0\uc11c runtime error(ArrayStoreException)\uac00 \ubc1c\uc0dd\ud55c\ub2e4. NonEmpty\uc5d0 Empty\ub97c \ub123\uc73c\ub824\uace0 \uc2dc\ub3c4\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 Scala\uc5d0\uc120 List Convariance\uac00 \uc548\ub41c\ub2e4. val a: Array[NonEmpty] = Array(new NonEmpty(1, Empty, Empty)) val b: Array[IntSet] = a b(0) = Empty val s: NonEmpty = a(0) \uc704\uc5d0\uc11c 2\ubc88\uca30 \ub77c\uc778\uc5d0\uc11c\uac00 error\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Scala\uc5d0\uc120 List Convariance\uac00 \uc548\ub418\uae30 \ub54c\ubb38\uc774\ub2e4.","title":"Covariance"},{"location":"scala/fpp-in-scala/#lecture-45-decomposition","text":"","title":"Lecture 4.5 - Decomposition"},{"location":"scala/fpp-in-scala/#decomposition","text":"\uc608\ub97c\ub4e4\uc5b4 \uc218\ud559 \uc5f0\uc0b0\uc744 \uc704\ud55c \uc791\uc740 \uc778\ud130\ud504\ub9ac\ud130\ub97c \ub9cc\ub4e0\ub2e4\uace0 \ud558\uc790. \uac04\ub2e8\ud558\uac8c \ud558\uae30 \uc704\ud574 numbers\uc640 additions\ub9cc \ud55c\ub2e4. Expressions\ub294 class hierarchy\ub85c \ud45c\ud604\ub420 \uc218 \uc788\uace0 \uae30\ubcf8 trait\uc778 Expr\uacfc two subclasses\uc778 Number\uc640 Sum\uc73c\ub85c \uad6c\uc131\ud560 \uc218 \uc788\ub2e4. trait Expr { def isNumber: Boolean def isSum: Boolean def numValue: Int def leftOp: Expr def rightOp: Expr } class Number(n: Int) extends Expr { def isNumber: Boolean = true def isSum: Boolean = false def numValue: Int = n def leftOp: Expr = throw new Error(\"Number.leftOp\") def rightOp: Expr = throw new Error(\"Number.rightOp\") } class Sum(e1: Expr, e2: Expr) extends Expr { def isNumber: Boolean = false def isSum: Boolean = true def numValue: Int = throw new Error(\"Sum.numValue\") def leftOp: Expr = e1 def rightOp: Expr = e2 } def eval(e: Expr): Int = { if (e.isNumber) e.numValue else if (e. isSum) eval(e.leftOp) + eval(e.rightOp) else throw new Error(\"Unknown expression\" + e) } \ub9cc\uc57d Expr\uc5d0 \uc0c8\ub85c\uc6b4 \uba54\uc11c\ub4dc\ub97c \ucd94\uac00\ud55c\ub2e4\uace0 \uac00\uc815\ud558\uba74 \uc774\ub97c \ud655\uc7a5\ud558\ub294 class\ub4e4\uc5d0\ub3c4 \ubaa8\ub450 \uad6c\ud604\uc744 \ucd94\uac00\ud574\uc57c \ud55c\ub2e4. \uc0c8\ub85c\uc6b4 \ud074\ub798\uc2a4\ub97c \ucd94\uac00\ud560\uc218\ub85d \uacc4\uc18d\ud574\uc11c \ub9ce\uc740 \uba54\uc11c\ub4dc\ub97c \ucd94\uac00\ud574\uc57c \ud55c\ub2e4.","title":"Decomposition"},{"location":"scala/fpp-in-scala/#non-solution-type-tests-and-type-casts","text":"def isInstanceOf[T]: Boolean // check whether this object's type conforms to 'T' def asInstanceOf[T]: T // treat this object as an instance of type 'T' // throws 'ClassCastException' if it isn't def eval(e: Expr): Int = if (e.isInstanceOf[Number]) e.asInstanceOf[Number].numValue else if (e.isInstanceOf[Sum]) eval(e.asInstanceOf[Sum].leftOp) + eval(e.asInstanceOf[Sum].rightOp) else throw new Error(\"Unknown expression \" + e) no need for classification methods, access \uba54\uc18c\ub4dc\ub97c \ud544\uc694\ud560 \ub54c\ub9cc \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4. low-level, potentially unsafe\ud558\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. type cast\ub97c \uc218\ud589\ud588\uc744 \ub54c \uc624\ub958\uac00 \ubc1c\uc0dd\ud558\ub294 \uac83\uc744 runtime\uc5d0\uc11c \uc54c \uc218 \uc788\ub2e4.","title":"Non-Solution: Type Tests and Type Casts"},{"location":"scala/fpp-in-scala/#solution1-object-oriented-decomposition","text":"trait Expr { def eval: Int } class Number(n: Int) extends Expr { def eval: Int = n } class Sum(e1: Expr, e2: Expr) extends Expr { def eval: Int = e1.eval + e2.eval \ub9cc\uc57d \uc0c8\ub85c\uc6b4 \uba54\uc18c\ub4dc\uac00 \ucd94\uac00\ub41c\ub2e4\uba74 \ubaa8\ub4e0 \ud074\ub798\uc2a4 hierarchy\uc5d0 \uc788\ub294 \ud074\ub798\uc2a4\uc5d0 \uba54\uc18c\ub4dc\ub97c \ucd94\uac00\ud574\uc57c \ud55c\ub2e4.","title":"Solution1: Object-Oriented Decomposition"},{"location":"scala/fpp-in-scala/#lecture-46-pattern-matching","text":"good fit for the problem of decomposition","title":"Lecture 4.6 - Pattern Matching"},{"location":"scala/fpp-in-scala/#case-classes","text":"case classes \uc758 \uc815\uc758\ub294 \uc77c\ubc18 class \uc815\uc758\uc640 \ube44\uc2b7\ud558\ub2e4. \uc55e\uc5d0 case\ub97c \ubd99\uc774\ub294 \uac83 \ube7c\uace0 trait Expr case class Number(n: Int) extends Expr case class Sum(e1: Expr, e2: Expr) extends Expr \uc704\uc640 \uac19\uc774 \ud558\uba74 \uc790\ub3d9\uc801\uc73c\ub85c object\uc640 apply methods\uac00 \uc0dd\uc131\ub41c\ub2e4. object Number { def apply(n: Int) = new Number(n) } object Sum { def apply(e1: Expr, e2: Expr) = new Sum(e1, e2) } object\ub85c \ub9cc\ub4e4\uc5b4\uc84c\uae30 \ub54c\ubb38\uc5d0 new Number(1) \ub300\uc2e0 Number(1)\ub85c\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4.","title":"Case Classes"},{"location":"scala/fpp-in-scala/#pattern-matching","text":"\ud074\ub798\uc2a4 \uad6c\uc870\uc5d0\uc11c C\uc640 Java\uc758 switch \uc758 generalization\uc774\ub2e4. def eval(e: Expr): Int = e match { case Number(n) => n case Sum(e1, e2) => eval(e1) + eval(e2) } \ub9cc\uc57d \uc77c\uce58\ud558\ub294 \ud328\ud134\uc774 \uc5c6\ub2e4\uba74 MatchError\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Pattern\uc774 \ub420 \uc218 \uc788\ub294 \uac83\uc740 \uc544\ub798\uc640 \uac19\ub2e4. - constructors, eg. Number, Sum - variables, e.g. n, e1, e2 - wildcard patterns _, - constants, e.g. 1, true. \ucc38\uace0\ub85c \uc544\ub798\uc640 \uac19\uc740 \uc81c\uc57d \uc0ac\ud56d\uc774 \uc788\ub2e4. variables\ub294 \ubc18\ub4dc\uc2dc \uc18c\ubb38\uc790\ub85c \uc2dc\uc791\ud574\uc57c \ud558\uba70 pattern\uc548\uc5d0 \ubcc0\uc218\uba85\uc740 \ud55c \ubc88\ub9cc \uc4f0\uc5ec\uc57c \ud55c\ub2e4. \uc608\ub85c Sum(x, x)\ub294 \uc548\ub41c\ub2e4. constants\uc758 names\uc740 \ub300\ubb38\uc790\ub85c \uc2dc\uc791\ud574\uc57c \ud55c\ub2e4.","title":"Pattern Matching"},{"location":"scala/fpp-in-scala/#lecture-47-lists","text":"val fruit = List(\"apples\", \"oranges\", \"pears\") val nums = List(1,2,3,4) val diag3 = List(List(1, 0, 0)) val empty = List() List\uc640 Array\ub294 \uc911\uc694\ud55c \ucc28\uc774\uac00 \uc788\ub2e4. - List\ub294 immutable \ud558\ub2e4. -> element of a list cannot be changed - List\ub294 recursive \ud558\ub2e4. \ubc18\uba74 Array\ub294 flat\ud558\ub2e4.","title":"Lecture 4.7 - Lists"},{"location":"scala/fpp-in-scala/#the-list-type","text":"array\uac19\uc774 lists\ub294 homogeneous\ud558\ub2e4. (list\uc758 \ubaa8\ub4e0 element\uac00 \ub3d9\uc77c\ud55c \ud0c0\uc785\uc774\uc5b4\uc57c \ud55c\ub2e4) val fruit: List[String] = List(\"apples\", \"oranges\", \"pears\") val nums: List[Int] = List(1,2,3,4) val diag3: List[List[Int]] = List(List(1, 0, 0)) val empty: List[Nothing] = List()","title":"The List Type"},{"location":"scala/fpp-in-scala/#constructors-of-lists","text":"empty list Nil operation :: (pronounced cons) x :: xs \ub294 \uccab \ubc88\uc9f8 element\uac00 x\uc774\uace0 \uadf8 \ub4a4\ub85c xs\uac00 \uc624\ub294 \uc0c8\ub85c\uc6b4 list\ub97c \uc0dd\uc131\ud55c\ub2e4. fruit = \"apples\" :: (\"oranged\" :: (\"pears\" :: Nil)) nums = 1 :: (2 :: (3 :: (4 :: Nil))) empty = Nil A :: B :: C\ub294 A :: (B :: C)\ub85c \uc778\ud130\ud504\ub9ac\ud2b8 \ub41c\ub2e4. val nums = 1 :: 2 :: 3 :: 4 :: Nil // \uc704 \ud45c\ud604\uacfc \uac19\uc740 \uc758\ubbf8\ub85c \ub2e4\ub974\uac8c \uc4f8 \uc218 \uc788\ub2e4. Nil.::(4).::(3).::(2).::(1)","title":"Constructors of Lists"},{"location":"scala/fpp-in-scala/#operations-on-lists","text":"head tail isEmpty","title":"Operations on Lists"},{"location":"scala/fpp-in-scala/#list-patterns","text":"Nil -> The Nil constant p :: ps -> head\uac00 p\uc774\uace0 \ub098\uba38\uc9c0\uac00 ps\uc5d0 \ub9e4\uce6d List(p1, ... , pn) -> same as p1 :: ... :: pn :: Nil examples 1 :: 2 :: xs 1\uacfc 2\ub85c \uc2dc\uc791\ud558\ub294 list x :: Nil length\uac00 1\uc778 List List (x) x :: Nil\uacfc \ub3d9\uc77c List() empty list, Nil\uacfc \ub3d9\uc77c List(2 :: xs) 2\ub85c \uc2dc\uc791\ud558\ub294 \ub9ac\uc2a4\ud2b8\ub97c \ud3ec\ud568\ud558\ub294 \ub9ac\uc2a4\ud2b8","title":"List Patterns"},{"location":"scala/fpp-in-scala/#sorting-lists","text":"Insertion Sort List(7,3,9,2)\uac00 \uc788\uc744 \ub54c tail List\uc778 List(3,9,2)\ub97c sort\ud574\uc11c (2, 3, 9)\ub97c \uc5bb\uace0 7\uc744 \uc801\uc808\ud55c \uc704\uce58\uc5d0 \ubc30\uce58\ud574 (2, 3, 7, 9)\ub97c \ub9cc\ub4e0\ub2e4. def isort(xs: List[Int]): List[Int] = xs match { case List() => List() case y :: ys => insert(y, isort(ys)) }","title":"Sorting Lists"},{"location":"scala/fpp-in-scala/#week5","text":"","title":"Week5"},{"location":"scala/fpp-in-scala/#lecture-51-more-functions-on-lists","text":"","title":"Lecture 5.1 - More Functions on Lists"},{"location":"scala/fpp-in-scala/#list-methods1","text":"xs.length xs.last -> xs\uac00 empty\ub77c\uba74 Error xs.init -> last\ub97c \uc81c\uc678\ud55c \ub098\uba38\uc9c0 \ub9ac\uc2a4\ud2b8. xs\uac00 empty\ub77c\uba74 Error xs take n -> xs \ub9ac\uc2a4\ud2b8 \ub0b4 \ub9e8 \uc55e\ubd80\ud130 n\uac1c\ub97c \ucde8\ud55c \ub9ac\uc2a4\ud2b8. \ub610\ub294 n\uc774 xs \uae38\uc774 \ubcf4\ub2e4 \ud06c\ub2e4\uba74 xs \uc790\uccb4\ub97c \ub9ac\ud134 xs drop n -> xs \ub9ac\uc2a4\ud2b8 \ub0b4 \ub9e8 \uc55e\ubd80\ud130 n \uac1c\ub97c \ubc84\ub9b0 \ub4a4\uc758 \ub9ac\uc2a4\ud2b8 xs(n)","title":"List Methods(1)"},{"location":"scala/fpp-in-scala/#list-methods1_1","text":"Creating new Lists: xs ++ ys -> xs \ub9ac\uc2a4\ud2b8 \ub4a4\uc5d0 ys\ub97c \ud569\uce5c \ub9ac\uc2a4\ud2b8 xs.reverse xs updated (n, x) -> n\ubc88\uc9f8 index\uc5d0\ub294 x\ub97c \ub123\uace0 \ub098\uba38\uc9c0\ub294 xs\uc778 \ub9ac\uc2a4\ud2b8 Finding elements: xs indxOf x -> xs \ub9ac\uc2a4\ud2b8\ub0b4 x\uc640 \uac19\uc740 \uccab \ubc88\uc9f8 element\uc758 index. \uc5c6\ub2e4\uba74 -1 \ub9ac\ud134 xs contains x -> same as xs indexOf x >= 0","title":"List Methods(1)"},{"location":"scala/fpp-in-scala/#implementations","text":"def last[T](xs: List[T]): T = xs match { case List() => throw new Error(\"last of empty list\") case List(x) => x case y :: ys => last(ys) } def init[T](xs: List[T]): List[T] = xs match { case List() => throw new Error(\"init of empty list\") case List(x) => List() case y :: ys => y :: init(ys) } def concat[T](xs: List[T], ys: List[T]): = xs match { case List() => ys case z :: zs => z :: concat(zs, ys) } def reverse[T](xs: List[T]): List[T] = xs match { case List() => List() case y :: ys => reverse(ys) ++ List(y) // ++ \ub294 concatenation\uc774\ub2e4. } \uc704\uc758 reverse\ud568\uc218\ub294 concat\ud560\ub54c n, reverse\ud560 \ub54c n n * n \uc758 \ubcf5\uc7a1\ub3c4\ub97c \uac16\ub294\ub2e4. def removeAt(n: Int, xs: List[Int]) = (xs take n) ::: (xs drop n + 1)","title":"Implementations"},{"location":"scala/fpp-in-scala/#lecture-52-pairs-and-tuples","text":"insertion sort\ubcf4\ub2e4 \uc88b\uc740 \uc54c\uace0\ub9ac\uc998\uc778 merge sort\ub97c \uad6c\ud604\ud574\ubcf4\uc790. \ub9cc\uc57d \ub9ac\uc2a4\ud2b8\uac00 \ube44\uc5b4\uc788\uac70\ub098 \ud55c \uac1c\uc758 \uc5d8\ub9ac\uba3c\ud2b8\uac00 \uc788\ub2e4\uba74 \uc774\ubbf8 \uc18c\ud305\ub41c \uac83\uc774\ub2e4. \uadf8\ub807\uc9c0 \uc54a\ub2e4\uba74 - \ub9ac\uc2a4\ud2b8\ub97c \uc808\ubc18\uc73c\ub85c \ub098\ub204\uc5b4 \ub450 \uac1c\uc758 \uc11c\ube0c \ub9ac\uc2a4\ud2b8\ub97c \ub9cc\ub4e0\ub2e4. - \uc11c\ube0c \ub9ac\uc2a4\ud2b8\ub97c sort \ud55c\ub2e4 - \ub450 \uac1c\uc758 sort\ub41c sub lists\ub97c \ud569\uce5c\ub2e4.","title":"Lecture 5.2 - Pairs and Tuples"},{"location":"scala/fpp-in-scala/#splitat-function","text":"returns two sublists \uc8fc\uc5b4\uc9c4 index\ub85c \ubd80\ud130\uc758 \uc55e \uc694\uc18c\ub4e4 \uc8fc\uc5b4\uc9c4 index\ub85c \ubd80\ud130\uc758 \ub4a4 \uc694\uc18c\ub4e4 lists\uac00 pair \ub85c \ub9ac\ud134\ub41c\ub2e4.","title":"SplitAt Function"},{"location":"scala/fpp-in-scala/#detour-pair-and-tuples","text":"x,y pair\ub294 (x, y) \ub85c \ud45c\ud604\ud55c\ub2e4. val pair = (\"answer\", 42) val (label, value) = pair","title":"Detour: Pair and Tuples"},{"location":"scala/fpp-in-scala/#the-tuple-class","text":"\ubaa8\ub4e0 Tuple n \ud074\ub798\uc2a4\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ud328\ud134\uc73c\ub85c \ubaa8\ub378 \ub41c\ub2e4. case class Tuple2[T1, T2](_1: +T1, _2: +T2) { override def toString = \"(\" + _1 + \",\" + _2 + \")\" } \uc544\ub798\ub294 pattern binding val (label, value) = pair \ub300\uc2e0 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. val label = pair._1 val value = pair._2 \ud558\uc9c0\ub9cc \ud328\ud134 \ub9e4\uce6d \ud3fc\uc774 \uc77c\ubc18\uc801\uc73c\ub85c \uc120\ud638\ub41c\ub2e4. def msort(xs: List[Int]): List[Int] = { val n = xs.length / 2 if (n == 0) xs else { def merge(xs: List[Int], ys: List[Int]): List[Int] = (xs, ys) match { case (Nil, ys) => ys case (xs, Nil) => xs case (x :: xs1, y :: ys1) => if (x < y) x :: merge (xs1, ys) else y :: merge(xs, ys1) } val (fst, snd) = xs splitAt n merge(msort(fst), msort(snd)) } } val nums = List(2, -4, 5, 7, 1) msort(nums)","title":"The Tuple class"},{"location":"scala/fpp-in-scala/#lecture-53-implicit-parameters","text":"","title":"Lecture 5.3 - Implicit Parameters"},{"location":"scala/fpp-in-scala/#making-sort-more-general","text":"msort\ub97c \ubaa8\ub4e0 \ud0c0\uc785\uc5d0 \ub300\ud574\uc11c \uad6c\ud604\ud574\ubcf4\uc790 def msort[T](xs: List[T]): List[T] = ??? \uc774\uc804\uc5d0 \uad6c\ud604\ud588\ub358 \ucf54\ub4dc\ub97c \uadf8\ub300\ub85c \ubc14\uafbc\ub2e4\uace0 \ud558\uba74 \ub3d9\uc791\ud558\uc9c0 \uc54a\ub294\ub2e4. < \uac00 \ubaa8\ub4e0 \ud0c0\uc785 T\uc5d0\uc11c \uc815\uc758\ub418\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc774\ub2e4. => \ube44\uad50 \ud568\uc218\uac00 \ud544\uc694\ud558\ub2e4.","title":"Making Sort more General"},{"location":"scala/fpp-in-scala/#parametrization-with-ordered","text":"Ordering\uc744 \uc704\ud55c standard library\uac00 \uc788\ub2e4. scala.math.Ordering[T] \uc774\ub97c \uc774\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uac00\ub2a5\ud558\ub2e4. import math.Ordering def msort[T](xs: List[T])(ord: Ordering[T]): List[T] = { val n = xs.length / 2 if (n == 0) xs else { def merge(xs: List[T], ys: List[T]): List[T] = (xs, ys) match { case (Nil, ys) => ys case (xs, Nil) => xs case (x :: xs1, y :: ys1) => if (ord.lt(x, y)) x :: merge (xs1, ys) else y :: merge(xs, ys1) } val (fst, snd) = xs splitAt n merge(msort(fst)(ord), msort(snd)(ord)) } } val nums = List(2, -4, 5, 7, 1) // x\uc640 y\uc5d0 \ud0c0\uc785\uc744 \uc9c0\uc815 \uc548\ud574\ub3c4 scala compiler\uac00 Int\ub97c \uc778\uc9c0 \ud55c\ub2e4. msort(nums)(Ordering.Int) val fruits = List(\"apple\", \"pineapple\", \"orange\", \"banana\") // compareTo\ub294 JAVA String \uba54\uc11c\ub4dc\uc774\ub2e4. // first string \uc774 second string \ubcf4\ub2e4 \uc791\uc73c\uba74 -1 \uac19\uc73c\uba74 0 \ud06c\uba74 1\uc744 \ub9ac\ud134\ud55c\ub2e4. msort(fruits)(Ordering.String)","title":"Parametrization with Ordered"},{"location":"scala/fpp-in-scala/#aside-implicit-parameters","text":"passing lt or ord values\ub294 \uac70\ucd94\uc7a5\uc2a4\ub7fd\ub2e4. implicit paramter\ub85c \uc774\ub97c \ud53c\ud560 \uc218 \uc788\ub2e4. import math.Ordering def msort[T](xs: List[T])(implicit ord: Ordering[T]): List[T] = { val n = xs.length / 2 if (n == 0) xs else { def merge(xs: List[T], ys: List[T]): List[T] = (xs, ys) match { case (Nil, ys) => ys case (xs, Nil) => xs case (x :: xs1, y :: ys1) => if (ord.lt(x, y)) x :: merge (xs1, ys) else y :: merge(xs, ys1) } val (fst, snd) = xs splitAt n merge(msort(fst), msort(snd)) } } val nums = List(2, -4, 5, 7, 1) // x\uc640 y\uc5d0 \ud0c0\uc785\uc744 \uc9c0\uc815 \uc548\ud574\ub3c4 scala compiler\uac00 Int\ub97c \uc778\uc9c0 \ud55c\ub2e4. msort(nums) val fruits = List(\"apple\", \"pineapple\", \"orange\", \"banana\") // compareTo\ub294 JAVA String \uba54\uc11c\ub4dc\uc774\ub2e4. // first string \uc774 second string \ubcf4\ub2e4 \uc791\uc73c\uba74 -1 \uac19\uc73c\uba74 0 \ud06c\uba74 1\uc744 \ub9ac\ud134\ud55c\ub2e4. msort(fruits) \uc704\uc640 \uac19\uc774 implicit\uc744 \uc120\uc5b8\ud574\uc8fc\uba74 \ud0c0\uc785\uc744 \uc548\ub118\uaca8\uc918\ub3c4 \ucef4\ud30c\uc77c\ub7ec\uac00 \ud574\uacb0\ud574\uc900\ub2e4.","title":"Aside: Implicit Parameters"},{"location":"scala/fpp-in-scala/#rules-for-implicit-parameters","text":"function\uc774 type T\uc5d0 \ub300\ud55c implicit arguments\ub97c \ubc1b\ub294 \ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 implicit definition\uc744 \ucc3e\ub294\ub2e4. - is marked implicit - has a type compatible with T - is visible at the point of the function call, or is defined in a companion object associated with T \ub9cc\uc57d \uc774 \uc911\uc5d0 \uac00\uc7a5 \uad6c\uccb4\uc801\uc778 \ud558\ub098\uc758 \uc815\uc758\uac00 \uc788\ub2e4\uba74 \uc774\ub97c \uc801\uc6a9\ud55c\ub2e4. \uadf8 \uc678\uc758 \ubaa8\ub4e0 \uacbd\uc6b0\ub294 \uc5d0\ub7ec\ub97c \ubc1c\uc0dd\uc2dc\ud0a8\ub2e4.","title":"Rules for Implicit Parameters"},{"location":"scala/fpp-in-scala/#lecture-54-higher-order-list-functions","text":"lists\uc758 \ubaa8\ub4e0 \ud568\uc218\ub4e4\uc740 first order\uc774\ub2e4. \uadf8 \ub9d0\uc740 functions\ub4e4\uc774 primitive type\uc758 lists\ub97c arguments\ub85c \ubc1b\uace0 \uadf8\ub4e4\uc744 \uacb0\uacfc\ub85c\uc11c \ub9ac\ud134\ud55c\ub2e4\ub294 \uac83\uc774\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 \ub2e4\ub978 \ud568\uc218\ub97c arguments\ub85c \ubc1b\ub294 higher-order list functions\ub97c \uad6c\ud604\ud574\ubcf8\ub2e4.","title":"Lecture 5.4 - Higher-Order List Functions"},{"location":"scala/fpp-in-scala/#map","text":"map\uc758 \uc2e4\uc81c \uad6c\ud604\uc740 \uc544\ub798 \ubcf4\ub2e4 \ubcf5\uc7a1\ud558\ub2e4. \uc2e4\uc81c \ucf54\ub4dc\ub294 tail-recusrive \ud558\uace0, \ubaa8\ub4e0 collection\uc5d0 \ub300\ud574 \uc801\uc6a9 \uac00\ub2a5\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. abstract class List[T] { def map[U](f: T => U): List[U] = this match { case Nil => this case x :: xs => f(x) :: xs.map(f) } } \uc774\ub97c \uc774\uc6a9\ud55c \ud568\uc218\ub294 \uc544\ub798\uc640 \uac19\ub2e4. def scalaList(xs: List[Double], factor: Double) = xs map (x => x * factor)","title":"Map"},{"location":"scala/fpp-in-scala/#filtering","text":"\ub610 \ub2e4\ub978 common operation on lists\uc5d0\ub294 \uc8fc\uc5b4\uc9c4 \uc870\uac74\uc5d0 \ub9de\ub294 elements\ub9cc select\ud558\ub294 filtering\uc774 \uc788\ub2e4. def filter(p: T => Boolean): List[T] = this match { case Nil => this case x :: xs => if(p(x)) x :: xs.filter(p) else xs.filter(p) } // filter \uc0ac\uc6a9 \uc548 \ud55c \ubc84\uc804 def posElems(xs: List[Int]): List[Int] = xs match { case Nil => xs case y :: ys => if (y > 0) y :: posElems(ys) else posElems(ys) } // filter \uc0ac\uc6a9 \ubc84\uc804 def posElems(xs: List[Int]): List[Int] = xs filter (x => x > 0)","title":"Filtering"},{"location":"scala/fpp-in-scala/#variations-of-filter","text":"xs filterNot p // same as filter(x => !p(x)) xs partition p // same as (xs filter p, xs filterNot p) xs takeWhile p // p\ub97c \ub9cc\uc871\ud558\ub294 \ubaa8\ub4e0 elements\ub4e4\uc758 longest prefix of list xs xs dropWhile p // \uccab \uc694\uc18c \ubd80\ud130 p\ub97c \ub9cc\uc871\ud558\ub294 elements\ub97c \ubaa8\ub450 \uc81c\uc678\ud558\uace0 \ubd88\ub9cc\uc871 \ub41c element \ubd80\ud130 \ub9c8\uc9c0\ub9c9\uae4c\uc9c0\uc758 list xs span p // same as (xs takeWhile p, xs dropWhile p) val nums = List(2, -4, 5, 7, 1) val fruits = List(\"apple\", \"pineapple\", \"orange\", \"banana\") nums filter (x => x > 0) nums filterNot (x => x > 0) nums partition (x => x > 0) nums takeWhile (x => x > 0) // 2 nums dropWhile (x => x > 0) // -4, 5, 7, 1 nums span (x => x > 0) // (List(2), List(-4, 5, 7, 1)) val data = List(\"a\", \"a\", \"a\" , \"b\", \"c\" , \"c\", \"a\") def pack[T](xs: List[T]): List[List[T]] = xs match { case Nil => Nil case x :: xs1 => val (first, rest) = xs span(y => y == x) first :: pack(rest) } pack(data) // List(List(a,a,a), List(b)... def encode[T](xs: List[T]): List[(T, Int)] = pack(xs) map (ys => (ys.head, ys.length)) encode(data) // List((a, 3), (b, 1), (c, 2), (a, 1))","title":"Variations of Filter"},{"location":"scala/fpp-in-scala/#lecture-55-reduction-of-lists","text":"\uc774\ubc88\uc5d0\ub294 \uc800\ubc88\uc5d0 \uc774\uc5b4 hof\ub97c \ubc30\uc6b0\ub294\ub370 fold or reduce combiators\ub77c \ubd88\ub9ac\ub294 \ud568\uc218\ub4e4\uc744 \ubc30\uc6b8 \uac83\uc774\ub2e4.","title":"Lecture 5.5 - Reduction of Lists"},{"location":"scala/fpp-in-scala/#reduceleft","text":"def sum(xs: List[Int]): Int = xs match { case Nil => 0 case y :: ys => y + sum(ys) } \uc704 \uc640 \uac19\uc740 \uba54\uc11c\ub4dc\ub294 redceLeft\ub85c abstract \uac00\ub2a5\ud558\ub2e4. reduceLeft\ub294 given binary operator\ub97c list\uc758 adjacent elements \uc0ac\uc774\uc5d0 insert\ud558\ub294 \uac83\uc774\ub2e4. List(x1, ..., xn) reduceLeft op = (... (x1 op x2) op ... ) op xn reduceLeft\ub97c \uc0ac\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uac00\ub2a5\ud558\ub2e4 def sum(xs: List[Int]) = (0 :: xs) reduceLeft ((x, y) => x + y) def product(xs: List[Int]) = (1 :: xs) reduceLeft ((x, y) => x * y)","title":"ReduceLeft"},{"location":"scala/fpp-in-scala/#a-shorter-way-to-write-functions","text":"((x, y) => x * y) \ub300\uc2e0 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. (_ * _) \ubaa8\ub4e0 _ \ub294 new parameter\ub97c \ub098\ud0c0\ub0b8\ub2e4. \uc67c\ucabd\ubd80\ud130 \uc624\ub978\ucabd\uc73c\ub85c \uc774 parameter\ub294 next outer pair of parentheses\uae4c\uc9c0 \uc720\ud6a8\ud558\ub2e4. def sum(xs: List[Int]) = (0 :: xs) reduceLeft (_ + _) def product(xs: List[Int]) = (1 :: xs) reduceLeft (_ * _)","title":"A Shorter Way To Write Functions"},{"location":"scala/fpp-in-scala/#foldleft","text":"reduceLeft\uc758 \ub354 general \ud55c function\uc774 \uc788\ub294\ub370 \uc774\ub97c foldLeft\ub77c\uace0 \ud55c\ub2e4. reduceLeft\uc640 \ube44\uc2b7\ud558\uc9c0\ub9cc additional parameter\ub85c\uc11c accumulator z \ub97c \ubc1b\ub294\ub2e4. empty list\uc77c \uacbd\uc6b0 z\ub97c \ub9ac\ud134\ud55c\ub2e4. def sum(xs: List[Int]) = (xs foldLeft 0) (_ + _) def product(xs: List[Int]) = (xs foldLeft 1) (_ * _)","title":"FoldLeft"},{"location":"scala/fpp-in-scala/#implementations-of-reduceleft-and-foldleft","text":"abstract class List[T] { ... def reduceLeft(op: (T, T) => T): T = this match { case Nil => throw new Error(\"Nil.reduceLeft\") case x :: xs => (xs foldLeft x)(op) } def foldLeft[U](z: U)(op: (U, T) => U): U = this match { case Nil => z case x :: xs => (xs foldLeft op(z, x))(op) } }","title":"Implementations of ReduceLeft and FoldLeft"},{"location":"scala/fpp-in-scala/#foldright-and-reduceright","text":"foldLeft\uc640 reduceLeft\ub294 unfold on tress that lean to the left \ub97c \ud55c\ub2e4. \ubc18\ub300\ub85c FoldRight\uc640 ReduceRight\ub294 produce trees which lean to the right \ud55c\ub2e4. List(x1, ..., x{n-1}, xn) reduceRight op = x1 op ( ... (x{n-1} op xn) ... ) (List(x1, ..., xn) foldRight acc)(op) = x1 op ( ... (xn op acc) ... ) abstract class List[T] { ... def reduceRight(op: (T, T) => T): T = this match { case Nil => throw new Error(\"Nil.reduceLeft\") case x :: xs => op(x, xs.reduceRight(op)) } def foldRight[U](z: U)(op: (U, T) => U): U = this match { case Nil => z case x :: xs => op(x, (xs foldRight z)(op)) } }","title":"FoldRight and ReduceRight"},{"location":"scala/fpp-in-scala/#difference-between-foldleft-and-foldright","text":"produce different tree Left leaning trees, Right leaning trees \ub458 \uc774 \ub3d9\uc77c\ud55c \uacb0\uacfc\ub97c \ub9cc\ub4e4\uc5b4 \ub0bc \uc218\ub3c4 \uc788\uc9c0\ub9cc(\ud6a8\uc728\uc131\uc5d0\uc120 \ucc28\uc774\uac00 \uc788\uc744 \uc218 \uc788\ub2e4) \ub54c\ub54c\ub85c\ub294 \ud55c \uac00\uc9c0\ub9cc \uc801\uc6a9 \uac00\ub2a5\ud558\ub2e4. def concat[T](xs: List[T], ys: List[T]): List[T] = (xs foldRight ys) (_ :: _) \uc704\uc758 \ud568\uc218\uc5d0\uc11c foldLeft\ub294 \uc548\ub41c\ub2e4. \uc65c\ub0d0?! :: \uc5f0\uc0b0\uc790\uc5d0 paramter \ud0c0\uc785 T\uac00 \uc62c \uc218 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4. List\ub9cc \uc804\ub2ec\ud574\uc57c \ud55c\ub2e4. (\uadf8\ub9bc\uc744 \uadf8\ub824\ubcf4\uba74 foldLeft\uc758 \uc624\ub978\ucabd\uc5d4 T\uac12\ub4e4\uc774 \uc628\ub2e4.)","title":"Difference between FoldLeft and FoldRight"},{"location":"scala/fpp-in-scala/#lecture-56-reasoning-about-concat","text":"","title":"Lecture 5.6 - Reasoning About Concat"},{"location":"scala/fpp-in-scala/#lecture-57-a-larger-equational-proof-on-lists","text":"","title":"Lecture 5.7 - A Larger Equational Proof on Lists"},{"location":"scala/fpp-in-scala/#week6","text":"","title":"Week6"},{"location":"scala/fpp-in-scala/#lecture-61-other-collections","text":"\uc774\uc804\uc5d0 \ubd24\ub4ef\uc774 list\ub294 linear\uc774\ub2e4. \uccab \ubc88\uc9f8 element\uc5d0 \uc811\uadfc\ud558\ub294 \uac83\uc774 \ub9c8\uc9c0\ub9c9 element\ub97c \uc811\uadfc\ud558\ub294 \uac83\ubcf4\ub2e4 \ube60\ub974\ub2e4. \uc2a4\uce7c\ub77c\uc5d0\ub294 sequence\ub97c \uad6c\ud604\ud55c \ub610 \ub2e4\ub978 Vector \uac00 \uc788\ub2e4. \uc774\ub294 list\ubcf4\ub2e4 \ubcf4\ub2e4 \ubc38\ub7f0\uc2a4\ub41c access pattern\uc744 \uac16\ub294\ub2e4. \uc5ec\ub7ec depth\uc758 Vector\uc758 \uacf5\uc2dd\uc740 log 32 n (n\uc740 vector size\uc774\ub2e4.) \uc774 \uc758\ubbf8\ub294 vector\uc758 \ud06c\uae30\uac00 \ucee4\uc838\ub3c4 access \uc18d\ub3c4\uac00 \ub290\ub9ac\uac8c \uc99d\uac00\ud55c\ub2e4\ub294 \uac83\uc774\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 decent random access\uc5d0\uc11c list \ubcf4\ub2e4 vector\uac00 \ube60\ub974\ub2e4. \ub610\ud55c vector\ub294 bulk operation\uc5d0 \uc88b\ub2e4. - sequence\ub97c traverse\ud558\ub294 \uac83 - map \ud568\uc218 \uc801\uc6a9 - fold \ud568\uc218 \uc801\uc6a9 Vector\uac00 \ub354 \uc88b\ub2e4\uba74 list\ub294 \uc65c \uc368\uc57c \ud558\ub098? head\ub9cc \uc5bb\uc744 \uacbd\uc6b0 \ud639\uc740 tail\ub9cc list\uac00 \uc0c1\uc218 \uc2dc\uac04\uc73c\ub85c\uc11c \ub354 \uc720\ub9ac\ud558\ub2e4. \uc774\ucc98\ub7fc access pattern\uc774 recursive structures\uc77c \ub550 list\uac00 \ub354 \uc720\ub9ac\ud558\ub2e4. \ub9cc\uc57d access pattern\uc774 bulk operation\uc774\ub77c\uba74 map, fold, filter \ucc98\ub7fc, vector\uac00 \ub354 \uc720\ub9ac\ud558\ub2e4. \ub2e4\ud589\ud788\ub3c4 Vector\uc640 List\uac04\uc758 \uc804\ud658\uc740 \uc27d\ub2e4 val nums = Vector(1, 2, 3, -88) val people = Vector(\"Vector\", \"James\", \"Peter\") \ub610\ud55c list\uc640 \ub3d9\uc77c\ud55c operations\uc744 \uc9c0\uc6d0\ud55c\ub2e4. :: \uc744 \uc81c\uc678\ud558\uace0 \ub300\uc2e0 \uc544\ub798\uc640 \uac19\uc740 \uac83\ub4e4\uc774 \uc788\ub2e4. x +: xs // Create a new vector with leading element x, followed by all elements of xs xs :+ x // Create a new vector with trailing element x, preceded by all elements of xs ( : \uc740 \ud56d\uc0c1 sequence\ub97c \uac00\ub9ac\ud0a8\ub2e4.) vector\uc5d0 add\ub294 \uc5b4\ub5bb\uac8c \ud560\uae4c? \uc544\ub798 \uadf8\ub9bc\uacfc \uac19\uc774 \ub9e8 \uc544\ub798\ucabd\uc5d0 \uc704\uce58\ud55c \ube68\uac04\uc0c9\uc758 Vector\ub97c \ucd94\uac00\ud55c\ub2e4\uace0 \ud588\uc744 \ub54c \uc704\ucabd \ub808\ubca8\ub85c \uc62c\ub77c\uac00\uba74\uc11c \uc774\uc804\uacfc \ub3d9\uc77c\ud55c element\ub4e4\uc744 \uac00\ub9ac\ud0a4\uba74\uc11c \ud558\ub098\ub294 \uc0c8\ub85c \ub9cc\ub4e0 element\ub97c \uac00\ub9ac\ud0a4\ub294 \uc0c8\ub85c\uc6b4 \ubca1\ud130\ub97c \ub9cc\ub4e4\uba74\uc11c root \ub808\ubca8\uae4c\uc9c0 \uc62c\ub77c\uac04\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \ubcf5\uc7a1\ub3c4\ub294 log 32 n \uc774\ub2e4. (Object\ub97c \uc0dd\uc131\ud558\ub294\ub370 \uac78\ub9ac\ub294 \uc2dc\uac04) list\uc640 vector\uc758 base class\ub294 Seq\uc774\ub2e4. \uadf8\ub9ac\uace0 Seq\uc740 Iterable\uc758 subclass\uc774\ub2e4. Arrays\uc640 Strings\ub294 Seq\uc640 \ub3d9\uc77c\ud55c operations\ub97c \uc81c\uacf5\ud55c\ub2e4. \uadf8\ub807\uae30\uc5d0 \ud544\uc694\ud560\ub54c \uba85\uc2dc\uc801\uc73c\ub85c \ubcc0\uacbd\ud560 \uc218 \uc788\ub2e4. val xs: Array[Int] = Array(1,2,3) xs map (x => 2 * x) val ys: String = \"Hello world!\" ys filter (_.isUpper) \ub610\ub2e4\ub978 sequence type\uc73c\ub85c range \uac00 \uc788\ub2e4. sequence of evenly spaced integers\ub97c \ud45c\ud604\ud55c\ub2e4. 3\uac1c\uc758 operators\uac00 \uc788\ub2e4. - to (inclusive) - until (exclusive) - by (to determine step value) val r: Range = 1 until 5 // 1,2,3,4 val s: Range = 1 to 5 // 1,2,3,4,5 1 to 10 by 3 // 1,4,7,10 6 to 1 by -2 // 6,4,2 Ranges\ub294 lower bound, upper bound, step value 3\uac1c\uc758 \ud544\ub4dc\ub97c \uac16\ub294 single Object\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. Sequence\ub294 \uc544\ub798\uc640 \uac19\uc740 operations\ub3c4 \uc81c\uacf5\ud55c\ub2e4. xs exists p xs forall p // xs\uc758 \ubaa8\ub4e0 \uc694\uc18c\uc5d0 p(x)\uac00 true\ub77c\uba74 true \uadf8 \uc678\ub294 false xs zip ys // xs\uc640 ys\uc758 \uc694\uc18c\ub4e4\uc744 pair\ub85c \ub9cc\ub4e0 \uc0c8\ub85c\uc6b4 sequence\ub97c \ub9cc\ub4e0\ub2e4. \uc608\ub85c List(1,2) List('a', 'b')\uac00 \uc788\ub2e4\uba74 List((1, 'a'), (2, 'b'))\uac00 \ub41c\ub2e4. xs.unzip xs.flatMap f // f\ub97c xs\uc758 \ubaa8\ub4e0 element\uc5d0 \uc801\uc6a9\ud558\uace0 results\ub97c concatenate\ud55c\ub2e4. xs.sum xs.product xs.max xs.min val s = \"Hello World\" s filter (c => c.isUpper) // HW s exists (c => c.isUpper) // true s forall (c => c.isUpper) // false val pairs = List(1,2,3) zip s // List((1, H), (2, e), (3, l)) pairs.unzip // (List(1,2,3), List(H,e,l)) s flatMap (c => List('.', c)) // .H.e.l.l.o. .W.o.r.l.d xs.sum // 50 xs.max // 44 def combinations(m: Int, n: Int) = (1 to m) flatMap (x => (1 to n) map (y => (x, y))) combinations(4,4) def scalaProduct(xs: Vector[Double], ys: Vector[Double]): Double = (xs zip ys).map(xy => xy._1 * xy._2).sum // \uc704 \uc2dd\uc740 pattern matching function value\ub85c \uc544\ub798\ucc98\ub7fc \uc4f8 \uc218 \uc788\ub2e4. def scalaProductUsingPattern(xs: Vector[Double], ys: Vector[Double]): Double = (xs zip ys).map{ case (x, y) => x * y}.sum def isPrime(n: Int): Boolean = (2 until n) forall (d => n % d != 0)","title":"Lecture 6.1 - Other Collections"},{"location":"scala/fpp-in-scala/#lecture-62-combinatorial-search-and-for-expressions","text":"\uc885\uc885 Higher order functions \uc640 colledctions\ub294 imperative language\uc5d0\uc11c\uc758 loops\ub97c replace \ud55c\ub2e4. \uc911\ucca9\ub41c loops\ub294 \uc774\ub7ec\ud55c hof\uc758 \uc870\ud569\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \uc608\ub97c \ub4e4\uc5b4 integer i,j \uc5d0 \ub300\ud574 1 <= j < i < n\uc5d0 \ub300\ud574 i + j\uac00 prime\uc778 i, j\ub97c \ucc3e\ub294\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \ub9cc\uc57d n\uc774 7\uc774\ub77c\uba74 i,j\ub294 (2, 1), (3, 2), (4, 1) \ub4f1 \uc77c \uac83\uc774\ub2e4. \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc73c\ub85c\ub294 i\uc640 j\ub97c nested loops\ub85c \uad6c\uc131\ud558\uace0 buffer\ub97c \uc0dd\uc131\ud55c \ub4a4 prime\uc774\ub77c\uba74 buffer\uc5d0 \ucd94\uac00\ud560 \uac83\uc774\ub2e4. (i, j) pair\ub97c \ucc3e\ub294 \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc740 \uc544\ub798\uc640 \uac19\ub2e4. (1 until n) map (i => (1 until i) map (j => (i, j))) // Vector(Vector(), Vector((2,1)), Vector((3,1), (3,2)...) \ud558\uc9c0\ub9cc \uc704 \uacb0\uacfc\ub97c \ubcf4\uba74 Vectors of vectors\uac00 \ub9cc\ub4e4\uc5b4\uc9c4\ub2e4. \uc774\ub97c \ud558\ub098\uc758 Vectors\ub85c \ubc14\uafb8\ub824\uba74 \uc5b4\ub5bb\uac8c \ud574\uc57c \ud560\uae4c \uc704 \ucf54\ub4dc\uc758 \uacb0\uacfc\ub97c xss\ub77c\uace0 \ud560\ub54c \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218 \uc788\ub2e4. // 1. foldRight\uc744 \uc774\uc6a9\ud55c \ubc29\ubc95 (xss foldRight Seq[Int]())(_ ++ _) // 2. buil-in method \uc778 flatten\uc744 \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95 xss.flatten ((1 until n) map (i => (1 until i) map (j => (i, j)))).flatten \uc720\uc6a9\ud55c Law\uac00 \uc788\ub2e4. xs flatMap f = (xs map f).flatten \uadf8\ub807\uae30\uc5d0 \uc544\ub798\uc640 \uac19\uc774 \uac00\ub2a5\ud558\ub2e4 (1 until n) flatMap(i => (1 until i) map (j => (i, j))) \ud558\uc9c0\ub9cc \uc5ec\uc804\ud788 \uc644\uc131\uc740 \uc548\ub410\ub2e4. sum\uc758 \uacb0\uacfc\uac00 prime\uc778 pair\ub9cc \ud544\uc694\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. def isPrime(n: Int) = (2 until n) forall (n % _ != 0) (1 until n) flatMap(i => (1 until i) map (j => (i, j))) filter (pair => isPrime(pair._1 + pair._2)) HOF\uc778 map, flatMap, filter\ub294 manipulating lists\ub97c \uc81c\uacf5\ud558\ub294 \uac15\ub825\ud55c \ubc29\ubc95\uc774\ub2e4. \ud558\uc9c0\ub9cc \ub54c\ub54c\ub85c \uc774\ub294 \uc774\ud574\ud558\uae30 \uc5b4\ub835\uac8c \ud55c\ub2e4. Scala\uc758 for expreesion notaion\uc774 \ud574\uacb0\ud574 \uc904 \uc218 \uc788\ub2e4. // \uc544\ub798\uc640 \uac19\uc740 class\uac00 \uc788\ub2e4\uace0 \ud558\uc790 case class Person(name: String, age: Int) // 20\uc0b4\uc774 \ub118\ub294 Person\uc758 names\ub97c \uc5bb\uace0 \uc2f6\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud560 \uc218 \uc788\ub2e4. for (p <- persons if p.age > 20) yield p.name // \uc704 \ud45c\ud604\uc740 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. persons filter (p => p.age > 20) map (p => p.name) \uc704 \ucf54\ub4dc\ub97c \ubd24\uc744 \ub54c for\ub97c \uc774\uc6a9\ud55c \ubc29\uc2dd\uc774 \ub354 \uc774\ud574\ud558\uae30 \uc27d\ub2e4. imperative languages\uc758 loop\uc640 \ub2e4\ub978\uc810\uc740 \uacb0\uacfc\ub85c \uc0c8\ub85c\uc6b4 list\ub97c \ub9cc\ub4e4\uc5b4 \ub0b8\ub2e4\ub294 \uc810\uc774\ub2e4.","title":"Lecture 6.2 - Combinatorial Search and For-Expressions"},{"location":"scala/fpp-in-scala/#syntax-of-for","text":"for ( s ) yield e s\ub294 sequence of generators and filters \uc774\uace0, e\ub294 iteration\uc5d0\uc11c return \ub418\ub294 value\uc758 expression\uc774\ub2e4. generator \uc758 form\uc740 p <- e \uc774\ub2e4. p\ub294 pattern\uc774\uace0 e \ub294 collection\uc758 value\uac00 \ub418\ub294 expression\uc774\ub2e4. filter \uc758 form\uc740 if f \uc774\ub2e4. f\ub294 boolean expression\uc774\ub2e4. sequence\ub294 \ubc18\ub4dc\uc2dc generator\ub85c \uc2dc\uc791\ud574\uc57c \ud55c\ub2e4. sequence\ub0b4 \ub9cc\uc57d \uc5ec\ub7ec \uac1c\uc758 generator\uac00 \uc788\ub2e4\uba74, \ub9c8\uc9c0\ub9c9 generator\ub294 \uccab \ubc88\uc9f8\ubcf4\ub2e4 \ube60\ub974\ub2e4. ( s ) \ub300\uc2e0 { s } \ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. \uadf8\ub9ac\uace0 sequence of generators and filters\ub294 semicolon \uc5c6\uc774 \uc5ec\ub7ec \uc904\ub85c \uc4f0\uc77c \uc218 \uc788\ub2e4. \uc804\uc5d0 \ud480\uc5c8\ub358 prime pair\ub97c \uad6c\ud558\ub294 \ubc95\uc744 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. for { i <- 1 until n j <- 1 until i if isPrime(i + j) } yield (i, j) def scalarProduct(xs: List[Double], ys: List[Double]): Double = { (for { x <- xs y <- ys } yield x * y).sum }","title":"Syntax of For"},{"location":"scala/fpp-in-scala/#lecture-63-combinatorial-search-example","text":"Set\uc740 Scala collections\uc758 \ub610 \ub2e4\ub978 basic abstraction\uc774\ub2e4. val fruit = Set(\"apple\", \"banana\", \"pear\") val s = (1 to 6).toSet \ub300\ubd80\ubd84\uc758 sequence operations\ub294 sets\uc5d0\uc11c\ub3c4 \uac00\ub2a5\ud558\ub2e4. s map (_ + 2) // Set(3,4,5,6,7,8) fruit filter (_.startsWith == \"app\") s.nonEmpty","title":"Lecture 6.3 - Combinatorial Search Example"},{"location":"scala/fpp-in-scala/#sets-vs-sequences","text":"\ucc28\uc774\uc810\uc774 3\uac00\uc9c0 \uc788\ub2e4. 1. Sets\ub294 unordered\uc774\ub2e4. 2. Sets\ub294 \uc911\ubcf5\ub41c elements\ub97c \uac16\uc9c0 \ubabb\ud55c\ub2e4. 3. fundamental operation\uc774 sets\uc5d0\ub294 \ud3ec\ud568\ub418\uc5b4 \uc788\ub2e4. s contains 5","title":"Sets vs Sequences"},{"location":"scala/fpp-in-scala/#example-n-queens","text":"recursive alogrithm\uc73c\ub85c \ud574\uacb0 \uac00\ub2a5\ud558\ub2e4. - \uc0ac\uc774\uc988\uac00 n\uc778 \uccb4\uc2a4 \ubcf4\ub4dc\uc5d0\uc11c k-1 \uac1c\uc758 queen\uc744 \ub193\ub294 \ubaa8\ub4e0 solutions\ub97c \uc774\ubbf8 \uc0dd\uc131\ud588\ub2e4\uace0 \uac00\uc815\ud558\uc790 - \uac01 solution\uc740 0~n-1 \uae4c\uc9c0\uc758 columns\ub97c \uac16\ub294 k-1\uc758 length \ub9ac\uc2a4\ud2b8\ub85c \ud45c\ud604 \uac00\ub2a5\ud558\ub2e4. - k-1 \ubc88\uc9f8\uc758 row\uc5d0\uc11c column number of the queen \ub9ac\uc2a4\ud2b8\uc758 \uccab \ubc88\uc9f8\ub85c \uc624\uace0 \uadf8 \ub2e4\uc74c\uc5d0\ub294 k-2 \ubc88\uc9f8\uc758 row\uc5d0 \uc788\ub294 column number of queen\uc774 \uc628\ub2e4. - solution set\uc740 set of lists\ub85c \ud45c\ud604\ub418\uba70, \uac01 element\ub294 solution\uc774 \ub41c\ub2e4. - \uc774\uc81c k\ubc88\uc9f8\uc5d0 queen\uc744 \ubc30\uce58\ud558\uae30 \uc704\ud574, \uc774\uc804\uc5d0 \ub193\uc600\ub358 \uac83\uc744 \uace0\ub824\ud558\uc5ec \uac00\ub2a5\ud55c \ubaa8\ub4e0 \ubc30\uce58 \uacbd\uc6b0\ub97c \ub9cc\ub4e4\uc5b4\uc57c \ud55c\ub2e4. def queens(n: Int): Set[List[Int]] = { def placeQueens(k: Int): Set[List[Int]] = { if (k == 0) Set(List()) else // { // placeQueens(k-1) flatMap ((a) => { // 0 until 2 map (b => { // if isSafe(b, a) // b :: a // }) // }) // } for { queens <- placeQueens(k - 1) col <- 0 until n if isSafe(col, queens) } yield col :: queens } def isSafe(col: Int, queens: List[Int]): Boolean = { val row = queens.length val queensWithRow = (row -1 to 0 by -1) zip queens queensWithRow forall { case (r, c) => col != c && math.abs(col - c) != row - r } } placeQueens(n) } def show(queens: List[Int]): String = { val lines = for (col <- queens.reverse) yield Vector.fill(queens.length)(\"* \").updated(col, \"X \").mkString \"\\n\" + (lines mkString \"\\n\") } def main(args: Array[String]): Unit = { val s = (queens(4) map show) mkString \"\\n\" println(s) println((queens(5) take 3 map show) mkString \"\\n\") }","title":"Example: N-Queens"},{"location":"scala/fpp-in-scala/#lecture-64-maps","text":"\ub610 \ub2e4\ub978 fundamental collection type\uc73c\ub85c map\uc774 \uc788\ub2e4. val romanNumerals: Map[String, Int] = Map(\"I\" -> 1, \"V\" -> 5, \"X\" -> 10) val capitalOfCountry = Map(\"US\" -> \"Washington\") Class Map[Key, Value]\ub294 colletion type\uc778 Iterable[(Key, Value)]\ub85c \ud655\uc7a5\ud560 \uc218 \uc788\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 iterables\uac00 \ud560 \uc218 \uc788\ub294 collection operations\uac00 \uac00\ub2a5\ud558\ub2e4. val countryOfCaptials = capitalOfCountry map { case (x, y) => (y, x) } map\uc740 key/value \uc30d\uc758 iterable\uc744 \ud655\uc7a5\ud55c \uac83\uc774\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 key -> value \ub294 (key, value) \ub85c \uc4f8 \uc218\ub3c4 \uc788\ub2e4. Class Map[Key, Value]\ub294 \ub610\ud55c Key => Value\uc758 type\uc744 \uac16\ub294 function\uc73c\ub85c \ud655\uc7a5\ud560 \uc218 \uc788\ub2e4. \ub530\ub77c\uc11c \ud568\uc218\uac00 \ud560 \uc218 \uc788\ub294 \uac83\uc740 \ud560 \uc218 \uc788\ub2e4. capitalOfCountry(\"US\") map\uc5d0 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294 key\ub97c \uc8fc\uba74 error\uac00 \ubc1c\uc0dd\ud55c\ub2e4. capitalOfCountry(\"Andorra\") // java.util.NoSuchElementException \uc8fc\uc5b4\uc9c4 key\uac00 map\uc5d0 \uc874\uc7ac\ud558\ub294\uc9c0 \uc54c \uc218 \uc5c6\uc744 \ub54c query\ub97c \ud558\uba74 \ub41c\ub2e4. capitalOfCountry get \"US\" // Some(\"Washington\") capitalOfCountry get \"Andorra\" // None get operation\uc758 \uacb0\uacfc\ub294 Option value\uc774\ub2e4.","title":"Lecture 6.4 - Maps"},{"location":"scala/fpp-in-scala/#the-option-type","text":"standard library\uc5d0 \uc18d\ud55c\ub2e4. trait Option[+A] case class Some[+A](value: A) extends Option[A] object None extends Option[Nothing] map get key\ub294 \ub2e4\uc74c\uc744 \ub9ac\ud134\ud55c\ub2e4. - None : \ub9f5\uc5d0 \ud574\ub2f9 key\uc5d0 \ud574\ub2f9\ud558\ub294 \uac12\uc774 \uc5c6\uc744 \uacbd\uc6b0 - Some(x): map\uc5d0 \ud574\ub2f9 key\uc5d0 \ud574\ub2f9\ud558\ub294 \uac12\uc774 \uc788\uc744 \uacbd\uc6b0 value x options\uac00 case classes\uc774\ubbc0\ub85c pattern matching\uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. def showCapital(country: String) = capitalOfCountry.get(country) match { case Some(capital) => capital case None => \"missing data\" } showCapital(\"US\") showCapital(\"Andorra\") Options\ub294 \ub2e4\ub978 collections\uc5d0\uc11c\ub3c4 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4.","title":"The Option Type"},{"location":"scala/fpp-in-scala/#sorted-and-groupby","text":"SQL queries\uc5d0\uc11c \uc720\uc6a9\ud55c groupBy\uc640 orderBy\ub97c \uc4f8 \uc218 \uc788\ub2e4. val fruit = List(\"apple\", \"pear\", \"orange\", \"pineapple\") fruit sortWith (_.length < _.length) // List(\"pear\", \"apple\", \"orange\", \"pineapple\") fruit.sorted // List(\"apple\", \"orange\", \"pear\", \"pineapple\") fruit groupBy (_.head) // Map(p -> List(pear, pineapple), a -> List(apple), o -> List(orange)) map\uc740 partial functions \uc774\ub2e4. map(key) \ub97c \uc218\ud589\ud588\uc744 \ub54c \ud574\ub2f9 \ud558\ub294 \uac12\uc774 \uc5c6\uc73c\uba74 exception\uc744 \ubc1c\uc0dd\uc2dc\ud0a4\uae30 \ub54c\ubb38\uc774\ub2e4. withDefaultValue operation\uc744 \uc0ac\uc6a9\ud558\uba74 map\uc744 total function\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc900\ub2e4. val cap1 = capitalOfCountry withDefaultValue \"unknown\" cap1(\"Andorra\") // \"Unknown\" Poly\ub97c \uc5ec\ub7ec \uae30\uc220\uc744 \uc801\uc6a9\ud574\uc11c \ubc14\uafd4\ubcf4\uc790. class Poly(val terms0: Map[Int, Double]) { // p1\uacfc p2 \uc120\uc5b8\uc5d0\uc11c \ubcf4\uba74 \ud56d\uc0c1 Map\uc73c\ub85c \uac10\uc2f8\uc918\uc57c \ud588\ub2e4. // \uc544\ub798\uc640 \uac19\uc774 constructor\ub97c \uc0ac\uc6a9\ud558\uba74 \uc774\ub7f0 \ubc18\ubcf5\uc801\uc778 \ud589\ub3d9\uc744 \uc9c0\uc6b8 \uc218 \uc788\ub2e4. // *\uc758 \uc758\ubbf8\ub294 \ud574\ub2f9 \ud328\ud134\uc774 \ubc18\ubcf5\ub41c\ub2e4\ub294 \uac83\uc774\ub2e4. def this(bindings: (Int, Double)*) = this(bindings.toMap) // defaultValue\ub97c \uc120\uc5b8\ud568\uc73c\ub85c\uc11c addTerm\uc5d0\uc11c Option\ubd80\ubd84\uc744 \uc81c\uac70\ud588\ub2e4. val terms = terms0 withDefaultValue 0.0 // map\uc758 concatenation\uc778 ++\ub97c foldLeft\ub97c \ubcc0\uacbd\ud574\uc11c \uac04\ub2e8\ud558\uac8c \ubcc0\uacbd\ud588\ub2e4. def + (other: Poly) = new Poly((other.terms foldLeft terms)(addTerm)) def addTerm(terms: Map[Int, Double], term: (Int, Double)): Map[Int, Double] = { val (exp, coeff) = term terms + (exp -> (coeff + terms(exp))) } override def toString = (for ((exp, coeff) <- terms.toList.sorted.reverse) yield coeff+\"x^\"+exp) mkString \" + \" } val p1 = new Poly(1 -> 2.0, 3 -> 4.0, 5 -> 6.2) val p2 = new Poly(Map(0 -> 3.0, 3 -> 7.0)) p1 + p2 \uadf8\ub807\ub2e4\uba74 addTerm\uc744 foldLeft\ub85c \uad6c\ud604\ud558\ub294 \uac83\uacfc ++\ub85c \uad6c\ud604\ud558\ub294 \uac83 \uc911\uc5d0 \uc5b4\ub5a4 \uac83\uc774 \ub354 \ud6a8\uacfc\uc801\uc77c\uae4c? foldLeft\ub85c \uad6c\ud604\ud558\uba74 \uae30\uc874 data structure\uc5d0 \uacc4\uc18d \ub354\ud558\ub294 \ubc18\uba74 ++\ub85c \uad6c\ud604\ud558\uba74 \uc0c8\ub85c\uc6b4 List\uac00 \uacc4\uc18d \uc0dd\uc131\ub418\uae30 \ub584\ubb38\uc5d0 foldLeft \ubc84\uc804\uc774 \ub354\uc6b1 \ud6a8\uacfc\uc801\uc774\ub2e4.","title":"Sorted and GroupBy"},{"location":"scala/fpp-in-scala/#lecture-65-putting-the-pieces-together","text":"val mnemonics = Map( '2' -> \"ABC\", '3' -> \"DEF\", '4' -> \"GHI\", '5' -> \"JKL\", '6' -> \"MNO\", '7' -> \"PQRS\", '8' -> \"TUV\", '9' -> \"WXYZ\") \uc704\uc640 \uac19\uc740 dictionary words\uac00 \uc788\uc744 \ub54c method translate\uc744 \uad6c\ud604\ud55c\ub2e4\uace0 \ud574\ubcf4\uc790. translate(phoneNumber) \uc608\ub97c \ub4e4\uc5b4 \"7225247386\"\uc744 \uc704\uc758 mnemonics\ub97c \ucc38\uc870\ud574\uc11c \ubcc0\uacbd\ud558\ub294\ub370 \"Scala is fun\" solution phrases\uc911 \ud558\ub098\uc5ec\uc57c \ud55c\ub2e4. \uc704 \uc608\uc81c\ub294 Lutz Prechelt: An Empirical Comparison of Seven Programming Languages. (2000) Tested with Tcl, Python, Perl, Rexx, Java, C++, C. \ub17c\ubb38\uc5d0\uc11c \uac00\uc838\uc628 \uac83\uc774\ub2e4. scripting language\ub294 100 \ub77c\uc778\uc73c\ub85c \ud574\uacb0 \uac00\ub2a5\ud588\uace0 \ub2e4\ub978 \uac83\uc740 200-300 \ub77c\uc778\uc73c\ub85c \ud574\uacb0 \uac00\ub2a5\ud588\ub2e4 Scala Immutable collections are - easy to use: few steps to do the job - concise: one word replaces a whole loop - safe: type checker is really good at catching errors. - fast: collection ops are tuned, can be parallelized. - universal: one vocabulary to work on all kinds of collections. \uc774\uac83\ub4e4\uc774 \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uac1c\ubc1c\uc5d0 \uc788\uc5b4 \ub9e4\uc6b0 \ub9e4\ub825\uc801\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc900\ub2e4.","title":"Lecture 6.5 - Putting the Pieces Together"},{"location":"scala/my-scala/","text":"My Scala Set Set \uc740 Seq \ubcf4\ub2e4 random lookup\uc774 \ube60\ub974\ub2e4. ++ \ub97c \uc0ac\uc6a9\ud558\uba74 Set\uc5d0 multiple variables\ub97c \ucd94\uac00\ud560 \uc218 \uc788\ub2e4. val sets = Set() ++ ( for { num <- numsList } yield num ) Read File classpath \uc704\uce58\ub85c \ubd80\ud130 \ud30c\uc77c \uc77d\uae30 import scala.io.Source def getResourcePath(): String = { val txtFile = Source.fromFile(\"src/main/resources/config.txt\") txtFile.getLines().take(1).toList.head } resources\ud3f4\ub354\ub0b4 \ud30c\uc77c \uc77d\uae30 import scala.io.Source Option(getClass.getResourceAsStream(\"myfile.dat\")) match { case None => sys.error(\"File is Empty\") case Some(resource) => Source.fromInputStream(resource).getLines().toList }","title":"My Scala"},{"location":"scala/my-scala/#my-scala","text":"","title":"My Scala"},{"location":"scala/my-scala/#set","text":"Set \uc740 Seq \ubcf4\ub2e4 random lookup\uc774 \ube60\ub974\ub2e4. ++ \ub97c \uc0ac\uc6a9\ud558\uba74 Set\uc5d0 multiple variables\ub97c \ucd94\uac00\ud560 \uc218 \uc788\ub2e4. val sets = Set() ++ ( for { num <- numsList } yield num )","title":"Set"},{"location":"scala/my-scala/#read-file","text":"classpath \uc704\uce58\ub85c \ubd80\ud130 \ud30c\uc77c \uc77d\uae30 import scala.io.Source def getResourcePath(): String = { val txtFile = Source.fromFile(\"src/main/resources/config.txt\") txtFile.getLines().take(1).toList.head } resources\ud3f4\ub354\ub0b4 \ud30c\uc77c \uc77d\uae30 import scala.io.Source Option(getClass.getResourceAsStream(\"myfile.dat\")) match { case None => sys.error(\"File is Empty\") case Some(resource) => Source.fromInputStream(resource).getLines().toList }","title":"Read File"},{"location":"scala/my-spark/","text":"_* \uc5d0 \ub300\ud558\uc5ec List(1,2,3,4)\ub97c function(a, b, c, d)\uc758 parameter\ub85c \uc804\ub2ec\ud558\uace0 \uc2f6\uc744 \ub588 \uc5b4\ub5bb\uac8c \ud560\uae4c _* \ub97c \uc774\uc6a9\ud558\uba74 \uc704\uc640 \uac19\uc740 \uc694\uad6c\uc0ac\ud56d\uc744 \ud574\uacb0\ud560 \uc218 \uc788\ub2e4. Row(line.head.toString :: line.tail.map(_.toDouble): _*) broadcast variables Broadcast variables\ub294 cluster\ub0b4 \uac01 node\uc5d0 \uc624\uc9c1 \ud55c \ubc88 \ub9cc \ubcf4\ub0b4\ub3c4\ub85d \ud55c\ub2e4. \ub610\ud55c \ubcc0\uc218\ub4e4\uc744 cluster node\ub0b4 \uba54\ubaa8\ub9ac\uc5d0 \uce90\uc2f1\ud558\uc5ec \ud504\ub85c\uadf8\ub7a8 \uc218\ud589 \uc2dc \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud55c\ub2e4. val bcEmployees = sc.broadcast(employees)","title":"My Spark"},{"location":"scala/my-spark/#_","text":"List(1,2,3,4)\ub97c function(a, b, c, d)\uc758 parameter\ub85c \uc804\ub2ec\ud558\uace0 \uc2f6\uc744 \ub588 \uc5b4\ub5bb\uac8c \ud560\uae4c _* \ub97c \uc774\uc6a9\ud558\uba74 \uc704\uc640 \uac19\uc740 \uc694\uad6c\uc0ac\ud56d\uc744 \ud574\uacb0\ud560 \uc218 \uc788\ub2e4. Row(line.head.toString :: line.tail.map(_.toDouble): _*)","title":"_* \uc5d0 \ub300\ud558\uc5ec"},{"location":"scala/my-spark/#broadcast-variables","text":"Broadcast variables\ub294 cluster\ub0b4 \uac01 node\uc5d0 \uc624\uc9c1 \ud55c \ubc88 \ub9cc \ubcf4\ub0b4\ub3c4\ub85d \ud55c\ub2e4. \ub610\ud55c \ubcc0\uc218\ub4e4\uc744 cluster node\ub0b4 \uba54\ubaa8\ub9ac\uc5d0 \uce90\uc2f1\ud558\uc5ec \ud504\ub85c\uadf8\ub7a8 \uc218\ud589 \uc2dc \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud55c\ub2e4. val bcEmployees = sc.broadcast(employees)","title":"broadcast variables"},{"location":"scala/parallel-programming/","text":"parallel programming Week1 Parallel computing\uc740 many caculation\uc744 \ub3d9\uc2dc\uc5d0 computation \ud558\ub294 \uac83\uc774\ub2e4. Basic principle: computation\uc740 smaller subproblems\ub85c \ub098\ub20c \uc218 \uc788\uc73c\uba70 \uac01\uac01\uc740 \ub3d9\uc2dc\uc5d0 solved \ub420 \uc218 \uc788\ub2e4. Assumption: parallel hardware\uac00 \uc788\ub2e4\uba74 \uc774\ub7ec\ud55c computation\uc744 parallel\ud558\uac8c \uc2e4\ud589\ud560 \uc218 \uc788\ub2e4. 20\uc138\uae30\uc5d0 IBM\uc774 first commercial parallel computers\ub97c \ub9cc\ub4e4\uc5b4 \ub0c8\ub2e4. \uadf8 \ub2f9\uc2dc\uc5d0\ub294 \uc778\uae30\uac00 \uc5c6\uc5c8\uc9c0\ub9cc \ud604\uc7ac\ub294 computing performance\uac00 \uc99d\uac00\ud558\uba74\uc11c \uc778\uae30\uac00 \ub9ce\uc544\uc84c\ub2e4. \uc65c Parallel Computing\uc778\uac00? Parallel programming\uc740 sequential programming\ubcf4\ub2e4 \ub354 \uc5b4\ub835\ub2e4. - \uba87 \uba87 computation\uc740 divide\uac00 \ubd88\uac00\ub2a5\ud558\uac70\ub098 \uc5b4\ub835\ub2e4. - error \uc7a1\uae30 \ud798\ub4e4\ub2e4. \ub610 \ub9ce\uc740 \uc0c8\ub85c\uc6b4 \ud0c0\uc785\uc758 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Speedup is only reason why we bother paying for this complexity Parallelism and concurrency \ub294 \uad00\ub828\ub41c \ucee8\uc149\uc774\ub2e4. Parallel Programming\uc740 parallel hardware\ub97c \uc0ac\uc6a9\ud558\uc5ec \uacc4\uc0b0\uc744 \ub354 \ube68\ub9ac \uc218\ud589\ud55c\ub2e4. Efficiency \uac00 \ucd5c\ub300 \uad00\uc2ec\uc0ac\uc774\ub2e4. mainly concerned algorithmic problems, numerical computation or big data applications. \uc544\ub798\uc758 \ubb38\uc81c\uc5d0 \uad00\uc2ec\uc744 \uac16\ub294\ub2e4. \uc5b4\ub5bb\uac8c divide into sub problems \ud558\uace0 \uc5b4\ub5bb\uac8c \ub3d9\uc2dc\uc5d0 \uc218\ud589\ud558\ub290\ub0d0\uac00 \ubb38\uc81c\uc774\ub2e4. \ucd5c\ub300\ud55c\uc758 \uc18d\ub3c4\ub97c \uc704\ud574 \uc5b4\ub5bb\uac8c \uc790\uc6d0\uc744 \ud65c\uc6a9\ud560\uae4c\ub3c4 \ubb38\uc81c\uc774\ub2e4. Concurrent Program\uc740 \ub3d9\uc2dc\uc5d0 multiple execution\uc744 \uc218\ud589\ud560 \ub54c\ub3c4 \uc788\uc9c0\ub9cc \uc544\ub2d0 \ub54c\ub3c4 \uc788\ub2e4. Modularity, responsiveness or maintainability\uac00 \ucd5c\ub300 \uad00\uc2ec\uc0ac\ub2e4. targeted writing asynchronous applications. such as webservers, user interfaces or databases. \ub610\ud55c \uc544\ub798\uc758 \ubb38\uc81c\uc5d0 \uad00\uc2ec\uc744 \uac16\ub294\ub2e4. \uc5b8\uc81c \uacc4\uc0b0\uc744 \uc218\ud589\ud574\uc57c \ud560\uae4c? when and how \ub450 \uac1c\uc758 concurrent execution\uc774 \uc815\ubcf4\ub97c \uad50\ud658\ud574\uc57c \ud560\uae4c? \uc5b4\ub5bb\uac8c shared resources(file, db\ub4f1) \uc811\uadfc\uc744 \uad00\ub9ac\ud574\uc57c \ud558\ub294\uac00? \ub450 \uac1c\ub150\uc774 \uacf5\uc720\ud558\ub294 \uc0ac\ud56d\uc740 \uc788\uc9c0\ub9cc superset\uc740 \uc5c6\ub2e4. Parallelism manifests itself at di\ufb00erent granularity levels. - bit-level parallelism: processing multiple bits of data in parallel - instruction-level parallelism: executing di\ufb00erent instructions from the same instruction stream in parallel - task-level parallelism: executing separate instruction streams in parallel bit-level\uacfc intstruction-level\uc740 processor\uac00 \uc54c\uc544\uc11c \uc218\ud589\ud574\uc900\ub2e4. \uc6b0\ub9ac\uac00 \uc774\ubc88\uc5d0 \ub2e4\ub8f0 \uac83\uc740 task-level\uc774\ub2e4. parallel h/w\uc5d0\ub294 \ub2e4\uc591\ud55c \ud615\ud0dc\uac00 \uc788\ub2e4. - multi-core processors - symmetric multiprocessors(SMP): multiple identical processors\uac00 bus\uc640 \uc5f0\uacb0\ub418\uc5b4 memory\ub97c share\ud558\ub294 \uac83\uc774\ub2e4. - graphic processing unit: originally intended for graphics processing. \uae30\ubcf8\uc801\uc73c\ub860 execute program\uc744 \ud558\uc9c4 \uc54a\uc9c0\ub9cc host processor\uc5d0 \uc758\ud574\uc11c \uc218\ud589\ud560 \uc218 \uc788\ub2e4. - field-programmable gate arrays(FPGA): these are programmed in hardware description languages, such as Verilog and HTL - computer clusters: groups of computers connected via a network. not sharing memory \uc774\ubc88 \uacfc\uc815\uc5d0\uc11c\ub294 multi-core processors\uc640 symmetric multiprocessor\ub97c \uc911\uc810\uc801\uc73c\ub85c \ub2e4\ub8ec\ub2e4. Parallelism on the JVM I There are many forms of parallelism in the wild. From GPUs and custom parallel hardware, over multiprocessors, and multicore systems to distributed computer clusters. Each parallel programming model is tailored to a specific use case, and has certain associated characteristics. OS\ub780? We will say that an operating system is a piece of software that manages hardware and software resources, and schedules program execution. Process - OS\uc5d0\uc11c \uc2e4\ud589\ub418\ub294 program\uc758 instance same program\uc740 \ud558\ub098\uc758 process\uc5d0\uc11c \uc5ec\ub7ec \ubc88 \uc2dc\uc791\ub420 \uc218 \uc788\uace0, same OS\uc5d0\uc11c \ub3d9\uc2dc\uc5d0 \uc218\ud589\ub420 \uc218\ub3c4 \uc788\ub2e4. OS\ub294 \uc5ec\ub7ec process\ub97c \ud55c\uc815\ub41c CPU\uc5d0\uc11c \uc2e4\ud589\ud574\uc57c \ud55c\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 time slices of excution\uc774 \uc788\ub2e4. \uc774\ub7ec\ud55c \ub9e4\ucee4\ub2c8\uc998\uc744 multitasking\uc774\ub77c\uace0 \ud55c\ub2e4. \uadf8\ub9ac\uace0 \uac01 process\ub4e4\uc740 \uba54\ubaa8\ub9ac\ub97c \uc9c1\uc811\uc801\uc73c\ub85c \uacf5\uc720\ud560 \uc218 \uc5c6\ub2e4. = isolated memory area\uac00 \uc788\ub2e4. (\ub418\ub294 OS\uac00 \uc788\uae34 \ud558\ub514\u001d) \uadf8\ub798\uc11c! Threads\uac00 \ud544\uc694\ud558\ub2e4. each process\ub294 multiple independent concurrency units called threads\ub97c \uac16\ub294\ub2e4. using thread\uc758 \ub450 \uac00\uc9c0 \uc7a5\uc810\uc774 \uc788\ub2e4. - thread\ub294 \ud504\ub85c\uadf8\ub7a8 \ub0b4\uc5d0\uc11c programmatically started\ud560 \uc218 \uc788\ub2e4. = structure parallel computation\uc744 \ub9cc\ub4e4\uae30 \uc27d\ub2e4. \ud504\ub85c\uc138\uc11c\ubcf4\ub2e4. - \ub354 \uc911\uc694\ud55c \uac83\uc740 threads\ub294 same memory address space\ub97c \uacf5\uc720\ud55c\ub2e4. \uac01 thread\ub294 program counter(current executed method\uc758 \uba54\ubaa8\ub9ac\ub0b4 \uc704\uce58)\uc640 program stack(memory\ub0b4 \uacf5\uac04\uc73c\ub85c\uc11c \uc218\ud589\ud560 methods\ub97c \uac16\ub294\ub2e4.)\uc744 \uac16\ub294\ub2e4. JVM\uc5d0\uc11c \uc2e4\ud589\ub418\ub294 threads\ub294 each other's program stacks\ub97c modify \ud560 \uc218 \uc5c6\ub2e4. Each JVM process starts with a main thread. \ubcf4\ud1b5\uc758 \ud504\ub85c\uadf8\ub7a8\uc740 main thread\ub9cc \uc0ac\uc6a9\ud55c\ub2e4. parallel programming\uc5d0\uc11c\ub294 \uacc4\uc0b0\uc744 \uc704\ud574 \uc5ec\ub7ec threads\ub97c \uc0ac\uc6a9\ud55c\ub2e4. additional threads\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc544\ub798\uc758 \uc2a4\ud15d\uc744 \ub530\ub77c\uc57c \ud55c\ub2e4. 1. Define a Thread subclass 2. Instantiate a new Thread object. 3. Call start on the Thread object. class HelloThread extends Thread { override def run() { println(\"Hello World!\") } } val t = new HelloThread t.start() // main thread\uc640 HelloThread\ub97c \uc2e4\ud589\uc2dc\ud0a8\ub2e4. t.join() // join\uc740 hellothread\uac00 \ub05d\ub0a0\ub54c \uae4c\uc9c0 \uc2e4\ud589\uc744 \uba48\ucd98\ub2e4. \uc704\uc758 \ucf54\ub4dc\ub294 \uc798 \ub3d9\uc791\ud558\ub294 \uac83 \ucc98\ub7fc \ubcf4\uc778\ub2e4. class HelloThread2 extends Thread { override def run() { println(\"Hello\") println(\"World!\") } } def main(): Unit = { val t = new HelloThread2() val s = new HelloThread2() t.start() s.start() t.join() s.join() } main() \uc704\uc758 \ucf54\ub4dc\ub294 \uc218\ud589\ud560 \ub54c\ub9c8\ub2e4 \ub2e4\ub978 \uacb0\uacfc\uac00 \ub098\uc62c \uc218\ub3c4 \uc788\ub2e4. \uc65c\ub0d0\uba74 thread\uac00 \ub3d9\uc2dc\uc5d0 \uc218\ud589\ub418\uae30 \ub54c\ubb38\uc5d0 \uc5b4\uca54\ub54c\ub294 Hello Hello World World\uac00 \ub098\uc62c \uac83\uc774\uace0 \uc5b4\ub5a4\ub54c\ub294 Hello World Hello World\uac00 \ub098\uc62c \uac83\uc774\ub2e4. (two threads can overlap) \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \ub54c\ub54c\ub85c Hello World \uc758 \uc21c\uc11c\ub294 \ubcf4\uc7a5\ud558\uace0 \uc2f6\uc744 \uc218 \uc788\ub2e4(\uc0ac\uc774\uc5d0 \ub2e4\ub978 thread\uac00 \uc2e4\ud589\ub418\uc9c0 \uc54a\ub3c4\ub85d). Atomicity! An operation is atomic if it appears as if it occurred instantaneously from the point of view of other threads. \uc608) private var uidCount = 0L def getUniqueId(): Long = { uidCount = uidCount + 1 uidCount } \uc704 \ud568\uc218\ub294 uniqueId\ub97c \ub9ac\ud134\ud558\ub294 \ud568\uc218\uc774\ub2e4. \uadf8\ub807\uae30\uc5d0 \ub9e4\ubc88 \uace0\uc720\ud55c \uac12\uc744 \ub9cc\ub4e4\uc5b4 \ub0b4\uc57c \ud558\uba70 \uc774\ub97c \uc704\ud574 private var\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \ud604\uc7ac \uc774 \ud568\uc218\ub294 atomic \ud558\uc9c0 \uc54a\ub2e4. \uc65c\ub0d0\uba74 \uc5ec\ub7ec \uc4f0\ub808\ub4dc\uac00 \ub3d9\uc2dc\uc5d0 \uc0ac\uc6a9\ub41c\ub2e4\uace0 \ud560 \ub54c \uc704\uc5d0\uc11c \uc598\uae30\ud588\ub358 overlap\uc774 \ubc1c\uc0dd\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \uadf8\ub807\uac8c \ub418\uba74 id\uac00 1,2,3,4 \uc21c\ucc28\uc801\uc73c\ub85c \uac00\ub294\uac83\uc774 \uc544\ub2c8\ub77c 1,2,2,4,5,8,9 \uac19\uc774 \ub420 \uc218 \uc788\ub2e4. var uidCount = 0L def getUniqueId(): Long = { uidCount = uidCount + 1 uidCount } def startThread() = { val t = new Thread { override def run(): Unit = { val uids = for (i <-0 until 10) yield getUniqueId() println(uids) } } t.start() t } startThread() startThread() \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 java\uc640 scala\ub294 synchronized block\uc744 \uc9c0\uc6d0\ud55c\ub2e4. object x\uc758 synchronized call \uc774\ud6c4\uc758 code block\uc740 \ub3d9\uc2dc\uc5d0 \uc5ec\ub7ec \uc4f0\ub808\ub4dc\uc5d0\uc11c \uc2e4\ud589\ub420 \uc218 \uc5c6\ub2e4. (JVM\uc5d0\uc11c\ub294 synchronized object\ub97c monitor\ub77c\uace0 \uc9c0\uc815\ud55c\ub2e4. \uadf8\ub9ac\uace0 \ub098\uc11c \ub2e4\ub978 \uc4f0\ub808\ub4dc\uac00 \uc811\uadfc\ud558\ub824\uace0\ud558\uba74 block\ud55c\ub2e4.) private val x = new AnyRef {} private var uidCount = 0L def getUniqueId(): Long = x.synchronized { uidCount = uidCount + 1 uidCount } Parallelism on the JVM II \ub9cc\uc57d \uc740\ud589 \uc5c5\ubb34 \uac19\uc774 source\uc640 target\uc744 \uac19\uc774 block \ud574\uc57c \ud560 \ub54c\ub294 \uae30\uc874\uc758 synchronized\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc5c6\ub2e4. \uc774\ub7f4\ub550 more fine grained synchronization\uc744 \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4. class Account(private var amount: Int = 0) { def transfer(target: Account, n: Int) = this.synchronized { target.synchronized { this.amount -= n target.amount += n } } } A1\uc774 source A2\uac00 target\uc774\ub77c\uace0 \ud560 \ub54c thread\ub294 A1\uc5d0 monitor\ub97c \uc8fc\uace0 A1,A2\ub97c \ubb36\uc5b4\uc11c monitor\ub97c \ud558\ub098 \ub610 \uc900\ub2e4. class Account(private var amount: Int = 0) { def transfer(target: Account, n: Int) = this.synchronized { target.synchronized { this.amount -= n target.amount += n } } } def startThread(a: Account, b: Account, n: Int) = { val t = new Thread { override def run(): Unit = { for (i <- 0 until n) { a.transfer(b, 1) } } } t.start() t } val a1 = new Account(500000) val a2 = new Account(700000) val t = startThread(a1, a2, 150000) val s = startThread(a2, a1, 150000) t.join() // deadlock!! s.join() thread t\uac00 \ub05d\ub098\uc9c0 \uc54a\uc73c\uba74\uc11c deadlock\uc774 \uac78\ub9b0\ub2e4. deadlock\uc774\ub780 2\uac1c \ud639\uc740 \uadf8 \uc774\uc0c1\uc758 threads\ub4e4\uc774 \uc774\ubbf8 \uc810\uc720\uc911\uc778 resources \ub193\uc9c0 \uc54a\uc740 \ucc44 \uc11c\ub85c\uac00 \ub05d\ub098\uae38 \uae30\ub2e4\ub9ac\ub294 \uc0c1\ud669\uc774\ub2e4. \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c \ud55c \uac00\uc9c0 \ubc29\ubc95\uc740 same order\ub85c resource\ub97c \uc694\uccad\ud558\ub294 \uac83\uc774\ub2e4. \uc774\ub54c resource\uc5d0\ub294 \uc6b0\uc120\uc21c\uc704\uac00 \uc788\ub2e4\ub294 \uac00\uc815\uc774\ub2e4. val uid = getUniqueUid() private def lockAndTransfer(target: Account, n: Int) = this.synchronized { target.synchronized { this.amount -= n target.amount += n } } def transfer(target: Account, n: Int) = if (this.uid < target.uid) this.lockAndTransfer(target, n) else target.lockAndTransfer(this, -n) \uc704\uc758 \ucf54\ub4dc\ub294 \ub9e4 account object\ub9c8\ub2e4 uid\ub97c \ubd80\uc5ec\ud574\uc11c \uc774\uc5d0 \ub530\ub77c \uc6b0\uc120\uc21c\uc704\ub97c \ub9e4\uae30\ub294 \ubc29\ubc95\uc774\ub2e4. Memory Model\uc774\ub77c\ub294 \uac83\uc774 \uc788\ub2e4. Memory Model\uc774\ub780 threads\uac00 accessing shared memory\ub97c interact \ud560 \ub54c\uc758 \uaddc\uce59\ub4e4\uc744 \uc124\uba85\ud574 \ub193\uc740 \uac83\uc774\ub2e4. Java Memory Model\uc740 memory model for JVM\uc774\ub2e4. \ubaa8\ub4e0 \ub8f0\uc744 \ub2e4 \uc124\uba85\ud560 \uc21c \uc5c6\uc9c0\ub9cc \uae30\uc5b5\ud574\uc57c \ud560 \ub450 \uac00\uc9c0\uac00 \uc788\ub2e4. 1. \uba54\ubaa8\ub9ac\ub0b4 separate locations\uc5d0 writing\ud558\ub294 two threads\ub294 synchronization\uc774 \ud544\uc694 \uc5c6\ub2e4. 2. \ub610 \ub2e4\ub978 thread Y\ub97c \ud638\ucd9c\ud558\ub294 thread X\ub294 join\uc774 \ub9ac\ud134\ub41c \ud6c4 thread Y\uc758 \ubaa8\ub4e0 writes\ub97c observe\ud558\ub3c4\ub85d \ubcf4\uc7a5\ud574\uc57c \ud55c\ub2e4. Running Computations in Parallel Example: computing p-norm vector (a1, a2)\uc5d0 \ub300\ud55c 2-norm\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \uacc4\uc0b0 \ud55c\ub2e4. (a 1 2 + a 2 2 ) 1/2 array\uc5d0 \ub300\ud55c sum of power\ub294 \uc544\ub798\uc640 \uac19\uc774 \uad6c\ud560 \uc218 \uc788\ub2e4. def power(x: Int, p: Double): Int = math.exp(p * math.log(abs(x))).toInt def sumSegment(a: Array[Int], p: Double, s: Int, t: Int): Int = { var i= s var sum: Int = 0 while (i < t) { sum= sum + power(a(i), p) i= i + 1 } sum } \uc774 \uacc4\uc0b0\uc744 2 \uac1c\ub85c \ub098\ub20c \uc218 \uc788\ub2e4. 0~n \uae4c\uc9c0\uc758 \ud569 + n~m \uae4c\uc9c0\uc758 \ud569 def pNormTwoPart(a: Array[Int], p: Double): Int = { val m = a.length / 2 val (sum1, sum2) = (sumSegment(a, p, 0, m), sumSegment(a, p, m, a.length)) power(sum1 + sum2, 1/p) } \ud558\uc9c0\ub9cc \uc774\ub294 sequential\ud55c \ubc29\ubc95\uc774\ub2e4. sum1\uc774 \uacc4\uc0b0 \ub2e4 \ub418\uace0 \ub098\uc11c sum2\uac00 \uacc4\uc0b0\ub41c\ub2e4. parallel\ub85c\ub294 \uc5b4\ub5bb\uac8c \ud560\uae4c? def pNormTwoPart(a: Array[Int], p: Double): Int = { val m = a.length / 2 val (sum1, sum2) = parallel(sumSegment(a, p, 0, m), sumSegment(a, p, m, a.length)) power(sum1 + sum2, 1/p) } \uc55e\uc5d0 parallel\uc744 \ubd99\uc774\uba74 \ub41c\ub2e4. \uc774\ub807\uac8c \ub418\uba74 sum1\uacfc sum2\uac00 \ub3d9\uc2dc\uc5d0 \uacc4\uc0b0\ub41c\ub2e4. \ub9cc\uc57d 4(2 + 2)\uac1c\ub85c \ubd84\ub9ac\ud558\uba74 \uc5b4\ub5a8\uae4c? val m1 = a.length/4 val m2 = a.length/2 val m3 = 3*a.length/4 val ((sum1, sum2),(sum3,sum4)) = parallel( parallel(sumSegment(a, p, 0, m1), sumSegment(a, p, m1, m2)), parallel(sumSegment(a, p, m2, m3), sumSegment(a, p, m3, a.length)) ) generalize\ub294 \uc5b4\ub5bb\uac8c \ud560\uae4c? recursion\uc744 \uc0ac\uc6a9\ud558\uba74 \uac00\ub2a5\ud558\ub2e4! def pNormRec(a: Array[Int], p: Double): Int = power(segmentRec(a, p, 0, a.length), 1/p) // like sumSegment but parallel def segmentRec(a: Array[Int], p: Double, s: Int, t: Int) = { if (t - s < threshold) sumSegment(a, p, s, t) // small segment: do it sequentially else { val m = s + (t - s)/2 val (sum1, sum2) = parallel(segmentRec(a, p, s, m), segmentRec(a, p, m, t)) sum1 + sum2 } } \uadf8\ub807\ub2e4\uba74 signature of parallel\uc740 \uc5b4\ub5bb\uac8c \ub420\uae4c def parallel[A, B](taskA: => A, taskB: => B): (A, B) = { ... } \ud56d\uc0c1 \uac19\uc740 value\ub97c \ub9ac\ud134\ud574\uc57c \ud55c\ub2e4. (a,b) \ubcf4\ub2e4 parallel(a, b)\uac00 \ube60\ub974\ub2e4. by name\uc73c\ub85c argumentds\ub97c \ubc1b\ub294\ub2e4. \uc65c call by name\uc73c\ub85c \ud574\uc57c \ud560\uae4c? def parallel1[A, B](taskA: A, taskB: B): (A, B) = { ... } val (va, vb) = parallel1(a, b) \uc704\ub294 call by value\ubc84\uc804\uc774\ub2e4. \uc774\ub807\uac8c \ub418\uba74 \uc989\uc2dc \ud3c9\uac00\uac00 \ub418\uae30 \ub54c\ubb38\uc5d0 val (va, vb) = (a, b) \uac00 \ubc14\ub85c \uacc4\uc0b0\ub41c\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \uc774\ub294 sequential\ud558\uac8c \uacc4\uc0b0\ub418\ubc84\ub9b0 \ud6c4 \uadf8 \ub2e4\uc74c\ubd80\ud130 parallel\ud558\uac8c \uacc4\uc0b0\ub41c\ub2e4. (Because the parameters of parallel1 are call by value, a and b are evaluated sequentially in the second case, not in parallel as in the first case.) \ub2e4\ub978 \ucf54\ub4dc\ub97c \ubcf4\uc790. def sum1(a: Array[Int], p: Double, s: Int, t: Int): Int = { var i= s var sum: Int = 0 while (i < t) { sum= sum + a(i) // no exponentiation! i= i + 1 } sum } val ((sum1, sum2),(sum3,sum4)) = parallel( parallel(sum1(a, p, 0, m1), sum1(a, p, m1, m2)), parallel(sum1(a, p, m2, m3), sum1(a, p, m3, a.length)) ) \uc774 \ucf54\ub4dc\ub294 sumSegment\uc640 \ub2e4\ub974\uac8c speedup\uc744 \uc5bb\uae30\uac00 \ud798\ub4e4\ub2e4. \uc65c \uadf8\ub7f4\uae4c? Memory Bottleneck!! Array\ub294 random access memory\uc5d0 \uc800\uc7a5\ub41c\ub2e4. \uc6b0\ub9ac\uac00 multiple processor\ub97c \uac16\uace0 \uc788\ub354\ub77c\ub3c4 memory\ub294 shared\ub41c\ub2e4. \uadf8 \ub9d0\uc778 \uc989\uc2a8 \uacc4\uc0b0\uc2dc\uac04\uc740 \ud56d\uc0c1 \uba54\ubaa8\ub9ac\ub85c\ubd80\ud130 \ub370\uc774\ud130\ub97c fetch\ud558\ub294 \uc18d\ub3c4 \uc774\uc0c1\uc774 \ub41c\ub2e4. opportuniteis for spped-up\uc744 \uace0\ub824\ud560 \ub54c\ub294 number of cores \ubfd0\ub9cc \uc544\ub2c8\ub77c \ub2e4\ub978 shared resources(memory \ub4f1)\uc5d0\uc11c paralleism\uc774 \uac00\ub2a5\ud55c\uc9c0 \uc54c\uc544\uc57c \ud55c\ub2e4. Monte Carlo Method to Estimate Pi import scala.util.Random def mcCount(iter: Int): Int = { val randomX = new Random val randomY = new Random var hits = 0 for (i <- 0 until iter) { val x = randomX.nextDouble // in [0,1] val y = randomY.nextDouble // in [0,1] if (x*x + y*y < 1) hits = hits + 1 } hits } def monteCarloPiSeq(iter: Int): Double = 4.0 * mcCount(iter) / iter parallel \ubc84\uc804 def monteCarloPiPar(iter: Int): Double = { val ((pi1, pi2), (pi3, pi4)) = parallel( parallel(mcCount(iter/4), mcCount(iter/4)), parallel(mcCount(iter/4), mcCount(iter - 3*(iter/4))) ) 4.0 * (pi1 + pi2 + pi3 + pi4) / iter } First-Class Tasks val (v1, v2) = parallel(e1, e2) \uc704 \ud45c\ud604\uc740 task\ub97c \uc774\uc6a9\ud574 \ub2e4\uc74c\uacfc \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. val t1 = task(e1) val t2 = task(e2) val v1 = t1.join val v2 = t2.join t = task(e)\ub294 computation e \ub97c background \uc5d0\uc11c start \ud568\uc744 \uc758\ubbf8\ud55c\ub2e4. - t\ub294 task\uc774\uace0, \uacc4\uc0b0 e\ub97c \uc218\ud589\ud55c\ub2e4. - \ud604\uc7ac \uacc4\uc0b0\uc740 t\ub85c parallel\ud558\uac8c \uc9c4\ud589\ub41c\ub2e4. - e\uc758 \uacb0\uacfc\ub97c \uc5bb\uace0 \uc2f6\uc744\ub550 t.join \uc744 \uc0ac\uc6a9\ud55c\ub2e4. - t.join\uc740 \uacb0\uacfc\uac00 \uacc4\uc0b0\ub420 \ub54c\uae4c\uc9c0 blocks and wait\ud55c\ub2e4. Task interface def task(c: => A): Task[A] trait Task[A] { def join: A } join\uc744 implicit\uc73c\ub85c \ubcc0\uacbd\ud558\uba74 \uc544\ub798\uc640 \u314f\u3131\u314c\ub2e4. implicit def getJoin[T](x: Task[T]): T = x.join val ((part1, part2),(part3,part4)) = parallel( parallel(sumSegment(a, p, 0, mid1), sumSegment(a, p, mid1, mid2)), parallel(sumSegment(a, p, mid2, mid3), sumSegment(a, p, mid3, a.length)) ) power(part1 + part2 + part3 + part4, 1/p) \uc704\uc758 parallel p-form\uc740 \uc544\ub798\uc640 \uac19\uc774 task\ub97c \uc774\uc6a9\ud574\uc11c \ubcc0\uacbd\ud560 \uc218 \uc788\ub2e4. val t1 = task {sumSegment(a, p, 0, mid1)} val t2 = task {sumSegment(a, p, mid1, mid2)} val t3 = task {sumSegment(a, p, mid2, mid3)} val t4 = task {sumSegment(a, p, mid3, a.length)} power(t1 + t2 + t3 + t4, 1/p) \uadf8\ub807\ub2e4\uba74 parallel\uc744 task\ub97c \uc774\uc6a9\ud574\uc11c \ub9cc\ub4e4 \uc218 \uc788\uc744\uae4c? def parallel[A, B](cA: => A, cB: => B): (A, B) = { val tB: Task[B] = task { cB } val tA: A = cA (tA, tB.join) } \uc544\ub798\ub294 \uc798\ubabb\ub41c \ubc84\uc804\uc774\ub2e4. // WRONG def parallelWrong[A, B](cA: => A, cB: => B): (A, B) = { val tB: B = (task { cB }).join val tA: A = cA (tA, tB.join) } \ubb34\uc2a8 \uc77c\uc774 \uc77c\uc5b4\ub0a0\uae4c? cA\uc640 cB\uac00 parallel\ud558\uac8c \uc2e4\ud589\ub418\uc9c0 \uc54a\uace0 tB\uac00 \uba3c\uc800 \uacc4\uc0b0\ub418\uae30\ub97c \uae30\ub2e4\ub9b0 \ud6c4 tA\ub97c \uacc4\uc0b0\ud558\uac8c \ub41c\ub2e4. How Fast are Parallel Programs? \uc5b4\ub5bb\uac8c performance\ub97c \uce21\uc815\ud560 \uc218 \uc788\uc744\uae4c? - empirical measurement - asymptotic analysis Asymptotic anlysis\ub294 \uc544\ub798\uc758 \uacbd\uc6b0\uc5d0 algorithms scale\uc744 \uc5b4\ub5bb\uac8c \ud560\uc9c0 \uc774\ud574\ud558\ub294\ub370 \uc911\uc694\ud558\ub2e4. - inputs get larger - we have more h/w parallelism available worst-case bounds\ub97c \uc54c \uc218 \uc788\ub2e4. Testing and Benchmarking testing: \ud504\ub85c\uadf8\ub7a8\uc758 \uc77c\ubd80\ubd84\uc774 \uc758\ub3c4\ud55c \ub300\ub85c \ub3d9\uc791\ud558\ub294 \uc9c0\ub97c \uc54c\uae30 \uc704\ud568 \ub9de\ub294\uc9c0 \uc544\ub2cc\uc9c0\ub97c \ud310\ub2e8\ud558\ub294 binary(true / false) \uacb0\uacfc\ubb3c\uc744 \ub9cc\ub4e4\uc5b4 \ub0b8\ub2e4. benchmarking: \ud504\ub85c\uadf8\ub7a8 \uc77c\ubd80\ubd84\uc758 performance metrics\uc744 \uacc4\uc0b0\ud568 continuous value(\uc18c\uc694 \uc2dc\uac04 \ub4f1)\ub97c \ub9ac\ud134\ud55c\ub2e4. \uc65c Benchmarking\uc744 \ud574\uc57c \ud558\ub290\ub0d0? Parallel programs\ub294 speed up\uc774 \uac00\uc7a5 \uc911\uc694\ud558\ub2e4. \uadf8\ub807\uae30\uc5d0 sequential programs\ubcf4\ub2e4 benchmarking\uc774 \uc911\uc694\ud558\ub2e4. Performance(\ud2b9\ud788\ub098 running time)\ub294 \ub9ce\uc740 \uc694\uc18c\uc5d0 \uad00\ub828\uc788\ub2e4. - processor speed - number of processor - memory access latency and throughput (affects contention) - cache behavior(e.g. false sharing, associativity effects) - runtime behavior(e.g. garbage collection, JIT compilation, thread scheduling) \uadf8\ub798\uc11c measuring performance\ub294 \uc815\ud655\ud788 \uc54c\uc544\ub0b4\uae30 \ud798\ub4e4\ub2e4. \uc774\ub97c methodoliges\ub97c \ub9cc\ub4e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud55c\ub2e4. - \uc5ec\ub7ec\ubc88 \ubc18\ubcf5\ud574\uc11c \uc218\ud589\ud55c\ub2e4. - \ud1b5\uacc4\ub97c \ub0b8\ub2e4: mean, variance - \uc544\uc6c3\ub77c\uc774\uc5b4\ub97c \uc81c\uac70\ud55c\ub2e4. - ensuring steady state(warm-up) - preventing anomalies(GC, JIT compilation, aggresive optimizations) ScalaMeter\ub294 JVM \uc744 \uc704\ud574 benchmarking\uacfc performance regression testing\uc744 \ud574\uc900\ub2e4. - performance regression testing: \uc774\uc804\uc758 run\uacfc \ube44\uad50\ud574\uc11c performance\ub97c \ube44\uad50 - benchmarking: \ud604\uc7ac \ud504\ub85c\uadf8\ub7a8 \ud639\uc740 \ud504\ub85c\uadf8\ub7a8\uc758 \uc77c\ubd80\ubd84\uc758 performance\ub97c \uce21\uc815 \uc774\ubc88 \uac15\uc88c\uc5d0\uc11c\ub294 benchmarking\uc5d0 \uc9d1\uc911\ud55c\ub2e4. Using ScalaMeter first, add dependency libraryDependencies += \"com.storm-enroute\" %% \"scalameter-core\" % \"0.6\" \uadf8\ub9ac\uace0 \uc774\ub97c import\ud574\uc11c \uc0ac\uc6a9 import org.scalameter._ val time = measure { (0 until 1000000).toArray } println(s\"Array initialization time: $time ms\") JVM Warmup \uc704\uc758 \uc608\ub294 two consecutive runs of program \ud560 \ub54c different running times\ub97c \ubcf4\uc600\ub2e4 JVM program\uc774 \uc2dc\uc791\ud560 \ub54c warmup\uc744 \ud558\ub294 \uc2dc\uac04\uc774 \ud544\uc694\ud558\ub2e4. \uadf8\ub9ac\uace0 \uadf8 \uc774\ud6c4\uc5d0\ub294 maimum performance\ub97c \ubc1c\ud718\ud55c\ub2e4. - \uccab \ubc88\uca30\ub85c \ud504\ub85c\uadf8\ub7a8\uc774 interpreted mode\ub85c run \ub41c\ub2e4. - \uadf8\ub9ac\uace0 \ud504\ub85c\uadf8\ub7a8\uc758 \uc77c\ubd80\ubd84\uc774 machine code\ub85c compile\ub41c\ub2e4. - \ud6c4\uc5d0 JVM\uc740 additional dynamic optimizations\ub97c \uc801\uc6a9\ud560 \uc9c0\ub97c \uacb0\uc815\ud55c\ub2e4. - eventually, \ud504\ub85c\uadf8\ub7a8\uc774 steady state\ub85c \ub41c\ub2e4. \uc6b0\ub9ac\ub294 \ud56d\uc0c1 steady state program performance(warm up\uc774 \ub41c \ud6c4\uc5d0 \ud504\ub85c\uadf8\ub7a8 \uc18d\ub3c4)\ub97c \uce21\uc815\ud558\uace0 \uc2f6\ub2e4. ScalaMeter\uc758 Warmer objects\ub294 \uc774\ub97c benchmark \ud560 \uc218 \uc788\ub2e4. import org.scalameter._ val time = withWarmer(new Warmer.Default) measure { (0 until 1000000).toArray } configuration\ub3c4 \ucd94\uac00\ud560 \uc218 \uc788\ub2e4. val time = config( Key.exec.minWarmupRuns -> 20, Key.exec.maxWarmupRuns -> 60, Key.verbose -> true ) withWarmer(new Warmer.Default) measure { (0 until 1000000).toArray } ScalaMeter\ub294 running time\uc774\uc678\uc5d0\ub3c4 \uc544\ub798\uc758 \ub9ac\uc2a4\ud2b8\uc5d0 \uc18d\ud558\ub294 \uac83\ub4e4\uc744 \uce21\uc815\ud560 \uc218 \uc788\ub2e4. - Measurer.Default \u2013 plain running time - IgnoringGC \u2013 running time without GC pauses - OutlierElimination \u2013 removes statistical outliers - MemoryFootprint \u2013 memory footprint of an object. object\uac00 memory\uc5d0\uc11c \uc5bc\ub9c8\ub9cc\ud07c\uc758 \uc790\ub9ac\ub97c \ucc28\uc9c0\ud558\ub294\uc9c0 \uce21\uc815 - GarbageCollectionCycles \u2013 total number of GC pauses - newer ScalaMeter versions can also measure method invocation counts and boxing counts val time = withWarmer(new Measurer.MemoryFootprint) measure { (0 until 1000000).toArray } steady state\uc774\uc804\uc77c\ub54c\ub294 \uac12\uc774 -\ub85c \ub098\uc62c \uc218 \uc788\ub2e4. 4000\uadfc\ucc98\uc758 \uac12\uc774 \ub098\uc624\ub294\ub370 \uc774\ub294 4000KB = 4MB\ub97c \ucc28\uc9c0\ud55c\ub2e4\ub294 \uc758\ubbf8\uc774\ub2e4. Week2 fold\uc640 reduce\uc758 \ucc28\uc774\ub294 \ucd08\uae30\uac12\uc744 \uac16\ub290\ub0d0 \uc544\ub2c8\uba74 \ube48 List\ub97c \ucd08\uae30\uac12\uc73c\ub85c \uc0ac\uc6a9\ud558\ub290\ub0d0\uc758 \ucc28\uc774\ub2e4. left\uc640 Right\uc758 \ucc28\uc774\ub294 operation\uc744 \uc5b4\ub290 \ubc29\ud5a5\uc73c\ub85c \uc801\uc6a9\ud560 \uac83\uc778\uc9c0\uc758 \ucc28\uc774\ub2e4. List(1,3,8).foldLeft(100)((s,x) => s - x) == ((100 - 1) - 3) - 8 == 88 List(1,3,8).foldRight(100)((s,x) => s - x) == 1 - (3 - (8-100)) == -94 List(1,3,8).reduceLeft((s,x) => s - x) == (1 - 3) - 8 == -10 List(1,3,8).reduceRight((s,x) => s - x) == 1 - (3 - 8) == 6 sequential version of scanLeft def scanLeft[A](inp: Array[A], a0: A, f: (A,A) => A, out: Array[A]): Unit = { out(0)= a0 var a = a0 var i= 0 while (i < inp.length) { a= f(a,inp(i)) i= i + 1 out(i)= a } } def scanLeft[A](inp: Array[A], a0: A, f: (A,A) => A, out: Array[A]) = { val fi = { (i:Int,v:A) => reduceSeg1(inp, 0, i, a0, f) } mapSeg(inp, 0, inp.length, fi, out) val last = inp.length - 1 out(last + 1) = f(out(last), inp(last)) } Week3 Data-Parallelism Previously, we learned about task-parallel programming. A form of parallelization that distributes execution processes across computing nodes. We know how to express parallel programs with task and parallel constructs. Next, we learn about the data-parallel programming. A form of parallelization that distributes data across computing nodes. The simplest form of data-parallel programming is the parallel for loop. def initializeArray(xs: Array[Int])(v: Int): Unit = { for (i <- (0 until xs.length).par) { xs(i) = v } } Range\uc5d0 .par \ub97c \ubd99\uc774\uba74 parallel range \ub85c \ubcc0\ud658\ub41c\ub2e4. parallel loop\ub294 different processors\uc5d0\uc11c concurrently with each other\ub85c \uc2e4\ud589\ub420 \uac83\uc774\ub2e4 parallel loop\ub294 \uc5b4\ub5a0\ud55c \uac12\ub3c4 \ub9ac\ud134\ud558\uc9c0 \uc54a\ub294\ub2e4 \uadf8\ub807\uae30\uc5d0 \uc774\uc640 interact\ud560 \uc218 \uc788\ub294 \uc720\uc77c\ud55c \ubc29\ubc95\uc740 assign\ubc16\uc5d0 \uc5c6\ub2e4. \uc774\ub294 side effect\ub97c \uc720\ubc1c\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud55c\ub2e4. = not very functional \ud558\ub2e4. Workload \ub780 \uac01\uac01\uc758 input element\uc5d0 \ub300\ud574\uc11c \uc774\ub97c \uc2e4\ud589\ud558\ub294\ub370 \ud544\uc694\ud55c amount of work \uc774\ub2e4. different data-parallel programs\ub294 different workloads\ub97c \uac16\ub294\ub2e4. (= input element\uc5d0 \ub530\ub77c\uc11c \uc218\ud589\ub7c9\uc774 \ub2e4\ub974\ub2e4.) data-parallel scheduler \ub294 \uc774\ub7f0 different workloads\ub97c efficiently balance \ud574\uc8fc\uae30 \ub54c\ubb38\uc5d0 \ud504\ub85c\uadf8\ub798\uba38\uac00 \uc774\ub97c \uc9c1\uc811 \uad00\ub9ac\ud560 \ud544\uc694\ub294 \uc5c6\ub2e4. Scala\uc5d0\uc11c \ub300\ubd80\ubd84\uc758 collection operation\uc740 data-parallel\uc744 \uc9c0\uc6d0\ud55c\ub2e4. (1 until 1000).par .filter(n => n % 3 == 0) .count(n => n.toString == n.toString.reverse) filter\uc640 count\ub3c4 data parallel operation\uc744 \uc9c0\uc6d0\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uba87\uba87 collection operations\ub294 parallelizable \ud558\uc9c0 \uc54a\ub2e4. foldLeft\ub97c \uc774\uc6a9\ud55c sum\uc744 \uc0dd\uac01\ud574\ubcf4\uc790. def sum(xs: Array[Int]): Int = { xs.par.foldLeft(0)(_ + _) } \uc774\ub294 parallel \ud558\uac8c \uc218\ud589\ub418\ub294\uac00? \uc544\ub2c8\ub2e4. \uba3c\uc800 foldLeft\uc758 signature\ub97c \uc0b4\ud3b4\ubcf4\uc790. def foldLeft[B](z: B)(f: (B, A) => B): B \ud568\uc218 f\ub97c \ubcf4\uba74 A\uc640 B \ud0c0\uc785\uc744 \ubc1b\uc544\uc11c B \ud0c0\uc785\uc73c\ub85c \ub9cc\ub4e4\uc5b4 \uc900\ub2e4. \uc774 \uc758\ubbf8\ub294 foldLeft\ub97c \uc5f0\uc18d\uc801\uc73c\ub85c \uc218\ud589\ud560 \ub54c \uba3c\uc800 \uc218\ud589\ud55c foldLeft\uc758 \uacb0\uacfc\ub97c \ubc1b\uace0 \ub098\uc11c\uc57c \ub2e4\uc74c foldLeft\ub97c \uacc4\uc0b0\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud55c\ub2e4.(= \ucc38\uc870 \ud22c\uba85\ud558\uc9c0 \uc54a\ub2e4.) \uc774\uc640 \uac19\uc740 \uc885\ub958\uc758 foldRight, reduceLeft, reduceRight, scanLeft and scanRight\ub294 \ub3d9\uc77c\ud55c \ubb38\uc81c\ub97c \uac16\ub294\ub2e4. \ub9cc\uc57d \uc774\ub97c parallel\ud558\uac8c \ud558\uace0 \uc2f6\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uc815\uc758\uac00 \ud544\uc694\ud558\ub2e4. def fold(z: A)(f: (A, A) => A): A \uc774\ub97c \uc774\uc6a9\ud558\uba74 max\ub97c \uc544\ub798\uc640 \uac19\uc774 \uad6c\ud604\ud560 \uc218 \uc787\ub2e4. def max(xs: Array[Int]): Int = { xs.par.fold(Int.MinValue)(math.max) } data-parallel operation\uc744 \uc218\ud589\ud560 \ub54c\ub294 \uba87 \uac00\uc9c0 \uc8fc\uc758 \uc0ac\ud56d\uc774 \uc788\ub2e4. same memory locations\ub97c \uc218\uc815\ud558\ub294 \uac83\uc744 \ud53c\ud574\uc57c \ud55c\ub2e4. def intersection(a: GenSet[Int], b: GenSet[Int]): Set[Int] = { val result = mutable.Set[Int]() for (x <- a) if (b contains x) result += x result } intersection((0 until 1000).toSet, (0 until 1000 by 4).toSet) intersection((0 until 1000).par.toSet, (0 until 1000 by 4).par.toSet) \uc704\uc5d0\uc11c .par\ub85c \uc2e4\ud589\ud55c \ucf54\ub4dc\ub294 parallel\ud558\uac8c \ub3d9\uc791\ud558\uba74\uc11c result\ub77c\ub294 Set\uc744 \ubcc0\uacbd\ud55c\ub2e4. \ube44\ub85d \uc5ec\uae30\uc5d0\uc120 \uc624\ub958\uac00 \ubc1c\uc0dd\ud558\uc9c0 \uc54a\uc558\uc9c0\ub9cc \uc774\ub294 side effect\ub97c \ucd08\ub798\ud560 \uc218 \uc788\ub2e4. \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574\uc120 \uc544\ub798\uc640 \uac19\uc740 \ubc29\ubc95\uc774 \uc788\ub2e4. 1) use a concurrent collection, which can be mutated by multiple threads import java.util.concurrent._ def intersection(a: GenSet[Int], b: GenSet[Int]) = { val result = new ConcurrentSkipListSet[Int]() for (x <- a) if (b contains x) result += x result } intersection((0 until 1000).toSet, (0 until 1000 by 4).toSet) intersection((0 until 1000).par.toSet, (0 until 1000 by 4).par.toSet) 2) correct combinator\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 filter\ub97c \uc0ac\uc6a9\ud558\uba74 \ub41c\ub2e4. def intersection(a: GenSet[Int], b: GenSet[Int]): GenSet[Int] = { if (a.size < b.size) a.filter(b(_)) else b.filter(a(_)) } intersection((0 until 1000).toSet, (0 until 1000 by 4).toSet) intersection((0 until 1000).par.toSet, (0 until 1000 by 4).par.toSet) data-parallel operation \uc218\ud589 \uc911\uc77c \ub54c parallel collection\uc744 \uc218\uc815\ud558\uba74 \uc808\ub300 \uc548\ub41c\ub2e4. \ub610\ud55c \uc218\uc815\ub41c parallel collection\uc744 \uc77d\ub294 \uac83\ub3c4 \uc808\ub300 \uc548\ub41c\ub2e4. val graph = mutable.Map[Int, Int]() ++= (0 until 100000).map(i => (i, i + 1)) graph(graph.size - 1) = 0 for ((k, v) <- graph.par) graph(k) = graph(v) val violation = graph.find({ case (i, v) => v != (i + 2) % graph.size }) println(s\u201dviolation: $violation\u201d) \uc704 \ucf54\ub4dc\ub294 \ud504\ub85c\uc138\uc2a4\ub97c \uc9c4\ud589\ud558\uba74\uc11c collection\uc744 \uc218\uc815\ud55c\ub2e4. \uc704\uc758 \uc624\ub958\ub97c \ud53c\ud558\uae30 \uc704\ud574 \uc544\ub798\uc758 collection\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. TrieMap Collection\uc740 \uc774\ub7f0 rule\uc5d0 \ub300\ud574 exception\uc744 \uac16\uace0 \uc788\ub2e4. val graph = concurrent.TrieMap[Int, Int]() ++= (0 until 100000).map(i => (i, i + 1)) graph(graph.size - 1) = 0 val previous = graph.snapshot() for ((k, v) <- graph.par) graph(k) = previous(v) val violation = graph.find({ case (i, v) => v != (i + 2) % graph.size }) println(s\u201dviolation: $violation\u201d)","title":"Coursera/parallel-programming"},{"location":"scala/parallel-programming/#parallel-programming","text":"","title":"parallel programming"},{"location":"scala/parallel-programming/#week1","text":"Parallel computing\uc740 many caculation\uc744 \ub3d9\uc2dc\uc5d0 computation \ud558\ub294 \uac83\uc774\ub2e4. Basic principle: computation\uc740 smaller subproblems\ub85c \ub098\ub20c \uc218 \uc788\uc73c\uba70 \uac01\uac01\uc740 \ub3d9\uc2dc\uc5d0 solved \ub420 \uc218 \uc788\ub2e4. Assumption: parallel hardware\uac00 \uc788\ub2e4\uba74 \uc774\ub7ec\ud55c computation\uc744 parallel\ud558\uac8c \uc2e4\ud589\ud560 \uc218 \uc788\ub2e4. 20\uc138\uae30\uc5d0 IBM\uc774 first commercial parallel computers\ub97c \ub9cc\ub4e4\uc5b4 \ub0c8\ub2e4. \uadf8 \ub2f9\uc2dc\uc5d0\ub294 \uc778\uae30\uac00 \uc5c6\uc5c8\uc9c0\ub9cc \ud604\uc7ac\ub294 computing performance\uac00 \uc99d\uac00\ud558\uba74\uc11c \uc778\uae30\uac00 \ub9ce\uc544\uc84c\ub2e4. \uc65c Parallel Computing\uc778\uac00? Parallel programming\uc740 sequential programming\ubcf4\ub2e4 \ub354 \uc5b4\ub835\ub2e4. - \uba87 \uba87 computation\uc740 divide\uac00 \ubd88\uac00\ub2a5\ud558\uac70\ub098 \uc5b4\ub835\ub2e4. - error \uc7a1\uae30 \ud798\ub4e4\ub2e4. \ub610 \ub9ce\uc740 \uc0c8\ub85c\uc6b4 \ud0c0\uc785\uc758 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Speedup is only reason why we bother paying for this complexity Parallelism and concurrency \ub294 \uad00\ub828\ub41c \ucee8\uc149\uc774\ub2e4. Parallel Programming\uc740 parallel hardware\ub97c \uc0ac\uc6a9\ud558\uc5ec \uacc4\uc0b0\uc744 \ub354 \ube68\ub9ac \uc218\ud589\ud55c\ub2e4. Efficiency \uac00 \ucd5c\ub300 \uad00\uc2ec\uc0ac\uc774\ub2e4. mainly concerned algorithmic problems, numerical computation or big data applications. \uc544\ub798\uc758 \ubb38\uc81c\uc5d0 \uad00\uc2ec\uc744 \uac16\ub294\ub2e4. \uc5b4\ub5bb\uac8c divide into sub problems \ud558\uace0 \uc5b4\ub5bb\uac8c \ub3d9\uc2dc\uc5d0 \uc218\ud589\ud558\ub290\ub0d0\uac00 \ubb38\uc81c\uc774\ub2e4. \ucd5c\ub300\ud55c\uc758 \uc18d\ub3c4\ub97c \uc704\ud574 \uc5b4\ub5bb\uac8c \uc790\uc6d0\uc744 \ud65c\uc6a9\ud560\uae4c\ub3c4 \ubb38\uc81c\uc774\ub2e4. Concurrent Program\uc740 \ub3d9\uc2dc\uc5d0 multiple execution\uc744 \uc218\ud589\ud560 \ub54c\ub3c4 \uc788\uc9c0\ub9cc \uc544\ub2d0 \ub54c\ub3c4 \uc788\ub2e4. Modularity, responsiveness or maintainability\uac00 \ucd5c\ub300 \uad00\uc2ec\uc0ac\ub2e4. targeted writing asynchronous applications. such as webservers, user interfaces or databases. \ub610\ud55c \uc544\ub798\uc758 \ubb38\uc81c\uc5d0 \uad00\uc2ec\uc744 \uac16\ub294\ub2e4. \uc5b8\uc81c \uacc4\uc0b0\uc744 \uc218\ud589\ud574\uc57c \ud560\uae4c? when and how \ub450 \uac1c\uc758 concurrent execution\uc774 \uc815\ubcf4\ub97c \uad50\ud658\ud574\uc57c \ud560\uae4c? \uc5b4\ub5bb\uac8c shared resources(file, db\ub4f1) \uc811\uadfc\uc744 \uad00\ub9ac\ud574\uc57c \ud558\ub294\uac00? \ub450 \uac1c\ub150\uc774 \uacf5\uc720\ud558\ub294 \uc0ac\ud56d\uc740 \uc788\uc9c0\ub9cc superset\uc740 \uc5c6\ub2e4. Parallelism manifests itself at di\ufb00erent granularity levels. - bit-level parallelism: processing multiple bits of data in parallel - instruction-level parallelism: executing di\ufb00erent instructions from the same instruction stream in parallel - task-level parallelism: executing separate instruction streams in parallel bit-level\uacfc intstruction-level\uc740 processor\uac00 \uc54c\uc544\uc11c \uc218\ud589\ud574\uc900\ub2e4. \uc6b0\ub9ac\uac00 \uc774\ubc88\uc5d0 \ub2e4\ub8f0 \uac83\uc740 task-level\uc774\ub2e4. parallel h/w\uc5d0\ub294 \ub2e4\uc591\ud55c \ud615\ud0dc\uac00 \uc788\ub2e4. - multi-core processors - symmetric multiprocessors(SMP): multiple identical processors\uac00 bus\uc640 \uc5f0\uacb0\ub418\uc5b4 memory\ub97c share\ud558\ub294 \uac83\uc774\ub2e4. - graphic processing unit: originally intended for graphics processing. \uae30\ubcf8\uc801\uc73c\ub860 execute program\uc744 \ud558\uc9c4 \uc54a\uc9c0\ub9cc host processor\uc5d0 \uc758\ud574\uc11c \uc218\ud589\ud560 \uc218 \uc788\ub2e4. - field-programmable gate arrays(FPGA): these are programmed in hardware description languages, such as Verilog and HTL - computer clusters: groups of computers connected via a network. not sharing memory \uc774\ubc88 \uacfc\uc815\uc5d0\uc11c\ub294 multi-core processors\uc640 symmetric multiprocessor\ub97c \uc911\uc810\uc801\uc73c\ub85c \ub2e4\ub8ec\ub2e4.","title":"Week1"},{"location":"scala/parallel-programming/#parallelism-on-the-jvm-i","text":"There are many forms of parallelism in the wild. From GPUs and custom parallel hardware, over multiprocessors, and multicore systems to distributed computer clusters. Each parallel programming model is tailored to a specific use case, and has certain associated characteristics. OS\ub780? We will say that an operating system is a piece of software that manages hardware and software resources, and schedules program execution. Process - OS\uc5d0\uc11c \uc2e4\ud589\ub418\ub294 program\uc758 instance same program\uc740 \ud558\ub098\uc758 process\uc5d0\uc11c \uc5ec\ub7ec \ubc88 \uc2dc\uc791\ub420 \uc218 \uc788\uace0, same OS\uc5d0\uc11c \ub3d9\uc2dc\uc5d0 \uc218\ud589\ub420 \uc218\ub3c4 \uc788\ub2e4. OS\ub294 \uc5ec\ub7ec process\ub97c \ud55c\uc815\ub41c CPU\uc5d0\uc11c \uc2e4\ud589\ud574\uc57c \ud55c\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 time slices of excution\uc774 \uc788\ub2e4. \uc774\ub7ec\ud55c \ub9e4\ucee4\ub2c8\uc998\uc744 multitasking\uc774\ub77c\uace0 \ud55c\ub2e4. \uadf8\ub9ac\uace0 \uac01 process\ub4e4\uc740 \uba54\ubaa8\ub9ac\ub97c \uc9c1\uc811\uc801\uc73c\ub85c \uacf5\uc720\ud560 \uc218 \uc5c6\ub2e4. = isolated memory area\uac00 \uc788\ub2e4. (\ub418\ub294 OS\uac00 \uc788\uae34 \ud558\ub514\u001d) \uadf8\ub798\uc11c! Threads\uac00 \ud544\uc694\ud558\ub2e4. each process\ub294 multiple independent concurrency units called threads\ub97c \uac16\ub294\ub2e4. using thread\uc758 \ub450 \uac00\uc9c0 \uc7a5\uc810\uc774 \uc788\ub2e4. - thread\ub294 \ud504\ub85c\uadf8\ub7a8 \ub0b4\uc5d0\uc11c programmatically started\ud560 \uc218 \uc788\ub2e4. = structure parallel computation\uc744 \ub9cc\ub4e4\uae30 \uc27d\ub2e4. \ud504\ub85c\uc138\uc11c\ubcf4\ub2e4. - \ub354 \uc911\uc694\ud55c \uac83\uc740 threads\ub294 same memory address space\ub97c \uacf5\uc720\ud55c\ub2e4. \uac01 thread\ub294 program counter(current executed method\uc758 \uba54\ubaa8\ub9ac\ub0b4 \uc704\uce58)\uc640 program stack(memory\ub0b4 \uacf5\uac04\uc73c\ub85c\uc11c \uc218\ud589\ud560 methods\ub97c \uac16\ub294\ub2e4.)\uc744 \uac16\ub294\ub2e4. JVM\uc5d0\uc11c \uc2e4\ud589\ub418\ub294 threads\ub294 each other's program stacks\ub97c modify \ud560 \uc218 \uc5c6\ub2e4. Each JVM process starts with a main thread. \ubcf4\ud1b5\uc758 \ud504\ub85c\uadf8\ub7a8\uc740 main thread\ub9cc \uc0ac\uc6a9\ud55c\ub2e4. parallel programming\uc5d0\uc11c\ub294 \uacc4\uc0b0\uc744 \uc704\ud574 \uc5ec\ub7ec threads\ub97c \uc0ac\uc6a9\ud55c\ub2e4. additional threads\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc544\ub798\uc758 \uc2a4\ud15d\uc744 \ub530\ub77c\uc57c \ud55c\ub2e4. 1. Define a Thread subclass 2. Instantiate a new Thread object. 3. Call start on the Thread object. class HelloThread extends Thread { override def run() { println(\"Hello World!\") } } val t = new HelloThread t.start() // main thread\uc640 HelloThread\ub97c \uc2e4\ud589\uc2dc\ud0a8\ub2e4. t.join() // join\uc740 hellothread\uac00 \ub05d\ub0a0\ub54c \uae4c\uc9c0 \uc2e4\ud589\uc744 \uba48\ucd98\ub2e4. \uc704\uc758 \ucf54\ub4dc\ub294 \uc798 \ub3d9\uc791\ud558\ub294 \uac83 \ucc98\ub7fc \ubcf4\uc778\ub2e4. class HelloThread2 extends Thread { override def run() { println(\"Hello\") println(\"World!\") } } def main(): Unit = { val t = new HelloThread2() val s = new HelloThread2() t.start() s.start() t.join() s.join() } main() \uc704\uc758 \ucf54\ub4dc\ub294 \uc218\ud589\ud560 \ub54c\ub9c8\ub2e4 \ub2e4\ub978 \uacb0\uacfc\uac00 \ub098\uc62c \uc218\ub3c4 \uc788\ub2e4. \uc65c\ub0d0\uba74 thread\uac00 \ub3d9\uc2dc\uc5d0 \uc218\ud589\ub418\uae30 \ub54c\ubb38\uc5d0 \uc5b4\uca54\ub54c\ub294 Hello Hello World World\uac00 \ub098\uc62c \uac83\uc774\uace0 \uc5b4\ub5a4\ub54c\ub294 Hello World Hello World\uac00 \ub098\uc62c \uac83\uc774\ub2e4. (two threads can overlap) \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \ub54c\ub54c\ub85c Hello World \uc758 \uc21c\uc11c\ub294 \ubcf4\uc7a5\ud558\uace0 \uc2f6\uc744 \uc218 \uc788\ub2e4(\uc0ac\uc774\uc5d0 \ub2e4\ub978 thread\uac00 \uc2e4\ud589\ub418\uc9c0 \uc54a\ub3c4\ub85d). Atomicity! An operation is atomic if it appears as if it occurred instantaneously from the point of view of other threads. \uc608) private var uidCount = 0L def getUniqueId(): Long = { uidCount = uidCount + 1 uidCount } \uc704 \ud568\uc218\ub294 uniqueId\ub97c \ub9ac\ud134\ud558\ub294 \ud568\uc218\uc774\ub2e4. \uadf8\ub807\uae30\uc5d0 \ub9e4\ubc88 \uace0\uc720\ud55c \uac12\uc744 \ub9cc\ub4e4\uc5b4 \ub0b4\uc57c \ud558\uba70 \uc774\ub97c \uc704\ud574 private var\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \ud604\uc7ac \uc774 \ud568\uc218\ub294 atomic \ud558\uc9c0 \uc54a\ub2e4. \uc65c\ub0d0\uba74 \uc5ec\ub7ec \uc4f0\ub808\ub4dc\uac00 \ub3d9\uc2dc\uc5d0 \uc0ac\uc6a9\ub41c\ub2e4\uace0 \ud560 \ub54c \uc704\uc5d0\uc11c \uc598\uae30\ud588\ub358 overlap\uc774 \ubc1c\uc0dd\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \uadf8\ub807\uac8c \ub418\uba74 id\uac00 1,2,3,4 \uc21c\ucc28\uc801\uc73c\ub85c \uac00\ub294\uac83\uc774 \uc544\ub2c8\ub77c 1,2,2,4,5,8,9 \uac19\uc774 \ub420 \uc218 \uc788\ub2e4. var uidCount = 0L def getUniqueId(): Long = { uidCount = uidCount + 1 uidCount } def startThread() = { val t = new Thread { override def run(): Unit = { val uids = for (i <-0 until 10) yield getUniqueId() println(uids) } } t.start() t } startThread() startThread() \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 java\uc640 scala\ub294 synchronized block\uc744 \uc9c0\uc6d0\ud55c\ub2e4. object x\uc758 synchronized call \uc774\ud6c4\uc758 code block\uc740 \ub3d9\uc2dc\uc5d0 \uc5ec\ub7ec \uc4f0\ub808\ub4dc\uc5d0\uc11c \uc2e4\ud589\ub420 \uc218 \uc5c6\ub2e4. (JVM\uc5d0\uc11c\ub294 synchronized object\ub97c monitor\ub77c\uace0 \uc9c0\uc815\ud55c\ub2e4. \uadf8\ub9ac\uace0 \ub098\uc11c \ub2e4\ub978 \uc4f0\ub808\ub4dc\uac00 \uc811\uadfc\ud558\ub824\uace0\ud558\uba74 block\ud55c\ub2e4.) private val x = new AnyRef {} private var uidCount = 0L def getUniqueId(): Long = x.synchronized { uidCount = uidCount + 1 uidCount }","title":"Parallelism on the JVM I"},{"location":"scala/parallel-programming/#parallelism-on-the-jvm-ii","text":"\ub9cc\uc57d \uc740\ud589 \uc5c5\ubb34 \uac19\uc774 source\uc640 target\uc744 \uac19\uc774 block \ud574\uc57c \ud560 \ub54c\ub294 \uae30\uc874\uc758 synchronized\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc5c6\ub2e4. \uc774\ub7f4\ub550 more fine grained synchronization\uc744 \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4. class Account(private var amount: Int = 0) { def transfer(target: Account, n: Int) = this.synchronized { target.synchronized { this.amount -= n target.amount += n } } } A1\uc774 source A2\uac00 target\uc774\ub77c\uace0 \ud560 \ub54c thread\ub294 A1\uc5d0 monitor\ub97c \uc8fc\uace0 A1,A2\ub97c \ubb36\uc5b4\uc11c monitor\ub97c \ud558\ub098 \ub610 \uc900\ub2e4. class Account(private var amount: Int = 0) { def transfer(target: Account, n: Int) = this.synchronized { target.synchronized { this.amount -= n target.amount += n } } } def startThread(a: Account, b: Account, n: Int) = { val t = new Thread { override def run(): Unit = { for (i <- 0 until n) { a.transfer(b, 1) } } } t.start() t } val a1 = new Account(500000) val a2 = new Account(700000) val t = startThread(a1, a2, 150000) val s = startThread(a2, a1, 150000) t.join() // deadlock!! s.join() thread t\uac00 \ub05d\ub098\uc9c0 \uc54a\uc73c\uba74\uc11c deadlock\uc774 \uac78\ub9b0\ub2e4. deadlock\uc774\ub780 2\uac1c \ud639\uc740 \uadf8 \uc774\uc0c1\uc758 threads\ub4e4\uc774 \uc774\ubbf8 \uc810\uc720\uc911\uc778 resources \ub193\uc9c0 \uc54a\uc740 \ucc44 \uc11c\ub85c\uac00 \ub05d\ub098\uae38 \uae30\ub2e4\ub9ac\ub294 \uc0c1\ud669\uc774\ub2e4. \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c \ud55c \uac00\uc9c0 \ubc29\ubc95\uc740 same order\ub85c resource\ub97c \uc694\uccad\ud558\ub294 \uac83\uc774\ub2e4. \uc774\ub54c resource\uc5d0\ub294 \uc6b0\uc120\uc21c\uc704\uac00 \uc788\ub2e4\ub294 \uac00\uc815\uc774\ub2e4. val uid = getUniqueUid() private def lockAndTransfer(target: Account, n: Int) = this.synchronized { target.synchronized { this.amount -= n target.amount += n } } def transfer(target: Account, n: Int) = if (this.uid < target.uid) this.lockAndTransfer(target, n) else target.lockAndTransfer(this, -n) \uc704\uc758 \ucf54\ub4dc\ub294 \ub9e4 account object\ub9c8\ub2e4 uid\ub97c \ubd80\uc5ec\ud574\uc11c \uc774\uc5d0 \ub530\ub77c \uc6b0\uc120\uc21c\uc704\ub97c \ub9e4\uae30\ub294 \ubc29\ubc95\uc774\ub2e4. Memory Model\uc774\ub77c\ub294 \uac83\uc774 \uc788\ub2e4. Memory Model\uc774\ub780 threads\uac00 accessing shared memory\ub97c interact \ud560 \ub54c\uc758 \uaddc\uce59\ub4e4\uc744 \uc124\uba85\ud574 \ub193\uc740 \uac83\uc774\ub2e4. Java Memory Model\uc740 memory model for JVM\uc774\ub2e4. \ubaa8\ub4e0 \ub8f0\uc744 \ub2e4 \uc124\uba85\ud560 \uc21c \uc5c6\uc9c0\ub9cc \uae30\uc5b5\ud574\uc57c \ud560 \ub450 \uac00\uc9c0\uac00 \uc788\ub2e4. 1. \uba54\ubaa8\ub9ac\ub0b4 separate locations\uc5d0 writing\ud558\ub294 two threads\ub294 synchronization\uc774 \ud544\uc694 \uc5c6\ub2e4. 2. \ub610 \ub2e4\ub978 thread Y\ub97c \ud638\ucd9c\ud558\ub294 thread X\ub294 join\uc774 \ub9ac\ud134\ub41c \ud6c4 thread Y\uc758 \ubaa8\ub4e0 writes\ub97c observe\ud558\ub3c4\ub85d \ubcf4\uc7a5\ud574\uc57c \ud55c\ub2e4.","title":"Parallelism on the JVM II"},{"location":"scala/parallel-programming/#running-computations-in-parallel","text":"Example: computing p-norm vector (a1, a2)\uc5d0 \ub300\ud55c 2-norm\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \uacc4\uc0b0 \ud55c\ub2e4. (a 1 2 + a 2 2 ) 1/2 array\uc5d0 \ub300\ud55c sum of power\ub294 \uc544\ub798\uc640 \uac19\uc774 \uad6c\ud560 \uc218 \uc788\ub2e4. def power(x: Int, p: Double): Int = math.exp(p * math.log(abs(x))).toInt def sumSegment(a: Array[Int], p: Double, s: Int, t: Int): Int = { var i= s var sum: Int = 0 while (i < t) { sum= sum + power(a(i), p) i= i + 1 } sum } \uc774 \uacc4\uc0b0\uc744 2 \uac1c\ub85c \ub098\ub20c \uc218 \uc788\ub2e4. 0~n \uae4c\uc9c0\uc758 \ud569 + n~m \uae4c\uc9c0\uc758 \ud569 def pNormTwoPart(a: Array[Int], p: Double): Int = { val m = a.length / 2 val (sum1, sum2) = (sumSegment(a, p, 0, m), sumSegment(a, p, m, a.length)) power(sum1 + sum2, 1/p) } \ud558\uc9c0\ub9cc \uc774\ub294 sequential\ud55c \ubc29\ubc95\uc774\ub2e4. sum1\uc774 \uacc4\uc0b0 \ub2e4 \ub418\uace0 \ub098\uc11c sum2\uac00 \uacc4\uc0b0\ub41c\ub2e4. parallel\ub85c\ub294 \uc5b4\ub5bb\uac8c \ud560\uae4c? def pNormTwoPart(a: Array[Int], p: Double): Int = { val m = a.length / 2 val (sum1, sum2) = parallel(sumSegment(a, p, 0, m), sumSegment(a, p, m, a.length)) power(sum1 + sum2, 1/p) } \uc55e\uc5d0 parallel\uc744 \ubd99\uc774\uba74 \ub41c\ub2e4. \uc774\ub807\uac8c \ub418\uba74 sum1\uacfc sum2\uac00 \ub3d9\uc2dc\uc5d0 \uacc4\uc0b0\ub41c\ub2e4. \ub9cc\uc57d 4(2 + 2)\uac1c\ub85c \ubd84\ub9ac\ud558\uba74 \uc5b4\ub5a8\uae4c? val m1 = a.length/4 val m2 = a.length/2 val m3 = 3*a.length/4 val ((sum1, sum2),(sum3,sum4)) = parallel( parallel(sumSegment(a, p, 0, m1), sumSegment(a, p, m1, m2)), parallel(sumSegment(a, p, m2, m3), sumSegment(a, p, m3, a.length)) ) generalize\ub294 \uc5b4\ub5bb\uac8c \ud560\uae4c? recursion\uc744 \uc0ac\uc6a9\ud558\uba74 \uac00\ub2a5\ud558\ub2e4! def pNormRec(a: Array[Int], p: Double): Int = power(segmentRec(a, p, 0, a.length), 1/p) // like sumSegment but parallel def segmentRec(a: Array[Int], p: Double, s: Int, t: Int) = { if (t - s < threshold) sumSegment(a, p, s, t) // small segment: do it sequentially else { val m = s + (t - s)/2 val (sum1, sum2) = parallel(segmentRec(a, p, s, m), segmentRec(a, p, m, t)) sum1 + sum2 } } \uadf8\ub807\ub2e4\uba74 signature of parallel\uc740 \uc5b4\ub5bb\uac8c \ub420\uae4c def parallel[A, B](taskA: => A, taskB: => B): (A, B) = { ... } \ud56d\uc0c1 \uac19\uc740 value\ub97c \ub9ac\ud134\ud574\uc57c \ud55c\ub2e4. (a,b) \ubcf4\ub2e4 parallel(a, b)\uac00 \ube60\ub974\ub2e4. by name\uc73c\ub85c argumentds\ub97c \ubc1b\ub294\ub2e4. \uc65c call by name\uc73c\ub85c \ud574\uc57c \ud560\uae4c? def parallel1[A, B](taskA: A, taskB: B): (A, B) = { ... } val (va, vb) = parallel1(a, b) \uc704\ub294 call by value\ubc84\uc804\uc774\ub2e4. \uc774\ub807\uac8c \ub418\uba74 \uc989\uc2dc \ud3c9\uac00\uac00 \ub418\uae30 \ub54c\ubb38\uc5d0 val (va, vb) = (a, b) \uac00 \ubc14\ub85c \uacc4\uc0b0\ub41c\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \uc774\ub294 sequential\ud558\uac8c \uacc4\uc0b0\ub418\ubc84\ub9b0 \ud6c4 \uadf8 \ub2e4\uc74c\ubd80\ud130 parallel\ud558\uac8c \uacc4\uc0b0\ub41c\ub2e4. (Because the parameters of parallel1 are call by value, a and b are evaluated sequentially in the second case, not in parallel as in the first case.) \ub2e4\ub978 \ucf54\ub4dc\ub97c \ubcf4\uc790. def sum1(a: Array[Int], p: Double, s: Int, t: Int): Int = { var i= s var sum: Int = 0 while (i < t) { sum= sum + a(i) // no exponentiation! i= i + 1 } sum } val ((sum1, sum2),(sum3,sum4)) = parallel( parallel(sum1(a, p, 0, m1), sum1(a, p, m1, m2)), parallel(sum1(a, p, m2, m3), sum1(a, p, m3, a.length)) ) \uc774 \ucf54\ub4dc\ub294 sumSegment\uc640 \ub2e4\ub974\uac8c speedup\uc744 \uc5bb\uae30\uac00 \ud798\ub4e4\ub2e4. \uc65c \uadf8\ub7f4\uae4c? Memory Bottleneck!! Array\ub294 random access memory\uc5d0 \uc800\uc7a5\ub41c\ub2e4. \uc6b0\ub9ac\uac00 multiple processor\ub97c \uac16\uace0 \uc788\ub354\ub77c\ub3c4 memory\ub294 shared\ub41c\ub2e4. \uadf8 \ub9d0\uc778 \uc989\uc2a8 \uacc4\uc0b0\uc2dc\uac04\uc740 \ud56d\uc0c1 \uba54\ubaa8\ub9ac\ub85c\ubd80\ud130 \ub370\uc774\ud130\ub97c fetch\ud558\ub294 \uc18d\ub3c4 \uc774\uc0c1\uc774 \ub41c\ub2e4. opportuniteis for spped-up\uc744 \uace0\ub824\ud560 \ub54c\ub294 number of cores \ubfd0\ub9cc \uc544\ub2c8\ub77c \ub2e4\ub978 shared resources(memory \ub4f1)\uc5d0\uc11c paralleism\uc774 \uac00\ub2a5\ud55c\uc9c0 \uc54c\uc544\uc57c \ud55c\ub2e4.","title":"Running Computations in Parallel"},{"location":"scala/parallel-programming/#monte-carlo-method-to-estimate-pi","text":"import scala.util.Random def mcCount(iter: Int): Int = { val randomX = new Random val randomY = new Random var hits = 0 for (i <- 0 until iter) { val x = randomX.nextDouble // in [0,1] val y = randomY.nextDouble // in [0,1] if (x*x + y*y < 1) hits = hits + 1 } hits } def monteCarloPiSeq(iter: Int): Double = 4.0 * mcCount(iter) / iter parallel \ubc84\uc804 def monteCarloPiPar(iter: Int): Double = { val ((pi1, pi2), (pi3, pi4)) = parallel( parallel(mcCount(iter/4), mcCount(iter/4)), parallel(mcCount(iter/4), mcCount(iter - 3*(iter/4))) ) 4.0 * (pi1 + pi2 + pi3 + pi4) / iter }","title":"Monte Carlo Method to Estimate Pi"},{"location":"scala/parallel-programming/#first-class-tasks","text":"val (v1, v2) = parallel(e1, e2) \uc704 \ud45c\ud604\uc740 task\ub97c \uc774\uc6a9\ud574 \ub2e4\uc74c\uacfc \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. val t1 = task(e1) val t2 = task(e2) val v1 = t1.join val v2 = t2.join t = task(e)\ub294 computation e \ub97c background \uc5d0\uc11c start \ud568\uc744 \uc758\ubbf8\ud55c\ub2e4. - t\ub294 task\uc774\uace0, \uacc4\uc0b0 e\ub97c \uc218\ud589\ud55c\ub2e4. - \ud604\uc7ac \uacc4\uc0b0\uc740 t\ub85c parallel\ud558\uac8c \uc9c4\ud589\ub41c\ub2e4. - e\uc758 \uacb0\uacfc\ub97c \uc5bb\uace0 \uc2f6\uc744\ub550 t.join \uc744 \uc0ac\uc6a9\ud55c\ub2e4. - t.join\uc740 \uacb0\uacfc\uac00 \uacc4\uc0b0\ub420 \ub54c\uae4c\uc9c0 blocks and wait\ud55c\ub2e4.","title":"First-Class Tasks"},{"location":"scala/parallel-programming/#task-interface","text":"def task(c: => A): Task[A] trait Task[A] { def join: A } join\uc744 implicit\uc73c\ub85c \ubcc0\uacbd\ud558\uba74 \uc544\ub798\uc640 \u314f\u3131\u314c\ub2e4. implicit def getJoin[T](x: Task[T]): T = x.join val ((part1, part2),(part3,part4)) = parallel( parallel(sumSegment(a, p, 0, mid1), sumSegment(a, p, mid1, mid2)), parallel(sumSegment(a, p, mid2, mid3), sumSegment(a, p, mid3, a.length)) ) power(part1 + part2 + part3 + part4, 1/p) \uc704\uc758 parallel p-form\uc740 \uc544\ub798\uc640 \uac19\uc774 task\ub97c \uc774\uc6a9\ud574\uc11c \ubcc0\uacbd\ud560 \uc218 \uc788\ub2e4. val t1 = task {sumSegment(a, p, 0, mid1)} val t2 = task {sumSegment(a, p, mid1, mid2)} val t3 = task {sumSegment(a, p, mid2, mid3)} val t4 = task {sumSegment(a, p, mid3, a.length)} power(t1 + t2 + t3 + t4, 1/p) \uadf8\ub807\ub2e4\uba74 parallel\uc744 task\ub97c \uc774\uc6a9\ud574\uc11c \ub9cc\ub4e4 \uc218 \uc788\uc744\uae4c? def parallel[A, B](cA: => A, cB: => B): (A, B) = { val tB: Task[B] = task { cB } val tA: A = cA (tA, tB.join) } \uc544\ub798\ub294 \uc798\ubabb\ub41c \ubc84\uc804\uc774\ub2e4. // WRONG def parallelWrong[A, B](cA: => A, cB: => B): (A, B) = { val tB: B = (task { cB }).join val tA: A = cA (tA, tB.join) } \ubb34\uc2a8 \uc77c\uc774 \uc77c\uc5b4\ub0a0\uae4c? cA\uc640 cB\uac00 parallel\ud558\uac8c \uc2e4\ud589\ub418\uc9c0 \uc54a\uace0 tB\uac00 \uba3c\uc800 \uacc4\uc0b0\ub418\uae30\ub97c \uae30\ub2e4\ub9b0 \ud6c4 tA\ub97c \uacc4\uc0b0\ud558\uac8c \ub41c\ub2e4.","title":"Task interface"},{"location":"scala/parallel-programming/#how-fast-are-parallel-programs","text":"\uc5b4\ub5bb\uac8c performance\ub97c \uce21\uc815\ud560 \uc218 \uc788\uc744\uae4c? - empirical measurement - asymptotic analysis Asymptotic anlysis\ub294 \uc544\ub798\uc758 \uacbd\uc6b0\uc5d0 algorithms scale\uc744 \uc5b4\ub5bb\uac8c \ud560\uc9c0 \uc774\ud574\ud558\ub294\ub370 \uc911\uc694\ud558\ub2e4. - inputs get larger - we have more h/w parallelism available worst-case bounds\ub97c \uc54c \uc218 \uc788\ub2e4.","title":"How Fast are Parallel Programs?"},{"location":"scala/parallel-programming/#testing-and-benchmarking","text":"testing: \ud504\ub85c\uadf8\ub7a8\uc758 \uc77c\ubd80\ubd84\uc774 \uc758\ub3c4\ud55c \ub300\ub85c \ub3d9\uc791\ud558\ub294 \uc9c0\ub97c \uc54c\uae30 \uc704\ud568 \ub9de\ub294\uc9c0 \uc544\ub2cc\uc9c0\ub97c \ud310\ub2e8\ud558\ub294 binary(true / false) \uacb0\uacfc\ubb3c\uc744 \ub9cc\ub4e4\uc5b4 \ub0b8\ub2e4. benchmarking: \ud504\ub85c\uadf8\ub7a8 \uc77c\ubd80\ubd84\uc758 performance metrics\uc744 \uacc4\uc0b0\ud568 continuous value(\uc18c\uc694 \uc2dc\uac04 \ub4f1)\ub97c \ub9ac\ud134\ud55c\ub2e4. \uc65c Benchmarking\uc744 \ud574\uc57c \ud558\ub290\ub0d0? Parallel programs\ub294 speed up\uc774 \uac00\uc7a5 \uc911\uc694\ud558\ub2e4. \uadf8\ub807\uae30\uc5d0 sequential programs\ubcf4\ub2e4 benchmarking\uc774 \uc911\uc694\ud558\ub2e4. Performance(\ud2b9\ud788\ub098 running time)\ub294 \ub9ce\uc740 \uc694\uc18c\uc5d0 \uad00\ub828\uc788\ub2e4. - processor speed - number of processor - memory access latency and throughput (affects contention) - cache behavior(e.g. false sharing, associativity effects) - runtime behavior(e.g. garbage collection, JIT compilation, thread scheduling) \uadf8\ub798\uc11c measuring performance\ub294 \uc815\ud655\ud788 \uc54c\uc544\ub0b4\uae30 \ud798\ub4e4\ub2e4. \uc774\ub97c methodoliges\ub97c \ub9cc\ub4e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud55c\ub2e4. - \uc5ec\ub7ec\ubc88 \ubc18\ubcf5\ud574\uc11c \uc218\ud589\ud55c\ub2e4. - \ud1b5\uacc4\ub97c \ub0b8\ub2e4: mean, variance - \uc544\uc6c3\ub77c\uc774\uc5b4\ub97c \uc81c\uac70\ud55c\ub2e4. - ensuring steady state(warm-up) - preventing anomalies(GC, JIT compilation, aggresive optimizations) ScalaMeter\ub294 JVM \uc744 \uc704\ud574 benchmarking\uacfc performance regression testing\uc744 \ud574\uc900\ub2e4. - performance regression testing: \uc774\uc804\uc758 run\uacfc \ube44\uad50\ud574\uc11c performance\ub97c \ube44\uad50 - benchmarking: \ud604\uc7ac \ud504\ub85c\uadf8\ub7a8 \ud639\uc740 \ud504\ub85c\uadf8\ub7a8\uc758 \uc77c\ubd80\ubd84\uc758 performance\ub97c \uce21\uc815 \uc774\ubc88 \uac15\uc88c\uc5d0\uc11c\ub294 benchmarking\uc5d0 \uc9d1\uc911\ud55c\ub2e4.","title":"Testing and Benchmarking"},{"location":"scala/parallel-programming/#using-scalameter","text":"first, add dependency libraryDependencies += \"com.storm-enroute\" %% \"scalameter-core\" % \"0.6\" \uadf8\ub9ac\uace0 \uc774\ub97c import\ud574\uc11c \uc0ac\uc6a9 import org.scalameter._ val time = measure { (0 until 1000000).toArray } println(s\"Array initialization time: $time ms\")","title":"Using ScalaMeter"},{"location":"scala/parallel-programming/#jvm-warmup","text":"\uc704\uc758 \uc608\ub294 two consecutive runs of program \ud560 \ub54c different running times\ub97c \ubcf4\uc600\ub2e4 JVM program\uc774 \uc2dc\uc791\ud560 \ub54c warmup\uc744 \ud558\ub294 \uc2dc\uac04\uc774 \ud544\uc694\ud558\ub2e4. \uadf8\ub9ac\uace0 \uadf8 \uc774\ud6c4\uc5d0\ub294 maimum performance\ub97c \ubc1c\ud718\ud55c\ub2e4. - \uccab \ubc88\uca30\ub85c \ud504\ub85c\uadf8\ub7a8\uc774 interpreted mode\ub85c run \ub41c\ub2e4. - \uadf8\ub9ac\uace0 \ud504\ub85c\uadf8\ub7a8\uc758 \uc77c\ubd80\ubd84\uc774 machine code\ub85c compile\ub41c\ub2e4. - \ud6c4\uc5d0 JVM\uc740 additional dynamic optimizations\ub97c \uc801\uc6a9\ud560 \uc9c0\ub97c \uacb0\uc815\ud55c\ub2e4. - eventually, \ud504\ub85c\uadf8\ub7a8\uc774 steady state\ub85c \ub41c\ub2e4. \uc6b0\ub9ac\ub294 \ud56d\uc0c1 steady state program performance(warm up\uc774 \ub41c \ud6c4\uc5d0 \ud504\ub85c\uadf8\ub7a8 \uc18d\ub3c4)\ub97c \uce21\uc815\ud558\uace0 \uc2f6\ub2e4. ScalaMeter\uc758 Warmer objects\ub294 \uc774\ub97c benchmark \ud560 \uc218 \uc788\ub2e4. import org.scalameter._ val time = withWarmer(new Warmer.Default) measure { (0 until 1000000).toArray } configuration\ub3c4 \ucd94\uac00\ud560 \uc218 \uc788\ub2e4. val time = config( Key.exec.minWarmupRuns -> 20, Key.exec.maxWarmupRuns -> 60, Key.verbose -> true ) withWarmer(new Warmer.Default) measure { (0 until 1000000).toArray } ScalaMeter\ub294 running time\uc774\uc678\uc5d0\ub3c4 \uc544\ub798\uc758 \ub9ac\uc2a4\ud2b8\uc5d0 \uc18d\ud558\ub294 \uac83\ub4e4\uc744 \uce21\uc815\ud560 \uc218 \uc788\ub2e4. - Measurer.Default \u2013 plain running time - IgnoringGC \u2013 running time without GC pauses - OutlierElimination \u2013 removes statistical outliers - MemoryFootprint \u2013 memory footprint of an object. object\uac00 memory\uc5d0\uc11c \uc5bc\ub9c8\ub9cc\ud07c\uc758 \uc790\ub9ac\ub97c \ucc28\uc9c0\ud558\ub294\uc9c0 \uce21\uc815 - GarbageCollectionCycles \u2013 total number of GC pauses - newer ScalaMeter versions can also measure method invocation counts and boxing counts val time = withWarmer(new Measurer.MemoryFootprint) measure { (0 until 1000000).toArray } steady state\uc774\uc804\uc77c\ub54c\ub294 \uac12\uc774 -\ub85c \ub098\uc62c \uc218 \uc788\ub2e4. 4000\uadfc\ucc98\uc758 \uac12\uc774 \ub098\uc624\ub294\ub370 \uc774\ub294 4000KB = 4MB\ub97c \ucc28\uc9c0\ud55c\ub2e4\ub294 \uc758\ubbf8\uc774\ub2e4.","title":"JVM Warmup"},{"location":"scala/parallel-programming/#week2","text":"fold\uc640 reduce\uc758 \ucc28\uc774\ub294 \ucd08\uae30\uac12\uc744 \uac16\ub290\ub0d0 \uc544\ub2c8\uba74 \ube48 List\ub97c \ucd08\uae30\uac12\uc73c\ub85c \uc0ac\uc6a9\ud558\ub290\ub0d0\uc758 \ucc28\uc774\ub2e4. left\uc640 Right\uc758 \ucc28\uc774\ub294 operation\uc744 \uc5b4\ub290 \ubc29\ud5a5\uc73c\ub85c \uc801\uc6a9\ud560 \uac83\uc778\uc9c0\uc758 \ucc28\uc774\ub2e4. List(1,3,8).foldLeft(100)((s,x) => s - x) == ((100 - 1) - 3) - 8 == 88 List(1,3,8).foldRight(100)((s,x) => s - x) == 1 - (3 - (8-100)) == -94 List(1,3,8).reduceLeft((s,x) => s - x) == (1 - 3) - 8 == -10 List(1,3,8).reduceRight((s,x) => s - x) == 1 - (3 - 8) == 6 sequential version of scanLeft def scanLeft[A](inp: Array[A], a0: A, f: (A,A) => A, out: Array[A]): Unit = { out(0)= a0 var a = a0 var i= 0 while (i < inp.length) { a= f(a,inp(i)) i= i + 1 out(i)= a } } def scanLeft[A](inp: Array[A], a0: A, f: (A,A) => A, out: Array[A]) = { val fi = { (i:Int,v:A) => reduceSeg1(inp, 0, i, a0, f) } mapSeg(inp, 0, inp.length, fi, out) val last = inp.length - 1 out(last + 1) = f(out(last), inp(last)) }","title":"Week2"},{"location":"scala/parallel-programming/#week3-data-parallelism","text":"Previously, we learned about task-parallel programming. A form of parallelization that distributes execution processes across computing nodes. We know how to express parallel programs with task and parallel constructs. Next, we learn about the data-parallel programming. A form of parallelization that distributes data across computing nodes. The simplest form of data-parallel programming is the parallel for loop. def initializeArray(xs: Array[Int])(v: Int): Unit = { for (i <- (0 until xs.length).par) { xs(i) = v } } Range\uc5d0 .par \ub97c \ubd99\uc774\uba74 parallel range \ub85c \ubcc0\ud658\ub41c\ub2e4. parallel loop\ub294 different processors\uc5d0\uc11c concurrently with each other\ub85c \uc2e4\ud589\ub420 \uac83\uc774\ub2e4 parallel loop\ub294 \uc5b4\ub5a0\ud55c \uac12\ub3c4 \ub9ac\ud134\ud558\uc9c0 \uc54a\ub294\ub2e4 \uadf8\ub807\uae30\uc5d0 \uc774\uc640 interact\ud560 \uc218 \uc788\ub294 \uc720\uc77c\ud55c \ubc29\ubc95\uc740 assign\ubc16\uc5d0 \uc5c6\ub2e4. \uc774\ub294 side effect\ub97c \uc720\ubc1c\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud55c\ub2e4. = not very functional \ud558\ub2e4. Workload \ub780 \uac01\uac01\uc758 input element\uc5d0 \ub300\ud574\uc11c \uc774\ub97c \uc2e4\ud589\ud558\ub294\ub370 \ud544\uc694\ud55c amount of work \uc774\ub2e4. different data-parallel programs\ub294 different workloads\ub97c \uac16\ub294\ub2e4. (= input element\uc5d0 \ub530\ub77c\uc11c \uc218\ud589\ub7c9\uc774 \ub2e4\ub974\ub2e4.) data-parallel scheduler \ub294 \uc774\ub7f0 different workloads\ub97c efficiently balance \ud574\uc8fc\uae30 \ub54c\ubb38\uc5d0 \ud504\ub85c\uadf8\ub798\uba38\uac00 \uc774\ub97c \uc9c1\uc811 \uad00\ub9ac\ud560 \ud544\uc694\ub294 \uc5c6\ub2e4. Scala\uc5d0\uc11c \ub300\ubd80\ubd84\uc758 collection operation\uc740 data-parallel\uc744 \uc9c0\uc6d0\ud55c\ub2e4. (1 until 1000).par .filter(n => n % 3 == 0) .count(n => n.toString == n.toString.reverse) filter\uc640 count\ub3c4 data parallel operation\uc744 \uc9c0\uc6d0\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uba87\uba87 collection operations\ub294 parallelizable \ud558\uc9c0 \uc54a\ub2e4. foldLeft\ub97c \uc774\uc6a9\ud55c sum\uc744 \uc0dd\uac01\ud574\ubcf4\uc790. def sum(xs: Array[Int]): Int = { xs.par.foldLeft(0)(_ + _) } \uc774\ub294 parallel \ud558\uac8c \uc218\ud589\ub418\ub294\uac00? \uc544\ub2c8\ub2e4. \uba3c\uc800 foldLeft\uc758 signature\ub97c \uc0b4\ud3b4\ubcf4\uc790. def foldLeft[B](z: B)(f: (B, A) => B): B \ud568\uc218 f\ub97c \ubcf4\uba74 A\uc640 B \ud0c0\uc785\uc744 \ubc1b\uc544\uc11c B \ud0c0\uc785\uc73c\ub85c \ub9cc\ub4e4\uc5b4 \uc900\ub2e4. \uc774 \uc758\ubbf8\ub294 foldLeft\ub97c \uc5f0\uc18d\uc801\uc73c\ub85c \uc218\ud589\ud560 \ub54c \uba3c\uc800 \uc218\ud589\ud55c foldLeft\uc758 \uacb0\uacfc\ub97c \ubc1b\uace0 \ub098\uc11c\uc57c \ub2e4\uc74c foldLeft\ub97c \uacc4\uc0b0\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud55c\ub2e4.(= \ucc38\uc870 \ud22c\uba85\ud558\uc9c0 \uc54a\ub2e4.) \uc774\uc640 \uac19\uc740 \uc885\ub958\uc758 foldRight, reduceLeft, reduceRight, scanLeft and scanRight\ub294 \ub3d9\uc77c\ud55c \ubb38\uc81c\ub97c \uac16\ub294\ub2e4. \ub9cc\uc57d \uc774\ub97c parallel\ud558\uac8c \ud558\uace0 \uc2f6\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uc815\uc758\uac00 \ud544\uc694\ud558\ub2e4. def fold(z: A)(f: (A, A) => A): A \uc774\ub97c \uc774\uc6a9\ud558\uba74 max\ub97c \uc544\ub798\uc640 \uac19\uc774 \uad6c\ud604\ud560 \uc218 \uc787\ub2e4. def max(xs: Array[Int]): Int = { xs.par.fold(Int.MinValue)(math.max) } data-parallel operation\uc744 \uc218\ud589\ud560 \ub54c\ub294 \uba87 \uac00\uc9c0 \uc8fc\uc758 \uc0ac\ud56d\uc774 \uc788\ub2e4. same memory locations\ub97c \uc218\uc815\ud558\ub294 \uac83\uc744 \ud53c\ud574\uc57c \ud55c\ub2e4. def intersection(a: GenSet[Int], b: GenSet[Int]): Set[Int] = { val result = mutable.Set[Int]() for (x <- a) if (b contains x) result += x result } intersection((0 until 1000).toSet, (0 until 1000 by 4).toSet) intersection((0 until 1000).par.toSet, (0 until 1000 by 4).par.toSet) \uc704\uc5d0\uc11c .par\ub85c \uc2e4\ud589\ud55c \ucf54\ub4dc\ub294 parallel\ud558\uac8c \ub3d9\uc791\ud558\uba74\uc11c result\ub77c\ub294 Set\uc744 \ubcc0\uacbd\ud55c\ub2e4. \ube44\ub85d \uc5ec\uae30\uc5d0\uc120 \uc624\ub958\uac00 \ubc1c\uc0dd\ud558\uc9c0 \uc54a\uc558\uc9c0\ub9cc \uc774\ub294 side effect\ub97c \ucd08\ub798\ud560 \uc218 \uc788\ub2e4. \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574\uc120 \uc544\ub798\uc640 \uac19\uc740 \ubc29\ubc95\uc774 \uc788\ub2e4. 1) use a concurrent collection, which can be mutated by multiple threads import java.util.concurrent._ def intersection(a: GenSet[Int], b: GenSet[Int]) = { val result = new ConcurrentSkipListSet[Int]() for (x <- a) if (b contains x) result += x result } intersection((0 until 1000).toSet, (0 until 1000 by 4).toSet) intersection((0 until 1000).par.toSet, (0 until 1000 by 4).par.toSet) 2) correct combinator\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 filter\ub97c \uc0ac\uc6a9\ud558\uba74 \ub41c\ub2e4. def intersection(a: GenSet[Int], b: GenSet[Int]): GenSet[Int] = { if (a.size < b.size) a.filter(b(_)) else b.filter(a(_)) } intersection((0 until 1000).toSet, (0 until 1000 by 4).toSet) intersection((0 until 1000).par.toSet, (0 until 1000 by 4).par.toSet) data-parallel operation \uc218\ud589 \uc911\uc77c \ub54c parallel collection\uc744 \uc218\uc815\ud558\uba74 \uc808\ub300 \uc548\ub41c\ub2e4. \ub610\ud55c \uc218\uc815\ub41c parallel collection\uc744 \uc77d\ub294 \uac83\ub3c4 \uc808\ub300 \uc548\ub41c\ub2e4. val graph = mutable.Map[Int, Int]() ++= (0 until 100000).map(i => (i, i + 1)) graph(graph.size - 1) = 0 for ((k, v) <- graph.par) graph(k) = graph(v) val violation = graph.find({ case (i, v) => v != (i + 2) % graph.size }) println(s\u201dviolation: $violation\u201d) \uc704 \ucf54\ub4dc\ub294 \ud504\ub85c\uc138\uc2a4\ub97c \uc9c4\ud589\ud558\uba74\uc11c collection\uc744 \uc218\uc815\ud55c\ub2e4. \uc704\uc758 \uc624\ub958\ub97c \ud53c\ud558\uae30 \uc704\ud574 \uc544\ub798\uc758 collection\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. TrieMap Collection\uc740 \uc774\ub7f0 rule\uc5d0 \ub300\ud574 exception\uc744 \uac16\uace0 \uc788\ub2e4. val graph = concurrent.TrieMap[Int, Int]() ++= (0 until 100000).map(i => (i, i + 1)) graph(graph.size - 1) = 0 val previous = graph.snapshot() for ((k, v) <- graph.par) graph(k) = previous(v) val violation = graph.find({ case (i, v) => v != (i + 2) % graph.size }) println(s\u201dviolation: $violation\u201d)","title":"Week3 Data-Parallelism"}]}